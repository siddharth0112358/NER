{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "17f60a5376c77425fc92555942c601d199d1b6f4d9fe48b17446cfeffaeccd2f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://github.com/kuldeep7688/BioMedicalBertNer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Utility Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example.\n",
    "    \"\"\"\n",
    "    def __init__(self, guid, words=None, labels=None, sentence=None):\n",
    "        \"\"\"Contructs a InputExample object.\n",
    "        Args:\n",
    "            guid (TYPE): unique id for the example\n",
    "            words (TYPE): the words of the sequence\n",
    "            labels (TYPE): the labels for each work of the sentence\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "        self.sentence = sentence\n",
    "\n",
    "        if self.words is None and self.sentence:\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            # split sentence on whitepsace so that different tokens may be attributed to their original positions\n",
    "            for c in self.sentence:\n",
    "                if _is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            self.words = doc_tokens\n",
    "            if self.labels is None:\n",
    "                self.labels = [\"O\"]*len(self.words)\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"\n",
    "    A sigle set of input features for an example.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids=None, token_to_orig_index=None, orig_to_token_index=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        self.token_to_orig_index = token_to_orig_index\n",
    "        self.orig_to_token_index = orig_to_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples_from_file(data_dir, mode, line_splitter=\"\\t\"):\n",
    "    file_path = os.path.join(data_dir, \"{}.tsv\".format(mode))\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                if words:\n",
    "                    examples.append(\n",
    "                        InputExample(\n",
    "                            guid=\"{}-{}\".format(mode, guid_index),\n",
    "                            words=words,\n",
    "                            labels=labels\n",
    "                        )\n",
    "                    )\n",
    "                    guid_index += 1\n",
    "                    words = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                splits = line.split(line_splitter)\n",
    "                words.append(splits[0])\n",
    "                if len(splits) > 1:\n",
    "                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "                else:\n",
    "                    # examples could have no label for model == test\n",
    "                    labels.append(\"O\")\n",
    "        if words:\n",
    "            examples.append(\n",
    "                InputExample(\n",
    "                    guid=\"{}-{}\".format(mode, guid_index),\n",
    "                    words=words,\n",
    "                    labels=labels\n",
    "                )\n",
    "            )\n",
    "    return examples\n",
    "\n",
    "\n",
    "def get_i_label(beginning_label, label_map):\n",
    "    \"\"\"To properly label segments of words broken by BertTokenizer=.\n",
    "    \"\"\"\n",
    "    if \"B-\" in beginning_label:\n",
    "        i_label = \"I-\" + beginning_label.split(\"B-\")[-1]\n",
    "        return i_label\n",
    "    elif \"I-\" in beginning_label:\n",
    "        i_label = \"I-\" + beginning_label.split(\"I-\")[-1]\n",
    "        return i_label\n",
    "    else:\n",
    "        return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(\n",
    "    examples, label_map, max_seq_length,\n",
    "    tokenizer, label_end_token=\"<EOS>\",\n",
    "    pad_token_label_id=-1, mask_padding_with_zero=True,\n",
    "    logger=None, summary_writer=None, mode=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare features to be given as input to Bert\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        token_to_orig_index = []\n",
    "        orig_to_token_index = []\n",
    "        for word_idx, (word, label) in enumerate(zip(example.words, example.labels)):\n",
    "            orig_to_token_index.append(len(tokens))\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if len(word_tokens) > 0:\n",
    "                tokens.extend(word_tokens)\n",
    "                # USe the real label id for the first token of the word, and\n",
    "                # propagate I-tag for the splitted tokens\n",
    "                label_ids.extend(\n",
    "                    [label_map[label]] + [label_map[get_i_label(label, label_map)]]\n",
    "                    * (len(word_tokens) - 1)\n",
    "                )\n",
    "            for tok in word_tokens:\n",
    "                token_to_orig_index.append(word_idx)\n",
    "\n",
    "        special_tokens_count = 2 # for bert cls sentence sep\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[:(max_seq_length - special_tokens_count)]\n",
    "\n",
    "        tokens += [tokenizer.sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        segment_ids = [0]*len(tokens)\n",
    "\n",
    "        tokens = [tokenizer.cls_token] + tokens\n",
    "        label_ids = [pad_token_label_id] + label_ids\n",
    "        segment_ids = [0] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1]*len(input_ids)\n",
    "\n",
    "        # Zero pad up to the sequence length\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        input_mask += [0] * padding_length\n",
    "        segment_ids += [0] * padding_length\n",
    "        label_ids += [pad_token_label_id] * padding_length\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            if logger:\n",
    "                logger.info(\"****** EXAMPLES ********\")\n",
    "                logger.info(\"guid: {}\".format(example.guid))\n",
    "                logger.info(\"tokens: {}\".format(\" \".join([str(x) for x in tokens])))\n",
    "                logger.info(\"input ids : {}\".format(\" \".join([str(x) for x in input_ids])))\n",
    "                logger.info(\"input_mask : {}\".format(\" \".join([str(x) for x in input_mask])))\n",
    "                logger.info(\"segment_ids : {}\".format(\" \".join([str(x) for x in segment_ids])))\n",
    "                logger.info(\"label_ids : {}\".format(\" \".join([str(x) for x in label_ids])))\n",
    "\n",
    "            if summary_writer:\n",
    "                summary_writer.add_text(mode, \"guid: {}\".format(example.guid), 0)\n",
    "                summary_writer.add_text(mode, \"tokens: {}\".format(\" \".join([str(x) for x in tokens])), 0)\n",
    "                summary_writer.add_text(mode, \"input ids : {}\".format(\" \".join([str(x) for x in input_ids])), 0)\n",
    "                summary_writer.add_text(mode, \"input_mask : {}\".format(\" \".join([str(x) for x in input_mask])), 0)\n",
    "                summary_writer.add_text(mode, \"segment_ids : {}\".format(\" \".join([str(x) for x in segment_ids])), 0)\n",
    "                summary_writer.add_text(mode, \"label_ids : {}\".format(\" \".join([str(x) for x in label_ids])), 0)\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                label_ids=label_ids,\n",
    "                token_to_orig_index=token_to_orig_index,\n",
    "                orig_to_token_index=orig_to_token_index\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path):\n",
    "    if path:\n",
    "        with open(path, \"r\") as f:\n",
    "            labels = f.read().splitlines()\n",
    "\n",
    "        if \"O\" not in labels:\n",
    "            labels = [\"O\"] + labels\n",
    "        return labels\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_and_cache_examples(\n",
    "    max_seq_length, tokenizer, label_map, pad_token_label_id,\n",
    "    mode, data_dir=None, logger=None, summary_writer=None,\n",
    "    sentence_list=None, return_features_and_examples=False\n",
    "):\n",
    "    \"Loads data features from cache or dataset file\"\n",
    "\n",
    "    if sentence_list is None:\n",
    "        if data_dir:\n",
    "            print(\"Creating features from dataset file at {}\".format(data_dir))\n",
    "            examples = read_examples_from_file(data_dir, mode)\n",
    "    else:\n",
    "        # will mainly be used in\n",
    "        examples = []\n",
    "        for idx, sentence in enumerate(sentence_list):\n",
    "            examples.append(\n",
    "                InputExample(\n",
    "                    guid=idx, words=None, labels=None, sentence=sentence\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    features = convert_examples_to_features(\n",
    "        examples=examples, label_map=label_map,\n",
    "        max_seq_length=max_seq_length, mode=mode,\n",
    "        tokenizer=tokenizer,\n",
    "        pad_token_label_id=pad_token_label_id,\n",
    "        logger=logger, summary_writer=summary_writer\n",
    "    )\n",
    "    # Convert into tensors and build dataset\n",
    "    all_input_ids_list = []\n",
    "    all_input_mask_list = []\n",
    "    all_segment_ids_list = []\n",
    "    all_label_ids_list = []\n",
    "\n",
    "    for f in features:\n",
    "        all_input_ids_list.append(f.input_ids)\n",
    "        all_input_mask_list.append(f.input_mask)\n",
    "        all_segment_ids_list.append(f.segment_ids)\n",
    "        all_label_ids_list.append(f.label_ids)\n",
    "\n",
    "    all_input_ids = torch.tensor(all_input_ids_list, dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(all_input_mask_list, dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(all_segment_ids_list, dtype=torch.long)\n",
    "    all_label_ids = torch.tensor(all_label_ids_list, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids, all_input_mask, all_segment_ids, all_label_ids\n",
    "    )\n",
    "    if return_features_and_examples:\n",
    "        return dataset, examples, features\n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    print(\n",
    "        \"Number of trainable parameters in the model are {}\".format(\n",
    "            sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_result_matrix(\n",
    "    loss, label_map, predictions_tensor, sentence_input_ids,\n",
    "    labels_tensor, sep_token_id, give_lists=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the results given predictions and labels\n",
    "    \"\"\"\n",
    "#     label_to_not_consider_in_results = [\n",
    "#         idx for label, idx in label_map.items()\n",
    "#         if label in [\"O\"]\n",
    "#     ]\n",
    "    label2idx = {i: label for label, i in label_map.items()}\n",
    "\n",
    "    out_label_list = [[] for _ in range(labels_tensor.shape[0])]\n",
    "    preds_list = [[] for _ in range(predictions_tensor.shape[0])]\n",
    "\n",
    "    for i in range(labels_tensor.shape[0]):\n",
    "        for j in range(labels_tensor.shape[1]):\n",
    "            if sentence_input_ids[i, j] == sep_token_id:\n",
    "                break\n",
    "            out_label_list[i].append(label2idx[labels_tensor[i][j]])\n",
    "            preds_list[i].append(label2idx[predictions_tensor[i][j]])\n",
    "\n",
    "    if give_lists:\n",
    "        results = {\n",
    "            \"loss\": loss,\n",
    "            \"precision\": precision_score(out_label_list, preds_list),\n",
    "            \"recall\": recall_score(out_label_list, preds_list),\n",
    "            \"f1\": f1_score(out_label_list, preds_list),\n",
    "            \"out_label_list\": out_label_list,\n",
    "            \"preds_list\": preds_list\n",
    "        }\n",
    "    else:\n",
    "        results = {\n",
    "            \"loss\": loss,\n",
    "            \"precision\": precision_score(out_label_list, preds_list),\n",
    "            \"recall\": recall_score(out_label_list, preds_list),\n",
    "            \"f1\": f1_score(out_label_list, preds_list),\n",
    "            \"out_label_list\": out_label_list,\n",
    "            \"preds_list\": preds_list\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, dataset, batch_size, label_map, max_grad_norm,\n",
    "    optimizer, scheduler, device, sep_token_id, summary_writer=None\n",
    "):\n",
    "    tr_loss = 0.0\n",
    "\n",
    "    preds = []\n",
    "    out_label_ids = []\n",
    "    input_ids_list = []\n",
    "\n",
    "    model.train()\n",
    "    sampler = RandomSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "    print_stats_at_step = int(len(dataloader) / 20.0)\n",
    "    epoch_iterator = tqdm(dataloader)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        model.zero_grad()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2],\n",
    "            \"labels\": batch[3]\n",
    "        }\n",
    "        # getting outputs\n",
    "        logits, inputs[\"labels\"], loss = model(**inputs)\n",
    "\n",
    "        # propagating loss backwards and scheduler and opt. steps\n",
    "        loss.backward()\n",
    "        step_loss = loss.item()\n",
    "        tr_loss += step_loss\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if summary_writer:\n",
    "            summary_writer.add_scaler(\"Loss/train\", step_loss)\n",
    "            summary_writer.add_scaler(\"LR/train\", scheduler.get_lr()[0])\n",
    "\n",
    "\n",
    "        # appending predictions and labels to list\n",
    "        # for calculation of result\n",
    "#         if preds is None:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "        out_label_ids.append(inputs[\"labels\"].detach().cpu().numpy())\n",
    "        input_ids_list.append(inputs[\"input_ids\"][:, 1:].detach().cpu().numpy())\n",
    "#         else:\n",
    "#             preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "#             out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "#             input_ids_list = np.append(\n",
    "#                     input_ids_list,\n",
    "#                     inputs[\"input_ids\"][:, 1:].detach().cpu().numpy(),\n",
    "#                     axis=0\n",
    "#                 )\n",
    "\n",
    "        if step % print_stats_at_step == 0:\n",
    "#             temp_results = get_result_matrix(\n",
    "#                     tr_loss / (step + 1), label_map, preds, input_ids_list,\n",
    "#                     out_label_ids, sep_token_id, give_lists=False\n",
    "#                 )\n",
    "            epoch_iterator.set_description(\n",
    "                f'Tr Iter: {step+1}| step_loss: {step_loss: .3f}' #| avg_tr_f1: {temp_results[\"f1\"]: .3f}'\n",
    "            )\n",
    "\n",
    "    preds = np.vstack(preds)\n",
    "    input_ids_list = np.vstack(input_ids_list)\n",
    "    out_label_ids = np.vstack(out_label_ids)\n",
    "    epoch_loss = tr_loss / len(dataloader)\n",
    "    results = get_result_matrix(\n",
    "        epoch_loss, label_map, preds, input_ids_list,\n",
    "        out_label_ids, sep_token_id, give_lists=False\n",
    "    )\n",
    "\n",
    "    if summary_writer:\n",
    "        summary_writer.add_scaler(\"F1_epoch/train\", results[\"f1\"])\n",
    "        summary_writer.add_scaler(\"Precision_epoch/train\", results[\"precision\"])\n",
    "        summary_writer.add_scaler(\"Recall_epoch/train\", results[\"recall\"])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def eval_epoch(\n",
    "    model, dataset, batch_size, label_map, device, sep_token_id,\n",
    "    summary_writer=None, give_lists=False\n",
    "):\n",
    "    eval_loss = 0.0\n",
    "    preds = []\n",
    "    out_label_ids = []\n",
    "    input_ids_list = []\n",
    "\n",
    "    model.eval()\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "    print_stats_at_step = int(len(dataloader) / 10.0)\n",
    "    epoch_iterator = tqdm(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "                \"labels\": batch[3]\n",
    "            }\n",
    "            # getting outputs\n",
    "            logits, inputs[\"labels\"], loss = model(**inputs)\n",
    "\n",
    "            # propagating loss backwards and scheduler and opt. steps\n",
    "            step_loss = loss.item()\n",
    "            eval_loss += step_loss\n",
    "\n",
    "            # appending predictions and labels to list\n",
    "            # for calculation of result\n",
    "#             if preds is None:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "            out_label_ids.append(inputs[\"labels\"].detach().cpu().numpy())\n",
    "            input_ids_list.append(inputs[\"input_ids\"][:, 1:].detach().cpu().numpy())\n",
    "#             else:\n",
    "#                 preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "#                 out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "#                 input_ids_list = np.append(\n",
    "#                     input_ids_list,\n",
    "#                     inputs[\"input_ids\"][:, 1:].detach().cpu().numpy(),\n",
    "#                     axis=0\n",
    "#                 )\n",
    "\n",
    "            if step % print_stats_at_step == 0:\n",
    "#                 temp_results = get_result_matrix(\n",
    "#                     eval_loss / (step + 1), label_map, preds, input_ids_list,\n",
    "#                     out_label_ids, sep_token_id, give_lists=False\n",
    "#                 )\n",
    "                epoch_iterator.set_description(\n",
    "                    f'Eval Iter: {step+1}| step_loss: {step_loss: .3f}'\n",
    "                )\n",
    "    preds = np.vstack(preds)\n",
    "    input_ids_list = np.vstack(input_ids_list)\n",
    "    out_label_ids = np.vstack(out_label_ids)\n",
    "    epoch_loss = eval_loss / len(dataloader)\n",
    "    results = get_result_matrix(\n",
    "        eval_loss / (step + 1), label_map, preds, input_ids_list,\n",
    "        out_label_ids, sep_token_id, give_lists=False\n",
    "    )\n",
    "\n",
    "    if summary_writer:\n",
    "        summary_writer.add_scaler(\"Loss/eval\", epoch_loss)\n",
    "        summary_writer.add_scaler(\"F1_epoch/eval\", results[\"f1\"])\n",
    "        summary_writer.add_scaler(\"Precision_epoch/eval\", results[\"precision\"])\n",
    "        summary_writer.add_scaler(\"Recall_epoch/eval\", results[\"recall\"])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# below functions are helpful in Inferencing\n",
    "def predictions_from_model(model, tokenizer, dataset, batch_size, label2idx, device):\n",
    "    pred_logits = None\n",
    "    input_ids_list = None\n",
    "    model.eval()\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "    epoch_iterator = tqdm(dataloader, total=len(dataloader))\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "                \"labels\": None\n",
    "            }\n",
    "            # getting outputs\n",
    "            logits, _, _ = model(**inputs)\n",
    "\n",
    "            # appending predictions and labels to list\n",
    "            if pred_logits is None:\n",
    "                pred_logits = logits.detach().cpu().numpy()\n",
    "                input_ids_list = inputs[\"input_ids\"][:, 1:].detach().cpu().numpy()\n",
    "            else:\n",
    "                pred_logits = np.append(pred_logits, logits.detach().cpu().numpy(), axis=0)\n",
    "                input_ids_list = np.append(\n",
    "                    input_ids_list,\n",
    "                    inputs[\"input_ids\"][:, 1:].detach().cpu().numpy(),\n",
    "                    axis=0\n",
    "                )\n",
    "\n",
    "    idx2label = {i: label for label, i in label2idx.items()}\n",
    "    prediction_labels  = []\n",
    "    for sentence_label_logits, sentence_input_ids in zip(pred_logits, input_ids_list):\n",
    "        temp = []\n",
    "        for i, (p, w) in enumerate(zip(sentence_label_logits, sentence_input_ids)):\n",
    "            if w == tokenizer.sep_token_id:\n",
    "                break\n",
    "            temp.append(idx2label[p])\n",
    "        prediction_labels.append(temp)\n",
    "    return prediction_labels\n",
    "\n",
    "\n",
    "def align_predicted_labels_with_original_sentence_tokens(predicted_labels, examples, features, max_seq_length, num_special_tokens):\n",
    "    \"\"\"The label_predictions out of the model is according to the tokens (that we get after tokenizing every word using tokenizer).\n",
    "    We need to align the predictions with the original words of the sentence.\n",
    "    \"\"\"\n",
    "    aligned_predicted_labels = []\n",
    "    for idx, (feature, p_l_s) in enumerate(zip(features, predicted_labels)):\n",
    "        # print(idx)\n",
    "        temp = []\n",
    "        for i in range(len(feature.orig_to_token_index)):\n",
    "            token_idx = feature.orig_to_token_index[i]\n",
    "            if token_idx < (max_seq_length - num_special_tokens):\n",
    "                temp.append(p_l_s[token_idx])\n",
    "            else:\n",
    "                temp.append(\"O\")\n",
    "        aligned_predicted_labels.append(temp)\n",
    "\n",
    "    return aligned_predicted_labels, [ex.labels for ex in examples]\n",
    "\n",
    "\n",
    "def convert_to_ents(tokens, tags):\n",
    "    start_offset = None\n",
    "    end_offset = None\n",
    "    ent_type = None\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "    entities = []\n",
    "    start_char_offset = 0\n",
    "    for offset, (token, tag) in enumerate(zip(tokens, tags)):\n",
    "        token_tag = tag\n",
    "        if token_tag == \"O\":\n",
    "            if ent_type is not None and start_offset is not None:\n",
    "                end_offset = offset - 1\n",
    "                entity = {\n",
    "                    \"type\": ent_type,\n",
    "                    \"entity\": \" \".join(tokens[start_offset: end_offset + 1]),\n",
    "                    \"start_offset\": start_char_offset,\n",
    "                    \"end_offset\": start_char_offset + len(\" \".join(tokens[start_offset: end_offset + 1]))\n",
    "                }\n",
    "                entities.append(entity)\n",
    "                start_char_offset += len(\" \".join(tokens[start_offset: end_offset + 2])) + 1\n",
    "                start_offset = None\n",
    "                end_offset = None\n",
    "                ent_type = None\n",
    "            else:\n",
    "                start_char_offset += len(token) + 1\n",
    "        elif ent_type is None:\n",
    "            ent_type = token_tag[2:]\n",
    "            start_offset = offset\n",
    "        elif ent_type != token_tag[2:]:\n",
    "            end_offset = offset - 1\n",
    "            entity = {\n",
    "                \"type\": ent_type,\n",
    "                \"entity\": \" \".join(tokens[start_offset: end_offset + 1]),\n",
    "                \"start_offset\": start_char_offset,\n",
    "                \"end_offset\": start_char_offset + len(\" \".join(tokens[start_offset: end_offset + 1]))\n",
    "            }\n",
    "            entities.append(entity)\n",
    "            # start of a new entity\n",
    "            ent_type = token_tag[2:]\n",
    "            start_offset = offset\n",
    "            end_offset = None\n",
    "\n",
    "    # catches an entity that foes up untill the last token\n",
    "    if ent_type and start_offset is not None and end_offset is not None:\n",
    "        entity = {\n",
    "            \"type\": ent_type,\n",
    "            \"entity\": \" \".join(tokens[start_offset:]),\n",
    "            \"start_offset\": start_char_offset,\n",
    "            \"end_offset\": start_char_offset + len(\" \".join(tokens[start_offset:]))\n",
    "        }\n",
    "        entities.append(entity)\n",
    "    return text, entities"
   ]
  },
  {
   "source": [
    "# Building BERT models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "class BertCrfForNER(BertModel):\n",
    "    \"\"\"\n",
    "    This class inherits functionality from huggingface BertModel.\n",
    "    It applies a crf layer on the Bert outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, pad_idx, sep_idx, num_labels):\n",
    "        \"\"\"Inititalization\n",
    "        Args:\n",
    "            config (TYPE): model config flie (similar to bert_config.json)\n",
    "            num_labels : total number of layers using the bio format\n",
    "            pad_idx (TYPE): pad_idx of the tokenizer\n",
    "            device (TYPE): torch.device()\n",
    "        \"\"\"\n",
    "        super(BertCrfForNER, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sep_idx = sep_idx\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.crf_layer = CRF(self.num_labels, batch_first=True)\n",
    "        self.linear = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def create_mask_for_crf(self, inp):\n",
    "        \"\"\"Creates a mask for the feeding to crf layer.\n",
    "           Mask <PAD> and <SEP> token positions\n",
    "        Args:\n",
    "            inp (TYPE): input given to bert layer\n",
    "        \"\"\"\n",
    "\n",
    "        mask = (inp != self.pad_idx) & (inp != self.sep_idx)\n",
    "        # mask = [seq_len, batch_size]\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, token_type_ids=None,\n",
    "        position_ids=None, head_mask=None, labels=None\n",
    "    ):\n",
    "        \"\"\"Forwar propagate.\n",
    "        Args:\n",
    "            input_ids (TYPE): bert input ids\n",
    "            attention_mask (None, optional): attention mask for bert\n",
    "            token_type_ids (None, optional): token type ids for bert\n",
    "            position_ids (None, optional): position ids for bert\n",
    "            head_mask (None, optional): head mask for bert\n",
    "            labels (None, optional): labels required while training crf\n",
    "        \"\"\"\n",
    "        # getting outputs from Bert\n",
    "        outputs = self.bert(\n",
    "            input_ids, attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "        # taking tokens embeddings from the output\n",
    "        sequence_output = outputs[0]\n",
    "        # sequence_ouput = [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        logits = self.linear(sequence_output)\n",
    "        # logits = [batch_size, seq_len, num_labels]\n",
    "\n",
    "        # removing cls token\n",
    "        logits = logits[:, 1:, :]\n",
    "        if labels is not None:\n",
    "            labels = labels[:, 1:] # check whether labels include the cls token too or not\n",
    "        input_ids = input_ids[:, 1:]\n",
    "\n",
    "        mask = self.create_mask_for_crf(input_ids)\n",
    "        if labels is not None:\n",
    "            loss = self.crf_layer(\n",
    "                logits, labels, mask=mask\n",
    "            ) * torch.tensor(-1, device=self.device)\n",
    "        else:\n",
    "            loss = None\n",
    "        # this is the crf loss\n",
    "\n",
    "        out = self.crf_layer.decode(logits)\n",
    "        out = torch.tensor(out, dtype=torch.long, device=self.device)\n",
    "\n",
    "        # out = [batch_size, seq_length]\n",
    "        return out, labels, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForTokenClassification(BertModel):\n",
    "    \"\"\"\n",
    "    Simply doing token classification over bert outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels, classification_layer_sizes=[]):\n",
    "        super(BertForTokenClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout_layer = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.bert = BertModel(config)\n",
    "        self.input_layer_sizes = [config.hidden_size] + classification_layer_sizes\n",
    "        self.output_layer_size = classification_layer_sizes + [self.num_labels]\n",
    "        self.classification_module = nn.ModuleList(\n",
    "            nn.Linear(inp, out)\n",
    "            for inp, out in zip(self.input_layer_sizes, self.output_layer_size)\n",
    "        )\n",
    "        self.num_linear_layer = len(classification_layer_sizes) + 1\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, token_type_ids=None,\n",
    "        position_ids=None, head_mask=None, labels=None\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "\n",
    "        logits = outputs[0]\n",
    "        for layer_idx, layer in enumerate(self.classification_module):\n",
    "            if layer_idx + 1 != self.num_linear_layer:\n",
    "                logits = self.dropout_layer(F.relu(layer(logits)))\n",
    "            else:\n",
    "                logits = layer(logits)\n",
    "\n",
    "        # escaping cls token\n",
    "        logits = logits[:, 1:, :].contiguous()\n",
    "        if labels is not None:\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "        input_ids = input_ids[:, 1:].contiguous()\n",
    "        attention_mask = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss =loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        softs, out = torch.max(logits, axis=2)\n",
    "        return out, labels, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLstmCrf(BertModel):\n",
    "    \"\"\"On the outputs of Bert there is a LSTM layer.\n",
    "    On top of the LSTM there is a  CRF layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, config, pad_idx, lstm_hidden_dim,\n",
    "        num_lstm_layers, bidirectional, num_labels\n",
    "    ):\n",
    "        super(BertLstmCrf, self).__init__(config)\n",
    "        self.dropout_prob = config.hidden_dropout_prob\n",
    "        self.pad_idx = pad_idx\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        if self.num_lstm_layers > 1:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=config.hidden_size, hidden_size=self.lstm_hidden_dim,\n",
    "                num_layers=self.num_lstm_layers, bidirectional=self.bidirectional,\n",
    "                dropout=self.dropout_prob, batch_first=True\n",
    "            )\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=config.hidden_size, hidden_size=self.lstm_hidden_dim,\n",
    "                num_layers=self.num_lstm_layers, bidirectional=self.bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        if self.bidirectional is True:\n",
    "            self.linear = nn.Linear(self.lstm_hidden_dim*2, self.num_labels)\n",
    "        else:\n",
    "            self.linear = nn.Linear(self.lstm_hidden_dim, self.num_labels)\n",
    "\n",
    "        self.crf_layer = CRF(self.num_labels, batch_first=True)\n",
    "        self.dropout_layer = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def create_mask_for_crf(self, inp):\n",
    "        \"\"\"Creates a mask for the feesing to crf layer.\n",
    "        Args:\n",
    "            inp (TYPE): input given to bert layer\n",
    "        \"\"\"\n",
    "        mask = (inp != self.pad_idx) & (inp != self.sep_idx)\n",
    "        # mask = [seq_len, batch_size]\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, token_type_ids=None,\n",
    "        position_ids=None, head_mask=None, labels=None\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        lstm_out, (hidden, cell) = self.lstm(sequence_output)\n",
    "        logits = self.linear(self.dropout_layer(lstm_out))\n",
    "\n",
    "        # removing cls token\n",
    "        logits = logits[:, 1:, :]\n",
    "        if labels is not None:\n",
    "            labels = labels[:, 1:]\n",
    "        input_ids = input_ids[:, 1:]\n",
    "\n",
    "        # creating mask for crf\n",
    "        mask = self.create_mask_for_crf(input_ids)\n",
    "\n",
    "        # crf part\n",
    "        if labels is not None:\n",
    "            loss = self.crf_layer(logits, labels, mask=mask) * torch.tensor(-1, device=self.device)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        out = self.crf_layer.decode(logits)\n",
    "        out = torch.tensor(out, dtype=torch.long, device=self.device)\n",
    "        # out = [batch_Size, seq_len]\n",
    "        return out, labels, loss"
   ]
  },
  {
   "source": [
    "# Get scibert models\n",
    "https://github.com/allenai/scibert#pytorch-models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Train the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import fire\n",
    "import sys\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner_model(model_config_path, data_dir, logger_file_dir=None, labels_file=None):\n",
    "    # loading model config path\n",
    "    if os.path.exists(model_config_path):\n",
    "        with open(model_config_path, \"r\", encoding=\"utf-8\") as reader:\n",
    "            text = reader.read()\n",
    "        model_config_dict = json.loads(text)\n",
    "    else:\n",
    "        print(\"model_config_path doesn't exist.\")\n",
    "        sys.exit()\n",
    "\n",
    "    if os.path.exists(model_config_dict[\"final_model_saving_dir\"]):\n",
    "        output_model_file = model_config_dict[\"final_model_saving_dir\"] + \"pytorch_model.bin\"\n",
    "        output_config_file = model_config_dict[\"final_model_saving_dir\"] + \"bert_config.json\"\n",
    "        output_vocab_file = model_config_dict[\"final_model_saving_dir\"] + \"vocab.txt\"\n",
    "    else:\n",
    "        print(\"model_saving_dir doesn't exist.\")\n",
    "        sys.exit()\n",
    "\n",
    "    if os.path.exists(logger_file_dir):\n",
    "        logging.basicConfig(\n",
    "            filename=logger_file_dir + \"logs.txt\",\n",
    "            filemode=\"w\"\n",
    "        )\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        print(\"logger_file_path doesn't exist.\")\n",
    "        sys.exit()\n",
    "\n",
    "    if os.path.exists(labels_file):\n",
    "        print(\"Labels file exist\")\n",
    "    else:\n",
    "        print(\"labels_file doesn't exist.\")\n",
    "        sys.exit()\n",
    "\n",
    "    logger.info(\"Training configurations are given below ::\")\n",
    "    for key, val in model_config_dict.items():\n",
    "        logger.info(\"{} == {}\".format(key, val))\n",
    "\n",
    "    logger.info(\"Started training model :::::::::::::::::::::\")\n",
    "\n",
    "    bert_config = BertConfig.from_json_file(model_config_dict[\"bert_config_path\"])\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "        model_config_dict[\"bert_vocab_path\"],\n",
    "        config=bert_config,\n",
    "        do_lower_case=model_config_dict[\"tokenizer_do_lower_case\"]\n",
    "    )\n",
    "    # saving confgi and tokenizer\n",
    "    bert_tokenizer.save_vocabulary(output_vocab_file)\n",
    "    bert_config.to_json_file(output_config_file)\n",
    "\n",
    "    labels = get_labels(labels_file)\n",
    "    logger.info(\"Labels for Ner are: {}\".format(labels))\n",
    "\n",
    "    label2idx = {l: i for i, l in enumerate(labels)}\n",
    "\n",
    "\n",
    "    # preparing training data\n",
    "    train_dataset = load_and_cache_examples(\n",
    "        data_dir=data_dir,\n",
    "        max_seq_length=model_config_dict[\"max_seq_length\"],\n",
    "        tokenizer=bert_tokenizer,\n",
    "        label_map=label2idx,\n",
    "        pad_token_label_id=label2idx[\"O\"],\n",
    "        mode=\"train\", logger=logger\n",
    "    )\n",
    "    # preparing eval data\n",
    "    eval_dataset = load_and_cache_examples(\n",
    "        data_dir=data_dir,\n",
    "        max_seq_length=model_config_dict[\"max_seq_length\"],\n",
    "        tokenizer=bert_tokenizer,\n",
    "        label_map=label2idx,\n",
    "        pad_token_label_id=label2idx[\"O\"],\n",
    "        mode=\"dev\", logger=logger\n",
    "    )\n",
    "    logger.info(\"Training data and eval data loaded successfully.\")\n",
    "    # Change model_type as required\n",
    "    if model_config_dict[\"model_type\"] == \"crf\":\n",
    "        model = BertCrfForNER.from_pretrained(\n",
    "            model_config_dict[\"bert_model_path\"],\n",
    "            config=bert_config,\n",
    "            pad_idx=bert_tokenizer.pad_token_id,\n",
    "            sep_idx=bert_tokenizer.sep_token_id,\n",
    "            num_labels=len(labels)\n",
    "        )\n",
    "\n",
    "    logger.info(\"{} model loaded successfully.\".format(model_config_dict[\"model_type\"]))\n",
    "\n",
    "    # checking whether to finetune or not\n",
    "    if model_config_dict[\"finetune\"] == True:\n",
    "        logger.info(\"Finetuning bert.\")\n",
    "    else:\n",
    "        for param in list(model.bert.parameters()):\n",
    "            param.requires_grad = False\n",
    "        logger.info(\"Freezing Berts weights.\")\n",
    "\n",
    "    # preparing optimizer and scheduler\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0\n",
    "        }\n",
    "    ]\n",
    "    # total optimizer steps\n",
    "    t_total = int((len(train_dataset) / model_config_dict[\"train_batch_size\"]) * model_config_dict[\"num_epochs\"])\n",
    "    logger.info(\"t_total : {}\".format(t_total))\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=model_config_dict[\"learning_rate\"],\n",
    "        eps=model_config_dict[\"epsilon\"]\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=model_config_dict[\"warmup_steps\"],\n",
    "        num_training_steps=t_total\n",
    "    )\n",
    "    logger.info(\"{}\".format(count_parameters))\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    best_eval_f1 = 0.0\n",
    "    for epoch in range(model_config_dict[\"num_epochs\"]):\n",
    "        train_result = train_epoch(\n",
    "            model=model, dataset=train_dataset,\n",
    "            batch_size=model_config_dict[\"train_batch_size\"],\n",
    "            label_map=label2idx,\n",
    "            max_grad_norm=model_config_dict[\"max_grad_norm\"],\n",
    "            optimizer=optimizer, scheduler=scheduler, device=DEVICE,\n",
    "            sep_token_id=bert_tokenizer.sep_token_id\n",
    "        )\n",
    "        eval_result = eval_epoch(\n",
    "            model=model, dataset=eval_dataset,\n",
    "            batch_size=model_config_dict[\"validation_batch_size\"],\n",
    "            label_map=label2idx, device=DEVICE, sep_token_id=bert_tokenizer.sep_token_id,\n",
    "            give_lists=False\n",
    "        )\n",
    "        print(f'Epoch: {epoch + 1}')\n",
    "        print(f'Train Loss: {train_result[\"loss\"]: .4f}| Train F1: {train_result[\"f1\"]: .4f}')\n",
    "        print(f'Eval Loss: {eval_result[\"loss\"]: .4f}| Eval F1: {eval_result[\"f1\"]: .4f}')\n",
    "        logger.info(f'Epoch: {epoch + 1}')\n",
    "        logger.info(f'Train Loss: {train_result[\"loss\"]: .4f}| Train F1: {train_result[\"f1\"]: .4f}')\n",
    "        logger.info(f'Eval Loss: {eval_result[\"loss\"]: .4f}| Eval F1: {eval_result[\"f1\"]: .4f}')\n",
    "\n",
    "        if best_eval_f1 < eval_result[\"f1\"]:\n",
    "            best_eval_f1 = eval_result[\"f1\"]\n",
    "            # saving model to disk\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "            print(\"Saved a better model.\")\n",
    "            logger.info(\"Saved a beter model\")\n",
    "            del model_to_save\n",
    "\n",
    "    # loading the best model and test results\n",
    "    model.load_state_dict(torch.load(output_model_file))\n",
    "    logger.info(\"Loaded best model successfully.\")\n",
    "\n",
    "    test_dataset, test_examples, test_features = load_and_cache_examples(\n",
    "        data_dir=data_dir,\n",
    "        max_seq_length=model_config_dict[\"max_seq_length\"],\n",
    "        tokenizer=bert_tokenizer,\n",
    "        label_map=label2idx,\n",
    "        pad_token_label_id=label2idx[\"O\"],\n",
    "        mode=\"test\", logger=logger,\n",
    "        return_features_and_examples=True\n",
    "    )\n",
    "    logger.info(\"Test data loaded successfully.\")\n",
    "\n",
    "    test_label_predictions = predictions_from_model(\n",
    "        model=model, tokenizer=bert_tokenizer,\n",
    "        dataset=test_dataset,\n",
    "        batch_size=model_config_dict[\"validation_batch_size\"],\n",
    "        label2idx=label2idx, device=DEVICE\n",
    "    )\n",
    "    # restructure test_label_predictions with real labels\n",
    "    aligned_predicted_labels, true_labels = align_predicted_labels_with_original_sentence_tokens(\n",
    "        test_label_predictions, test_examples, test_features, max_seq_length=model_config_dict[\"max_seq_length\"],\n",
    "        num_special_tokens=model_config_dict[\"num_special_tokens\"]\n",
    "    )\n",
    "    print(\"Test Results classification report...\")\n",
    "    print(classification_report(true_labels, aligned_predicted_labels))\n",
    "    return aligned_predicted_labels, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Labels file exist\n",
      "Creating features from dataset file at /Users/sdeshpande/Desktop/bioinformatices/MTL-Bioinformatics-2016/data/NCBI-disease-IOB/\n",
      "Creating features from dataset file at /Users/sdeshpande/Desktop/bioinformatices/MTL-Bioinformatics-2016/data/NCBI-disease-IOB/\n",
      "Some weights of the model checkpoint at /Users/sdeshpande/Desktop/text_analysis_scripts/biomedical_bert_ner/scibert_scivocab_uncased/pytorch_model.bin were not used when initializing BertCrfForNER: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertCrfForNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertCrfForNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertCrfForNER were not initialized from the model checkpoint at /Users/sdeshpande/Desktop/text_analysis_scripts/biomedical_bert_ner/scibert_scivocab_uncased/pytorch_model.bin and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'crf_layer.start_transitions', 'crf_layer.end_transitions', 'crf_layer.transitions', 'linear.weight', 'linear.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Tr Iter: 169| step_loss:  591.380: 100%|| 170/170 [22:55<00:00,  8.09s/it]\n",
      "Eval Iter: 29| step_loss:  629.785: 100%|| 29/29 [03:38<00:00,  7.54s/it]\n",
      "Epoch: 1\n",
      "Train Loss:  763.2201| Train F1:  0.0362\n",
      "Eval Loss:  568.7555| Eval F1:  0.0328\n",
      "  0%|          | 0/170 [00:00<?, ?it/s]Saved a better model.\n",
      "Tr Iter: 169| step_loss:  455.464: 100%|| 170/170 [22:48<00:00,  8.05s/it]\n",
      "Eval Iter: 29| step_loss:  455.450: 100%|| 29/29 [03:32<00:00,  7.34s/it]\n",
      "  0%|          | 0/170 [00:00<?, ?it/s]Epoch: 2\n",
      "Train Loss:  489.4706| Train F1:  0.0337\n",
      "Eval Loss:  396.2671| Eval F1:  0.0046\n",
      "Tr Iter: 169| step_loss:  249.913: 100%|| 170/170 [22:28<00:00,  7.93s/it]\n",
      "Eval Iter: 29| step_loss:  384.884: 100%|| 29/29 [03:33<00:00,  7.35s/it]\n",
      "  0%|          | 0/170 [00:00<?, ?it/s]Epoch: 3\n",
      "Train Loss:  381.2615| Train F1:  0.0103\n",
      "Eval Loss:  331.5394| Eval F1:  0.0000\n",
      "Tr Iter: 169| step_loss:  294.320: 100%|| 170/170 [22:27<00:00,  7.92s/it]\n",
      "Eval Iter: 29| step_loss:  354.769: 100%|| 29/29 [03:26<00:00,  7.13s/it]\n",
      "  0%|          | 0/170 [00:00<?, ?it/s]Epoch: 4\n",
      "Train Loss:  338.2322| Train F1:  0.0029\n",
      "Eval Loss:  306.4297| Eval F1:  0.0000\n",
      "Tr Iter: 169| step_loss:  240.109: 100%|| 170/170 [21:49<00:00,  7.70s/it]\n",
      "Eval Iter: 29| step_loss:  346.655: 100%|| 29/29 [03:26<00:00,  7.12s/it]\n",
      "Epoch: 5\n",
      "Train Loss:  322.3650| Train F1:  0.0025\n",
      "Eval Loss:  299.9075| Eval F1:  0.0000\n",
      "Creating features from dataset file at /Users/sdeshpande/Desktop/bioinformatices/MTL-Bioinformatics-2016/data/NCBI-disease-IOB/\n",
      "100%|| 30/30 [03:28<00:00,  6.96s/it]\n",
      "Test Results classification report...\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  Disease       0.09      0.10      0.09       960\n",
      "\n",
      "micro avg       0.09      0.10      0.09       960\n",
      "macro avg       0.09      0.10      0.09       960\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'O', 'O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       "  ['O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O', 'O', 'O'],\n",
       "  ['O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O', 'O', 'O', 'O'],\n",
       "  ['O', 'O', 'B-Disease', 'I-Disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Disease',\n",
       "   'I-Disease',\n",
       "   'I-Disease',\n",
       "   'O',\n",
       "   'O']])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "train_ner_model('/Users/sdeshpande/Desktop/text_analysis_scripts/biomedical_bert_ner/crf_ner_config.json', '/Users/sdeshpande/Desktop/bioinformatices/MTL-Bioinformatics-2016/data/NCBI-disease-IOB/', '/Users/sdeshpande/Desktop/text_analysis_scripts/biomedical_bert_ner/log_dir/', '/Users/sdeshpande/Desktop/text_analysis_scripts/biomedical_bert_ner/NCBI_disease_label.txt')"
   ]
  },
  {
   "source": [
    "# Model interpretation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Device Being used as cpu \n",
      "\n",
      "Labels file exist\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]Model loaded successfully from the config provided.\n",
      "100%|| 8/8 [00:03<00:00,  2.32it/s][('Number of glucocorticoid receptors in lymphocytes and their sensitivity to '\n",
      "  'hormone action .',\n",
      "  [{'end_offset': 49,\n",
      "    'entity': 'lymphocytes',\n",
      "    'start_offset': 38,\n",
      "    'type': 'Disease'}]),\n",
      " ('The study demonstrated a decreased level of glucocorticoid receptors ( GR ) '\n",
      "  'in peripheral blood lymphocytes from hypercholesterolemic subjects , and an '\n",
      "  'elevated level in patients with acute myocardial infarction .',\n",
      "  [{'end_offset': 107,\n",
      "    'entity': 'lymphocytes',\n",
      "    'start_offset': 96,\n",
      "    'type': 'Disease'},\n",
      "   {'end_offset': 160,\n",
      "    'entity': 'elevated',\n",
      "    'start_offset': 152,\n",
      "    'type': 'Disease'}]),\n",
      " ('In the lymphocytes with a high GR number , dexamethasone inhibited [ 3H ] '\n",
      "  '-thymidine and [ 3H ] -acetate incorporation into DNA and cholesterol , '\n",
      "  'respectively , in the same manner as in the control cells .',\n",
      "  [{'end_offset': 18,\n",
      "    'entity': 'lymphocytes',\n",
      "    'start_offset': 7,\n",
      "    'type': 'Disease'}]),\n",
      " ('On the other hand , a decreased GR number resulted in a less efficient '\n",
      "  'dexamethasone inhibition of the incorporation of labeled compounds .',\n",
      "  [{'end_offset': 34, 'entity': 'GR', 'start_offset': 32, 'type': 'Disease'}]),\n",
      " ('hese data showed that the sensitivity of lymphocytes to glucocorticoids '\n",
      "  'changed only with a decrease of GR level .',\n",
      "  []),\n",
      " ('Treatment with I-hydroxyvitamin D3 ( 1-1.5 mg daily , within 4 weeks ) led '\n",
      "  'to normalization of total and ionized form of Ca2+ and of 25 ( OH ) D , but '\n",
      "  'did not affect the PTH content in blood .',\n",
      "  [{'end_offset': 45, 'entity': 'mg', 'start_offset': 43, 'type': 'Disease'},\n",
      "   {'end_offset': 60,\n",
      "    'entity': 'within',\n",
      "    'start_offset': 54,\n",
      "    'type': 'Disease'},\n",
      "   {'end_offset': 68,\n",
      "    'entity': 'weeks',\n",
      "    'start_offset': 63,\n",
      "    'type': 'Disease'}]),\n",
      " ('The data obtained suggest that under conditions of glomerulonephritis only '\n",
      "  'high content of receptors to 1.25 ( OH ) 2D3 in lymphocytes enabled to '\n",
      "  'perform the cell response to the hormone effect .',\n",
      "  [{'end_offset': 100,\n",
      "    'entity': 'receptors',\n",
      "    'start_offset': 91,\n",
      "    'type': 'Disease'}]),\n",
      " ('To investigate whether the tumor expression of beta-2-microglobulin ( beta '\n",
      "  '2-M ) could serve as a marker of tumor biologic behavior , the authors '\n",
      "  'studied specimens of breast carcinomas from 60 consecutive female patients '\n",
      "  '.',\n",
      "  []),\n",
      " ('Presence of beta 2-M was analyzed by immunohistochemistry .', []),\n",
      " ('I love data science', []),\n",
      " ('Humira showed better results than Cimzia for treating psoriasis .', []),\n",
      " ('Important advancements in the treatment of non - small cell lung cancer '\n",
      "  '(NSCLC) have been achieved over the past two decades, increasing our '\n",
      "  'understanding of the disease biology and mechanisms of tumour progression, '\n",
      "  'and advancing early detection and multimodal care .',\n",
      "  [{'end_offset': 125,\n",
      "    'entity': 'decades,',\n",
      "    'start_offset': 117,\n",
      "    'type': 'Disease'}]),\n",
      " ('The use of small molecule tyrosine kinase inhibitors and immunotherapy has '\n",
      "  'led to unprecedented survival benefits in selected patients .',\n",
      "  []),\n",
      " ('However, the overall cure and survival rates for NSCLC remain low, '\n",
      "  'particularly in metastatic disease .',\n",
      "  []),\n",
      " ('Therefore, continued research into new drugs and combination therapies is '\n",
      "  'required to expand the clinical benefit to a broader patient population and '\n",
      "  'to improve outcomes in NSCLC .',\n",
      "  []),\n",
      " ('The non-small cell lung cancer immune contexture. A major determinant of '\n",
      "  'tumor characteristics and patient outcome .',\n",
      "  [])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(\"Device Being used as {} \\n\".format(DEVICE))\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"inference_logs.txt\",\n",
    "    filemode=\"w\"\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "class NERTagger:\n",
    "    def __init__(\n",
    "        self, labels_file,\n",
    "        model_config_path, device\n",
    "    ):\n",
    "        self.model_config_path = model_config_path\n",
    "        self.labels_file = labels_file\n",
    "        self.device = device\n",
    "        if os.path.exists(self.model_config_path):\n",
    "            with open(self.model_config_path, \"r\", encoding=\"utf-8\") as reader:\n",
    "                text = reader.read()\n",
    "            self.model_config_dict = json.loads(text)\n",
    "        else:\n",
    "            print(\"model_config_path doesn't exist.\")\n",
    "            sys.exit()\n",
    "\n",
    "        if os.path.exists(self.model_config_dict[\"final_model_saving_dir\"]):\n",
    "            self.model_file = self.model_config_dict[\"final_model_saving_dir\"] + \"pytorch_model.bin\"\n",
    "            self.config_file = self.model_config_dict[\"final_model_saving_dir\"] + \"bert_config.json\"\n",
    "            self.vocab_file = self.model_config_dict[\"final_model_saving_dir\"] + \"vocab.txt\"\n",
    "        else:\n",
    "            print(\"model_saving_dir doesn't exist.\")\n",
    "            sys.exit()\n",
    "        if os.path.exists(self.labels_file):\n",
    "            print(\"Labels file exist\")\n",
    "        else:\n",
    "            print(\"labels_file doesn't exist.\")\n",
    "            sys.exit()\n",
    "\n",
    "        self.bert_config = BertConfig.from_json_file(self.config_file)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "            self.vocab_file,\n",
    "            config=self.bert_config,\n",
    "            do_lower_case=self.model_config_dict[\"tokenizer_do_lower_case\"]\n",
    "        )\n",
    "        self.labels = get_labels(self.labels_file)\n",
    "        self.label2idx = {l: i for i, l in enumerate(self.labels)}\n",
    "\n",
    "\n",
    "        if self.model_config_dict[\"model_type\"] == \"crf\":\n",
    "            self.model = BertCrfForNER.from_pretrained(\n",
    "                self.model_file,\n",
    "                config=self.bert_config,\n",
    "                pad_idx=self.bert_tokenizer.pad_token_id,\n",
    "                sep_idx=self.bert_tokenizer.sep_token_id,\n",
    "                num_labels=len(self.labels)\n",
    "            )\n",
    "        elif self.model_config_dict[\"model_type\"] == \"token_classification\":\n",
    "            self.model = BertForTokenClassification.from_pretrained(\n",
    "                self.model_file,\n",
    "                config=self.bert_config,\n",
    "                num_labels=len(self.labels),\n",
    "                classification_layer_sizes=self.model_config_dict[\"classification_layer_sizes\"]\n",
    "            )\n",
    "        elif  self.model_config_dict[\"model_type\"] == \"lstm_crf\":\n",
    "            self.model = BertLstmCrf.from_pretrained(\n",
    "                self.model_file,\n",
    "                config=self.bert_config,\n",
    "                num_labels=len(self.labels),\n",
    "                pad_idx=self.bert_tokenizer.pad_token_id,\n",
    "                lstm_hidden_dim=self.model_config_dict[\"lstm_hidden_dim\"],\n",
    "                num_lstm_layers=self.model_config_dict[\"num_lstm_layers\"],\n",
    "                bidirectional=self.model_config_dict[\"bidirectional\"]\n",
    "            )\n",
    "        self.model.to(self.device)\n",
    "        print(\"Model loaded successfully from the config provided.\")\n",
    "\n",
    "    def tag_sentences(self, sentence_list, logger, batch_size):\n",
    "        dataset, examples, features = load_and_cache_examples(\n",
    "            max_seq_length=self.model_config_dict[\"max_seq_length\"],\n",
    "            tokenizer=self.bert_tokenizer,\n",
    "            label_map=self.label2idx,\n",
    "            pad_token_label_id=self.label2idx[\"O\"],\n",
    "            mode=\"inference\", data_dir=None,\n",
    "            logger=logger, sentence_list=sentence_list,\n",
    "            return_features_and_examples=True\n",
    "        )\n",
    "\n",
    "        label_predictions = predictions_from_model(\n",
    "            model=self.model, tokenizer=self.bert_tokenizer,\n",
    "            dataset=dataset, batch_size=batch_size,\n",
    "            label2idx=self.label2idx, device=self.device\n",
    "        )\n",
    "        # restructure test_label_predictions with real labels\n",
    "        aligned_predicted_labels, _ = align_predicted_labels_with_original_sentence_tokens(\n",
    "            label_predictions, examples, features,\n",
    "            max_seq_length=self.model_config_dict[\"max_seq_length\"],\n",
    "            num_special_tokens=self.model_config_dict[\"num_special_tokens\"]\n",
    "        )\n",
    "        results = []\n",
    "        for label_tags, example in zip(aligned_predicted_labels, examples):\n",
    "            results.append(\n",
    "                convert_to_ents(example.words, label_tags)\n",
    "            )\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentence_list = [\n",
    "        \"Number of glucocorticoid receptors in lymphocytes and their sensitivity to hormone action .\",\n",
    "        \"The study demonstrated a decreased level of glucocorticoid receptors ( GR ) in peripheral blood lymphocytes from hypercholesterolemic subjects , and an elevated level in patients with acute myocardial infarction .\",\n",
    "        \"In the lymphocytes with a high GR number , dexamethasone inhibited [ 3H ] -thymidine and [ 3H ] -acetate incorporation into DNA and cholesterol , respectively , in the same manner as in the control cells .\",\n",
    "        \"On the other hand , a decreased GR number resulted in a less efficient dexamethasone inhibition of the incorporation of labeled compounds .\",\n",
    "        \"hese data showed that the sensitivity of lymphocytes to glucocorticoids changed only with a decrease of GR level .\",\n",
    "        \"Treatment with I-hydroxyvitamin D3 ( 1-1.5 mg daily , within 4 weeks ) led to normalization of total and ionized form of Ca2+ and of 25 ( OH ) D , but did not affect the PTH content in blood .\",\n",
    "        \"The data obtained suggest that under conditions of glomerulonephritis only high content of receptors to 1.25 ( OH ) 2D3 in lymphocytes enabled to perform the cell response to the hormone effect .\",\n",
    "        \"To investigate whether the tumor expression of beta-2-microglobulin ( beta 2-M ) could serve as a marker of tumor biologic behavior , the authors studied specimens of breast carcinomas from 60 consecutive female patients .\",\n",
    "        \"Presence of beta 2-M was analyzed by immunohistochemistry .\",\n",
    "        \"I love data science\",\n",
    "        \"Humira showed better results than Cimzia for treating psoriasis .\",\n",
    "        \"Important advancements in the treatment of non - small cell lung cancer (NSCLC) have been achieved over the past two decades, increasing our understanding of the disease biology and mechanisms of tumour progression, and advancing early detection and multimodal care .\",\n",
    "        \"The use of small molecule tyrosine kinase inhibitors and immunotherapy has led to unprecedented survival benefits in selected patients .\",\n",
    "        \"However, the overall cure and survival rates for NSCLC remain low, particularly in metastatic disease .\",\n",
    "        \"Therefore, continued research into new drugs and combination therapies is required to expand the clinical benefit to a broader patient population and to improve outcomes in NSCLC .\",\n",
    "        \"The non-small cell lung cancer immune contexture. A major determinant of tumor characteristics and patient outcome .\"\n",
    "    ]\n",
    "\n",
    "    tagger = NERTagger(\n",
    "        labels_file=\"/Users/sdeshpande/Desktop/text_analysis_scripts/biomedical_bert_ner/NCBI_disease_label.txt\",\n",
    "        model_config_path=\"/Users/sdeshpande/Desktop/text_analysis_scripts/biomedical_bert_ner/crf_ner_config.json\",\n",
    "        device=DEVICE\n",
    "    )\n",
    "    pprint(tagger.tag_sentences(sentence_list, logger=logger, batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['O', 'B-Disease', 'I-Disease']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}