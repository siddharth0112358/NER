{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Franck-Dernoncourt/NeuroNER/tree/master/neuroner/data/conll2003/en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Louis-udm/NER-BERT-CRF/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # #\n",
    "# NER_BERT_CRF.py\n",
    "# @author Zhibin.LU\n",
    "# @created Fri Feb 15 2019 22:47:19 GMT-0500 (EST)\n",
    "# @last-modified Sun Mar 31 2019 12:17:08 GMT-0400 (EDT)\n",
    "# @website: https://louis-udm.github.io\n",
    "# @description: Bert pytorch pretrainde model with or without CRF for NER\n",
    "# The NER_BERT_CRF.py include 2 model:\n",
    "# - model 1:\n",
    "#   - This is just a pretrained BertForTokenClassification, For a comparision with my BERT-CRF model\n",
    "# - model 2:\n",
    "#   - A pretrained BERT with CRF model.\n",
    "# - data set\n",
    "#   - [CoNLL-2003](https://github.com/FuYanzhe2/Name-Entity-Recognition/tree/master/BERT-BiLSTM-CRF-NER/NERdata)\n",
    "# # # #\n",
    "\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils import data\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import collections\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertForTokenClassification, BertLayerNorm\n",
    "import pickle\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version  3.8.3 (default, Jul  2 2020, 11:26:31) \n",
      "[Clang 10.0.0 ]\n",
      "PyTorch version  1.6.0\n",
      "Current dir: /Users/sdeshpande/Desktop/text_analysis_scripts\n"
     ]
    }
   ],
   "source": [
    "print('Python version ', sys.version)\n",
    "print('PyTorch version ', torch.__version__)\n",
    "\n",
    "#set_work_dir()\n",
    "print('Current dir:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available? False\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "cuda_yes = torch.cuda.is_available()\n",
    "# cuda_yes = False\n",
    "print('Cuda is available?', cuda_yes)\n",
    "device = torch.device(\"cuda:0\" if cuda_yes else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/sdeshpande/Desktop/text_analysis_scripts/conll2003/en/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Whether to run training.\"\n",
    "do_train = True\n",
    "# \"Whether to run eval on the dev set.\"\n",
    "do_eval = True\n",
    "# \"Whether to run the model in inference mode on the test set.\"\n",
    "do_predict = True\n",
    "# Whether load checkpoint file before train model\n",
    "load_checkpoint = True\n",
    "# \"The vocabulary file that the BERT model was trained on.\"\n",
    "max_seq_length = 180 #256\n",
    "batch_size = 32 #32\n",
    "# \"The initial learning rate for Adam.\"\n",
    "learning_rate0 = 5e-5\n",
    "lr0_crf_fc = 8e-5\n",
    "weight_decay_finetune = 1e-5 #0.01\n",
    "weight_decay_crf_fc = 5e-6 #0.005\n",
    "total_train_epochs = 3\n",
    "gradient_accumulation_steps = 1\n",
    "warmup_proportion = 0.1\n",
    "output_dir = './bert_output/'\n",
    "bert_model_scale = 'bert-base-cased'\n",
    "do_lower_case = False\n",
    "# eval_batch_size = 8\n",
    "# predict_batch_size = 8\n",
    "# \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "# \"E.g., 0.1 = 10% of training.\"\n",
    "# warmup_proportion = 0.1\n",
    "# \"How often to save the model checkpoint.\"\n",
    "# save_checkpoints_steps = 1000\n",
    "# \"How many steps to make in each estimator call.\"\n",
    "# iterations_per_loop = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "'''\n",
    "Functions and Classes for read and organize data set\n",
    "'''\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for NER.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          guid: Unique id for the example(a sentence or a pair of sentences).\n",
    "          words: list of words of sentence\n",
    "          labels_a/labels_b: (Optional) string. The label seqence of the text_a/text_b. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        # list of words of the sentence,example: [EU, rejects, German, call, to, boycott, British, lamb .]\n",
    "        self.words = words\n",
    "        # list of label sequence of the sentence,like: [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]\n",
    "        self.labels = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\n",
    "    result of convert_examples_to_features(InputExample)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids,  predict_mask, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.predict_mask = predict_mask\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"\n",
    "        Reads a BIO data.\n",
    "        \"\"\"\n",
    "        with open(input_file) as f:\n",
    "            # out_lines = []\n",
    "            out_lists = []\n",
    "            entries = f.read().strip().split(\"\\n\\n\")\n",
    "            for entry in entries:\n",
    "                words = []\n",
    "                ner_labels = []\n",
    "                pos_tags = []\n",
    "                bio_pos_tags = []\n",
    "                for line in entry.splitlines():\n",
    "                    pieces = line.strip().split()\n",
    "                    if len(pieces) < 1:\n",
    "                        continue\n",
    "                    word = pieces[0]\n",
    "                    # if word == \"-DOCSTART-\" or word == '':\n",
    "                    #     continue\n",
    "                    words.append(word)\n",
    "                    pos_tags.append(pieces[1])\n",
    "                    bio_pos_tags.append(pieces[2])\n",
    "                    ner_labels.append(pieces[-1])\n",
    "                # sentence = ' '.join(words)\n",
    "                # ner_seq = ' '.join(ner_labels)\n",
    "                # pos_tag_seq = ' '.join(pos_tags) \n",
    "                # bio_pos_tag_seq = ' '.join(bio_pos_tags) \n",
    "                # out_lines.append([sentence, pos_tag_seq, bio_pos_tag_seq, ner_seq])\n",
    "                # out_lines.append([sentence, ner_seq])\n",
    "                out_lists.append([words,pos_tags,bio_pos_tags,ner_labels])\n",
    "        return out_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLDataProcessor(DataProcessor):\n",
    "    '''\n",
    "    CoNLL-2003\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self._label_types = [ 'X', '[CLS]', '[SEP]', 'O', 'I-LOC', 'B-PER', 'I-PER', 'I-ORG', 'I-MISC', 'B-MISC', 'B-LOC', 'B-ORG']\n",
    "        self._num_labels = len(self._label_types)\n",
    "        self._label_map = {label: i for i,\n",
    "                           label in enumerate(self._label_types)}\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"train.txt\")))\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"valid.txt\")))\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_data(os.path.join(data_dir, \"test.txt\")))\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._label_types\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return self.get_num_labels\n",
    "\n",
    "    def get_label_map(self):\n",
    "        return self._label_map\n",
    "    \n",
    "    def get_start_label_id(self):\n",
    "        return self._label_map['[CLS]']\n",
    "\n",
    "    def get_stop_label_id(self):\n",
    "        return self._label_map['[SEP]']\n",
    "\n",
    "    def _create_examples(self, all_lists):\n",
    "        examples = []\n",
    "        for (i, one_lists) in enumerate(all_lists):\n",
    "            guid = i\n",
    "            words = one_lists[0]\n",
    "            labels = one_lists[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, words=words, labels=labels))\n",
    "        return examples\n",
    "\n",
    "    def _create_examples2(self, lines):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = i\n",
    "            text = line[0]\n",
    "            ner_label = line[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, text_a=text, labels_a=ner_label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example2feature(example, tokenizer, label_map, max_seq_length):\n",
    "\n",
    "    add_label = 'X'\n",
    "    # tokenize_count = []\n",
    "    tokens = ['[CLS]']\n",
    "    predict_mask = [0]\n",
    "    label_ids = [label_map['[CLS]']]\n",
    "    for i, w in enumerate(example.words):\n",
    "        # use bertTokenizer to split words\n",
    "        # 1996-08-22 => 1996 - 08 - 22\n",
    "        # sheepmeat => sheep ##me ##at\n",
    "        sub_words = tokenizer.tokenize(w)\n",
    "        if not sub_words:\n",
    "            sub_words = ['[UNK]']\n",
    "        # tokenize_count.append(len(sub_words))\n",
    "        tokens.extend(sub_words)\n",
    "        for j in range(len(sub_words)):\n",
    "            if j == 0:\n",
    "                predict_mask.append(1)\n",
    "                label_ids.append(label_map[example.labels[i]])\n",
    "            else:\n",
    "                # '##xxx' -> 'X' (see bert paper)\n",
    "                predict_mask.append(0)\n",
    "                label_ids.append(label_map[add_label])\n",
    "\n",
    "    # truncate\n",
    "    if len(tokens) > max_seq_length - 1:\n",
    "        print('Example No.{} is too long, length is {}, truncated to {}!'.format(example.guid, len(tokens), max_seq_length))\n",
    "        tokens = tokens[0:(max_seq_length - 1)]\n",
    "        predict_mask = predict_mask[0:(max_seq_length - 1)]\n",
    "        label_ids = label_ids[0:(max_seq_length - 1)]\n",
    "    tokens.append('[SEP]')\n",
    "    predict_mask.append(0)\n",
    "    label_ids.append(label_map['[SEP]'])\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    feat=InputFeatures(\n",
    "                # guid=example.guid,\n",
    "                # tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                predict_mask=predict_mask,\n",
    "                label_ids=label_ids)\n",
    "\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(data.Dataset):\n",
    "    def __init__(self, examples, tokenizer, label_map, max_seq_length):\n",
    "        self.examples=examples\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label_map=label_map\n",
    "        self.max_seq_length=max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat=example2feature(self.examples[idx], self.tokenizer, self.label_map, max_seq_length)\n",
    "        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.predict_mask, feat.label_ids\n",
    "\n",
    "    @classmethod\n",
    "    def pad(cls, batch):\n",
    "\n",
    "        seqlen_list = [len(sample[0]) for sample in batch]\n",
    "        maxlen = np.array(seqlen_list).max()\n",
    "\n",
    "        f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: X for padding\n",
    "        input_ids_list = torch.LongTensor(f(0, maxlen))\n",
    "        input_mask_list = torch.LongTensor(f(1, maxlen))\n",
    "        segment_ids_list = torch.LongTensor(f(2, maxlen))\n",
    "        predict_mask_list = torch.ByteTensor(f(3, maxlen))\n",
    "        label_ids_list = torch.LongTensor(f(4, maxlen))\n",
    "\n",
    "        return input_ids_list, input_mask_list, segment_ids_list, predict_mask_list, label_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    '''\n",
    "    0,1,2,3 are [CLS],[SEP],[X],O\n",
    "    '''\n",
    "    ignore_id=3\n",
    "    \n",
    "    num_proposed = len(y_pred[y_pred>ignore_id])\n",
    "    num_correct = (np.logical_and(y_true==y_pred, y_true>ignore_id)).sum()\n",
    "    num_gold = len(y_true[y_true>ignore_id])\n",
    "\n",
    "    try:\n",
    "        precision = num_correct / num_proposed\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1.0\n",
    "\n",
    "    try:\n",
    "        recall = num_correct / num_gold\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1.0\n",
    "\n",
    "    try:\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        if precision*recall==0:\n",
    "            f1=1.0\n",
    "        else:\n",
    "            f1=0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "'''\n",
    "Prepare data set\n",
    "'''\n",
    "# random.seed(44)\n",
    "np.random.seed(44)\n",
    "torch.manual_seed(44)\n",
    "if cuda_yes:\n",
    "    torch.cuda.manual_seed_all(44)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "conllProcessor = CoNLLDataProcessor()\n",
    "label_list = conllProcessor.get_labels()\n",
    "label_map = conllProcessor.get_label_map()\n",
    "train_examples = conllProcessor.get_train_examples(data_dir)\n",
    "dev_examples = conllProcessor.get_dev_examples(data_dir)\n",
    "test_examples = conllProcessor.get_test_examples(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 14987\n",
      "  Batch size = 32\n",
      "  Num steps = 1405\n"
     ]
    }
   ],
   "source": [
    "total_train_steps = int(len(train_examples) / batch_size / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"% len(train_examples))\n",
    "print(\"  Batch size = %d\"% batch_size)\n",
    "print(\"  Num steps = %d\"% total_train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)\n",
    "\n",
    "train_dataset = NerDataset(train_examples,tokenizer,label_map,max_seq_length)\n",
    "dev_dataset = NerDataset(dev_examples,tokenizer,label_map,max_seq_length)\n",
    "test_dataset = NerDataset(test_examples,tokenizer,label_map,max_seq_length)\n",
    "\n",
    "train_dataloader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "dev_dataloader = data.DataLoader(dataset=dev_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=NerDataset.pad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Use only BertForTokenClassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Use only BertForTokenClassification ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "'''\n",
    "#####  Use only BertForTokenClassification  #####\n",
    "'''\n",
    "print('*** Use only BertForTokenClassification ***')\n",
    "\n",
    "if load_checkpoint and os.path.exists(output_dir+'/ner_bert_checkpoint.pt'):\n",
    "    checkpoint = torch.load(output_dir+'/ner_bert_checkpoint.pt', map_location='cpu')\n",
    "    start_epoch = checkpoint['epoch']+1\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    valid_f1_prev = checkpoint['valid_f1']\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        bert_model_scale, state_dict=checkpoint['model_state'], num_labels=len(label_list))\n",
    "    print('Loaded the pretrain NER_BERT model, epoch:',checkpoint['epoch'],'valid acc:', \n",
    "            checkpoint['valid_acc'], 'valid f1:', checkpoint['valid_f1'])\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    valid_acc_prev = 0\n",
    "    valid_f1_prev = 0\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        bert_model_scale, num_labels=len(label_list))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "named_params = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay_finetune},\n",
    "    {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate0, warmup=warmup_proportion, t_total=total_train_steps)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, predict_dataloader, batch_size, epoch_th, dataset_name):\n",
    "    # print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total=0\n",
    "    correct=0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in predict_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "            out_scores = model(input_ids, segment_ids, input_mask)\n",
    "            # out_scores = out_scores.detach().cpu().numpy()\n",
    "            _, predicted = torch.max(out_scores, -1)\n",
    "            valid_predicted = torch.masked_select(predicted, predict_mask)\n",
    "            valid_label_ids = torch.masked_select(label_ids, predict_mask)\n",
    "            # print(len(valid_label_ids),len(valid_predicted),len(valid_label_ids)==len(valid_predicted))\n",
    "            all_preds.extend(valid_predicted.tolist())\n",
    "            all_labels.extend(valid_label_ids.tolist())\n",
    "            total += len(valid_label_ids)\n",
    "            correct += valid_predicted.eq(valid_label_ids).sum().item()\n",
    "\n",
    "    test_acc = correct/total\n",
    "    precision, recall, f1 = f1_score(np.array(all_labels), np.array(all_preds))\n",
    "    end = time.time()\n",
    "    print('Epoch:%d, Acc:%.2f, Precision: %.2f, Recall: %.2f, F1: %.2f on %s, Spend: %.3f minutes for evaluation' \\\n",
    "        % (epoch_th, 100.*test_acc, 100.*precision, 100.*recall, 100.*f1, dataset_name,(end-start)/60.0))\n",
    "    print('--------------------------------------------------------------')\n",
    "    return test_acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# train procedure using only BertForTokenClassification\n",
    "# train_start = time.time()\n",
    "global_step_th = int(len(train_examples) / batch_size / gradient_accumulation_steps * start_epoch)\n",
    "# for epoch in trange(start_epoch, total_train_epochs, desc=\"Epoch\"):\n",
    "for epoch in range(start_epoch, total_train_epochs):\n",
    "    tr_loss = 0\n",
    "    train_start = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "        loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = learning_rate0 * warmup_linear(global_step_th/total_train_steps, warmup_proportion)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step_th += 1\n",
    "          \n",
    "        print(\"Epoch:{}-{}/{}, CrossEntropyLoss: {} \".format(epoch, step, len(train_dataloader), loss.item()))\n",
    "    \n",
    "    print('--------------------------------------------------------------')\n",
    "    print(\"Epoch:{} completed, Total training's Loss: {}, Spend: {}m\".format(epoch, tr_loss, (time.time() - train_start)/60.0))\n",
    "    valid_acc, valid_f1 = evaluate(model, dev_dataloader, batch_size, epoch, 'Valid_set')\n",
    "    # Save a checkpoint\n",
    "    if valid_f1 > valid_f1_prev:\n",
    "        # model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'valid_acc': valid_acc,\n",
    "            'valid_f1': valid_f1, 'max_seq_length': max_seq_length, 'lower_case': do_lower_case},\n",
    "                    os.path.join(output_dir, 'ner_bert_checkpoint.pt'))\n",
    "        valid_f1_prev = valid_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_dataloader, batch_size, total_train_epochs-1, 'Test_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use BertModel + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "'''\n",
    "#####  Use BertModel + CRF  #####\n",
    "CRF is for transition and the maximum likelyhood estimate(MLE).\n",
    "Bert is for latent label -> Emission of word embedding.\n",
    "'''\n",
    "print('*** Use BertModel + CRF ***')\n",
    "\n",
    "def log_sum_exp_1vec(vec):  # shape(1,m)\n",
    "    max_score = vec[0, np.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n",
    "    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n",
    "\n",
    "def log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n",
    "    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_CRF_NER(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n",
    "        super(BERT_CRF_NER, self).__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.start_label_id = start_label_id\n",
    "        self.stop_label_id = stop_label_id\n",
    "        self.num_labels = num_labels\n",
    "        # self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device=device\n",
    "\n",
    "        # use pretrainded BertModel \n",
    "        self.bert = bert_model\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        # Maps the output of the bert into label space.\n",
    "        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.num_labels, self.num_labels))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n",
    "        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n",
    "        # so this enforcement is likely unimportant)\n",
    "        self.transitions.data[start_label_id, :] = -10000\n",
    "        self.transitions.data[:, stop_label_id] = -10000\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hidden2label.weight)\n",
    "        nn.init.constant_(self.hidden2label.bias, 0.0)\n",
    "        # self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)): \n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        '''\n",
    "        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX \n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "        \n",
    "        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n",
    "        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n",
    "        # self.start_label has all of the score. it is log,0 is p=1\n",
    "        log_alpha[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        # feats is the probability of emission, feat.shape=(1,tag_size)\n",
    "        for t in range(1, T):\n",
    "            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # log_prob of all barX\n",
    "        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n",
    "        return log_prob_all_barX\n",
    "\n",
    "    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n",
    "        '''\n",
    "        sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        '''\n",
    "        bert_seq_out, _ = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask, output_all_encoded_layers=False)\n",
    "        bert_seq_out = self.dropout(bert_seq_out)\n",
    "        bert_feats = self.hidden2label(bert_seq_out)\n",
    "        return bert_feats\n",
    "\n",
    "    def _score_sentence(self, feats, label_ids):\n",
    "        ''' \n",
    "        Gives the score of a provided label sequence\n",
    "        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "        batch_transitions = batch_transitions.flatten(1)\n",
    "\n",
    "        score = torch.zeros((feats.shape[0],1)).to(device)\n",
    "        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n",
    "        for t in range(1, T):\n",
    "            score = score + \\\n",
    "                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n",
    "                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        '''\n",
    "        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "\n",
    "        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        log_delta[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n",
    "        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n",
    "        for t in range(1, T):\n",
    "            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n",
    "            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n",
    "            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n",
    "            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # trace back\n",
    "        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n",
    "\n",
    "        # max p(z1:t,all_x|theta)\n",
    "        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n",
    "\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # choose the state of z_t according the state choosed of z_t+1.\n",
    "            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n",
    "\n",
    "        return max_logLL_allz_allx, path\n",
    "\n",
    "    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "        forward_score = self._forward_alg(bert_feats)\n",
    "        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        gold_score = self._score_sentence(bert_feats, label_ids)\n",
    "        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n",
    "        return torch.mean(forward_score - gold_score)\n",
    "\n",
    "    # this forward is just for predict, not for train\n",
    "    # dont confuse this with _forward_alg above.\n",
    "    def forward(self, input_ids, segment_ids, input_mask):\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, label_seq_ids = self._viterbi_decode(bert_feats)\n",
    "        return score, label_seq_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_label_id = conllProcessor.get_start_label_id()\n",
    "stop_label_id = conllProcessor.get_stop_label_id()\n",
    "\n",
    "bert_model = BertModel.from_pretrained(bert_model_scale)\n",
    "model = BERT_CRF_NER(bert_model, start_label_id, stop_label_id, len(label_list), max_seq_length, batch_size, device)\n",
    "\n",
    "#%%\n",
    "if load_checkpoint and os.path.exists(output_dir+'/ner_bert_crf_checkpoint.pt'):\n",
    "    checkpoint = torch.load(output_dir+'/ner_bert_crf_checkpoint.pt', map_location='cpu')\n",
    "    start_epoch = checkpoint['epoch']+1\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    valid_f1_prev = checkpoint['valid_f1']\n",
    "    pretrained_dict=checkpoint['model_state']\n",
    "    net_state_dict = model.state_dict()\n",
    "    pretrained_dict_selected = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(pretrained_dict_selected)\n",
    "    model.load_state_dict(net_state_dict)\n",
    "    print('Loaded the pretrain NER_BERT_CRF model, epoch:',checkpoint['epoch'],'valid acc:', \n",
    "            checkpoint['valid_acc'], 'valid f1:', checkpoint['valid_f1'])\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    valid_acc_prev = 0\n",
    "    valid_f1_prev = 0\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "new_param = ['transitions', 'hidden2label.weight', 'hidden2label.bias']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': weight_decay_finetune},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in param_optimizer if n in ('transitions','hidden2label.weight')] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': weight_decay_crf_fc},\n",
    "    {'params': [p for n, p in param_optimizer if n == 'hidden2label.bias'] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate0, warmup=warmup_proportion, t_total=total_train_steps)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate0)\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, predict_dataloader, batch_size, epoch_th, dataset_name):\n",
    "    # print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total=0\n",
    "    correct=0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in predict_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "            _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n",
    "            # _, predicted = torch.max(out_scores, -1)\n",
    "            valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
    "            valid_label_ids = torch.masked_select(label_ids, predict_mask)\n",
    "            all_preds.extend(valid_predicted.tolist())\n",
    "            all_labels.extend(valid_label_ids.tolist())\n",
    "            # print(len(valid_label_ids),len(valid_predicted),len(valid_label_ids)==len(valid_predicted))\n",
    "            total += len(valid_label_ids)\n",
    "            correct += valid_predicted.eq(valid_label_ids).sum().item()\n",
    "\n",
    "    test_acc = correct/total\n",
    "    precision, recall, f1 = f1_score(np.array(all_labels), np.array(all_preds))\n",
    "    end = time.time()\n",
    "    print('Epoch:%d, Acc:%.2f, Precision: %.2f, Recall: %.2f, F1: %.2f on %s, Spend:%.3f minutes for evaluation' \\\n",
    "        % (epoch_th, 100.*test_acc, 100.*precision, 100.*recall, 100.*f1, dataset_name,(end-start)/60.0))\n",
    "    print('--------------------------------------------------------------')\n",
    "    return test_acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# train procedure\n",
    "global_step_th = int(len(train_examples) / batch_size / gradient_accumulation_steps * start_epoch)\n",
    "\n",
    "# train_start=time.time()\n",
    "# for epoch in trange(start_epoch, total_train_epochs, desc=\"Epoch\"):\n",
    "for epoch in range(start_epoch, total_train_epochs):\n",
    "    tr_loss = 0\n",
    "    train_start = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "\n",
    "        neg_log_likelihood = model.neg_log_likelihood(input_ids, segment_ids, input_mask, label_ids)\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            neg_log_likelihood = neg_log_likelihood / gradient_accumulation_steps\n",
    "\n",
    "        neg_log_likelihood.backward()\n",
    "\n",
    "        tr_loss += neg_log_likelihood.item()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = learning_rate0 * warmup_linear(global_step_th/total_train_steps, warmup_proportion)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step_th += 1\n",
    "            \n",
    "        print(\"Epoch:{}-{}/{}, Negative loglikelihood: {} \".format(epoch, step, len(train_dataloader), neg_log_likelihood.item()))\n",
    "    \n",
    "    print('--------------------------------------------------------------')\n",
    "    print(\"Epoch:{} completed, Total training's Loss: {}, Spend: {}m\".format(epoch, tr_loss, (time.time() - train_start)/60.0))\n",
    "    valid_acc, valid_f1 = evaluate(model, dev_dataloader, batch_size, epoch, 'Valid_set')\n",
    "\n",
    "    # Save a checkpoint\n",
    "    if valid_f1 > valid_f1_prev:\n",
    "        # model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'valid_acc': valid_acc,\n",
    "            'valid_f1': valid_f1, 'max_seq_length': max_seq_length, 'lower_case': do_lower_case},\n",
    "                    os.path.join(output_dir, 'ner_bert_crf_checkpoint.pt'))\n",
    "        valid_f1_prev = valid_f1\n",
    "\n",
    "evaluate(model, test_dataloader, batch_size, total_train_epochs-1, 'Test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "'''\n",
    "Test_set prediction using the best epoch of NER_BERT_CRF model\n",
    "'''\n",
    "checkpoint = torch.load(output_dir+'/ner_bert_crf_checkpoint.pt', map_location='cpu')\n",
    "epoch = checkpoint['epoch']\n",
    "valid_acc_prev = checkpoint['valid_acc']\n",
    "valid_f1_prev = checkpoint['valid_f1']\n",
    "pretrained_dict=checkpoint['model_state']\n",
    "net_state_dict = model.state_dict()\n",
    "pretrained_dict_selected = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "net_state_dict.update(pretrained_dict_selected)\n",
    "model.load_state_dict(net_state_dict)\n",
    "print('Loaded the pretrain  NER_BERT_CRF  model, epoch:',checkpoint['epoch'],'valid acc:', \n",
    "      checkpoint['valid_acc'], 'valid f1:', checkpoint['valid_f1'])\n",
    "\n",
    "model.to(device)\n",
    "#evaluate(model, train_dataloader, batch_size, total_train_epochs-1, 'Train_set')\n",
    "evaluate(model, test_dataloader, batch_size, epoch, 'Test_set')\n",
    "# print('Total spend:',(time.time()-train_start)/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    demon_dataloader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=10,\n",
    "                                shuffle=False,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=pad)\n",
    "    for batch in demon_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "        _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n",
    "        # _, predicted = torch.max(out_scores, -1)\n",
    "        valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n",
    "        # valid_label_ids = torch.masked_select(label_ids, predict_mask)\n",
    "        for i in range(10):\n",
    "            print(predicted_label_seq_ids[i])\n",
    "            print(label_ids[i])\n",
    "            new_ids=predicted_label_seq_ids[i].cpu().numpy()[predict_mask[i].cpu().numpy()==1]\n",
    "            print(list(map(lambda i: label_list[i], new_ids)))\n",
    "            print(test_examples[i].labels)\n",
    "        break\n",
    "#%%\n",
    "print(conllProcessor.get_label_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
