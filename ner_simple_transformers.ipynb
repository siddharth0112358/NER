{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "17f60a5376c77425fc92555942c601d199d1b6f4d9fe48b17446cfeffaeccd2f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NER utilities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "https://github.com/ThilinaRajapakse/simpletransformers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Named entity recognition fine-tuning: utilities to work with CoNLL-2003 task. \"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import linecache\n",
    "import logging\n",
    "import os\n",
    "from io import open\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.functional import split\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels, x0=None, y0=None, x1=None, y1=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "            x0: (Optional) list. The list of x0 coordinates for each word.\n",
    "            y0: (Optional) list. The list of y0 coordinates for each word.\n",
    "            x1: (Optional) list. The list of x1 coordinates for each word.\n",
    "            y1: (Optional) list. The list of y1 coordinates for each word.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "        if x0 is None:\n",
    "            self.bboxes = None\n",
    "        else:\n",
    "            self.bboxes = [[a, b, c, d] for a, b, c, d in zip(x0, y0, x1, y1)]\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, bboxes=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        if bboxes:\n",
    "            self.bboxes = bboxes\n",
    "\n",
    "\n",
    "def read_examples_from_file(data_file, mode, bbox=False):\n",
    "    file_path = data_file\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if bbox:\n",
    "                if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                    if words:\n",
    "                        examples.append(\n",
    "                            InputExample(\n",
    "                                guid=\"{}-{}\".format(mode, guid_index),\n",
    "                                words=words,\n",
    "                                labels=labels,\n",
    "                                x0=x0,\n",
    "                                y0=y0,\n",
    "                                x1=x1,\n",
    "                                y1=y1,\n",
    "                            )\n",
    "                        )\n",
    "                        guid_index += 1\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                        x0 = []\n",
    "                        y0 = []\n",
    "                        x1 = []\n",
    "                        y1 = []\n",
    "                else:\n",
    "                    splits = line.split(\" \")\n",
    "                    words.append(splits[0])\n",
    "                    if len(splits) > 1:\n",
    "                        labels.append(splits[1].replace(\"\\n\", \"\"))\n",
    "                        x0.append(split[2].replace(\"\\n\", \"\"))\n",
    "                        y0.append(split[3].replace(\"\\n\", \"\"))\n",
    "                        x1.append(split[4].replace(\"\\n\", \"\"))\n",
    "                        y1.append(split[5].replace(\"\\n\", \"\"))\n",
    "                    else:\n",
    "                        # Examples could have no label for mode = \"test\"\n",
    "                        labels.append(\"O\")\n",
    "            else:\n",
    "                if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                    if words:\n",
    "                        examples.append(\n",
    "                            InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels)\n",
    "                        )\n",
    "                        guid_index += 1\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                else:\n",
    "                    splits = line.split(\" \")\n",
    "                    words.append(splits[0])\n",
    "                    if len(splits) > 1:\n",
    "                        labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "                    else:\n",
    "                        # Examples could have no label for mode = \"test\"\n",
    "                        labels.append(\"O\")\n",
    "        if words:\n",
    "            if bbox:\n",
    "                examples.append(\n",
    "                    InputExample(\n",
    "                        guid=\"%s-%d\".format(mode, guid_index), words=words, labels=labels, x0=x0, y0=y0, x1=x1, y1=y1\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                examples.append(InputExample(guid=\"%s-%d\".format(mode, guid_index), words=words, labels=labels))\n",
    "    return examples\n",
    "\n",
    "\n",
    "def get_examples_from_df(data, bbox=False):\n",
    "    if bbox:\n",
    "        return [\n",
    "            InputExample(\n",
    "                guid=sentence_id,\n",
    "                words=sentence_df[\"words\"].tolist(),\n",
    "                labels=sentence_df[\"labels\"].tolist(),\n",
    "                x0=sentence_df[\"x0\"].tolist(),\n",
    "                y0=sentence_df[\"y0\"].tolist(),\n",
    "                x1=sentence_df[\"x1\"].tolist(),\n",
    "                y1=sentence_df[\"y1\"].tolist(),\n",
    "            )\n",
    "            for sentence_id, sentence_df in data.groupby([\"sentence_id\"])\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            InputExample(guid=sentence_id, words=sentence_df[\"words\"].tolist(), labels=sentence_df[\"labels\"].tolist(),)\n",
    "            for sentence_id, sentence_df in data.groupby([\"sentence_id\"])\n",
    "        ]\n",
    "\n",
    "\n",
    "def convert_examples_with_multiprocessing(examples):\n",
    "    (\n",
    "        example_group,\n",
    "        label_map,\n",
    "        max_seq_length,\n",
    "        tokenizer,\n",
    "        cls_token_at_end,\n",
    "        cls_token,\n",
    "        cls_token_segment_id,\n",
    "        sep_token,\n",
    "        sep_token_extra,\n",
    "        pad_on_left,\n",
    "        pad_token,\n",
    "        pad_token_segment_id,\n",
    "        pad_token_label_id,\n",
    "        sequence_a_segment_id,\n",
    "        mask_padding_with_zero,\n",
    "    ) = examples\n",
    "\n",
    "    return [\n",
    "        convert_example_to_feature(\n",
    "            example,\n",
    "            label_map,\n",
    "            max_seq_length,\n",
    "            tokenizer,\n",
    "            cls_token_at_end,\n",
    "            cls_token,\n",
    "            cls_token_segment_id,\n",
    "            sep_token,\n",
    "            sep_token_extra,\n",
    "            pad_on_left,\n",
    "            pad_token,\n",
    "            pad_token_segment_id,\n",
    "            pad_token_label_id,\n",
    "            sequence_a_segment_id,\n",
    "            mask_padding_with_zero,\n",
    "        )\n",
    "        for example in example_group\n",
    "    ]\n",
    "\n",
    "\n",
    "def convert_example_to_feature(\n",
    "    example,\n",
    "    label_map,\n",
    "    max_seq_length,\n",
    "    tokenizer,\n",
    "    cls_token_at_end,\n",
    "    cls_token,\n",
    "    cls_token_segment_id,\n",
    "    sep_token,\n",
    "    sep_token_extra,\n",
    "    pad_on_left,\n",
    "    pad_token,\n",
    "    pad_token_segment_id,\n",
    "    pad_token_label_id,\n",
    "    sequence_a_segment_id,\n",
    "    mask_padding_with_zero,\n",
    "):\n",
    "    tokens = []\n",
    "    label_ids = []\n",
    "    bboxes = []\n",
    "    if example.bboxes:\n",
    "        for word, label, bbox in zip(example.words, example.labels, example.bboxes):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "            bboxes.extend([bbox] * len(word_tokens))\n",
    "\n",
    "        cls_token_box = [0, 0, 0, 0]\n",
    "        sep_token_box = [1000, 1000, 1000, 1000]\n",
    "        pad_token_box = [0, 0, 0, 0]\n",
    "\n",
    "    else:\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            if word_tokens:  # avoid non printable character like '\\u200e' which are tokenized as a void token ''\n",
    "                label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "    # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "    special_tokens_count = 3 if sep_token_extra else 2\n",
    "    if len(tokens) > max_seq_length - special_tokens_count:\n",
    "        tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "        label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "        if bboxes:\n",
    "            bboxes = bboxes[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids:   0   0   0   0  0     0   0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens += [sep_token]\n",
    "    label_ids += [pad_token_label_id]\n",
    "    if bboxes:\n",
    "        bboxes += [sep_token_box]\n",
    "    if sep_token_extra:\n",
    "        # roberta uses an extra separator b/w pairs of sentences\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        if bboxes:\n",
    "            bboxes += [sep_token_box]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    if cls_token_at_end:\n",
    "        tokens += [cls_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        segment_ids += [cls_token_segment_id]\n",
    "    else:\n",
    "        tokens = [cls_token] + tokens\n",
    "        label_ids = [pad_token_label_id] + label_ids\n",
    "        segment_ids = [cls_token_segment_id] + segment_ids\n",
    "        if bboxes:\n",
    "            bboxes = [cls_token_box] + bboxes\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "        label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "    else:\n",
    "        input_ids += [pad_token] * padding_length\n",
    "        input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        segment_ids += [pad_token_segment_id] * padding_length\n",
    "        label_ids += [pad_token_label_id] * padding_length\n",
    "        if bboxes:\n",
    "            bboxes += [pad_token_box] * padding_length\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(label_ids) == max_seq_length\n",
    "    if bboxes:\n",
    "        assert len(bboxes) == max_seq_length\n",
    "\n",
    "    if bboxes:\n",
    "        return InputFeatures(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids, bboxes=bboxes\n",
    "        )\n",
    "    else:\n",
    "        return InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids,)\n",
    "\n",
    "\n",
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    label_list,\n",
    "    max_seq_length,\n",
    "    tokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=\"[CLS]\",\n",
    "    cls_token_segment_id=1,\n",
    "    sep_token=\"[SEP]\",\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=-1,\n",
    "    sequence_a_segment_id=0,\n",
    "    mask_padding_with_zero=True,\n",
    "    process_count=cpu_count() - 2,\n",
    "    chunksize=500,\n",
    "    silent=False,\n",
    "    use_multiprocessing=True,\n",
    "    mode=\"dev\",\n",
    "    use_multiprocessing_for_evaluation=False,\n",
    "):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    if (mode == \"train\" and use_multiprocessing) or (mode == \"dev\" and use_multiprocessing_for_evaluation):\n",
    "        if chunksize == -1:\n",
    "            chunksize = max(len(examples) // (process_count * 2), 500)\n",
    "        examples = [\n",
    "            (\n",
    "                examples[i : i + chunksize],\n",
    "                label_map,\n",
    "                max_seq_length,\n",
    "                tokenizer,\n",
    "                cls_token_at_end,\n",
    "                cls_token,\n",
    "                cls_token_segment_id,\n",
    "                sep_token,\n",
    "                sep_token_extra,\n",
    "                pad_on_left,\n",
    "                pad_token,\n",
    "                pad_token_segment_id,\n",
    "                pad_token_label_id,\n",
    "                sequence_a_segment_id,\n",
    "                mask_padding_with_zero,\n",
    "            )\n",
    "            for i in range(0, len(examples), chunksize)\n",
    "        ]\n",
    "\n",
    "        with Pool(process_count) as p:\n",
    "            features = list(\n",
    "                tqdm(\n",
    "                    p.imap(convert_examples_with_multiprocessing, examples, chunksize=chunksize),\n",
    "                    total=len(examples),\n",
    "                    disable=silent,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            features = [feature for feature_group in features for feature in feature_group]\n",
    "    else:\n",
    "        features = [\n",
    "            convert_example_to_feature(\n",
    "                example,\n",
    "                label_map,\n",
    "                max_seq_length,\n",
    "                tokenizer,\n",
    "                cls_token_at_end,\n",
    "                cls_token,\n",
    "                cls_token_segment_id,\n",
    "                sep_token,\n",
    "                sep_token_extra,\n",
    "                pad_on_left,\n",
    "                pad_token,\n",
    "                pad_token_segment_id,\n",
    "                pad_token_label_id,\n",
    "                sequence_a_segment_id,\n",
    "                mask_padding_with_zero,\n",
    "            )\n",
    "            for example in tqdm(examples, disable=silent)\n",
    "        ]\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_labels(path):\n",
    "    if path:\n",
    "        with open(path, \"r\") as f:\n",
    "            labels = f.read().splitlines()\n",
    "        if \"O\" not in labels:\n",
    "            labels = [\"O\"] + labels\n",
    "        return labels\n",
    "    else:\n",
    "        return [\n",
    "            \"O\",\n",
    "            \"B-MISC\",\n",
    "            \"I-MISC\",\n",
    "            \"B-PER\",\n",
    "            \"I-PER\",\n",
    "            \"B-ORG\",\n",
    "            \"I-ORG\",\n",
    "            \"B-LOC\",\n",
    "            \"I-LOC\",\n",
    "        ]\n",
    "\n",
    "\n",
    "class LazyNERDataset(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, args):\n",
    "        self.data_file = data_file\n",
    "        self.lazy_loading_start_line = args.lazy_loading_start_line if args.lazy_loading_start_line else 0\n",
    "        self.example_lines, self.num_entries = self._get_examples(self.data_file, self.lazy_loading_start_line)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = args\n",
    "        self.pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_examples(data_file, lazy_loading_start_line):\n",
    "        example_lines = {}\n",
    "        start = lazy_loading_start_line\n",
    "        entry_num = 0\n",
    "        with open(data_file, encoding=\"utf-8\") as f:\n",
    "            for line_idx, _ in enumerate(f, 1):\n",
    "                if _ == \"\\n\" and line_idx > lazy_loading_start_line:\n",
    "                    example_lines[entry_num] = (start, line_idx)\n",
    "                    start = line_idx + 1\n",
    "                    entry_num += 1\n",
    "\n",
    "        return example_lines, entry_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start, end = self.example_lines[idx]\n",
    "        words, labels = [], []\n",
    "        for idx in range(start, end):\n",
    "            line = linecache.getline(self.data_file, idx).rstrip(\"\\n\")\n",
    "            splits = line.split(\" \")\n",
    "            words.append(splits[0])\n",
    "            if len(splits) > 1:\n",
    "                labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "            else:\n",
    "                # Examples could have no label for mode = \"test\"\n",
    "                labels.append(\"O\")\n",
    "        if words:\n",
    "            example = InputExample(guid=\"%s-%d\".format(\"train\", idx), words=words, labels=labels)\n",
    "\n",
    "        label_map = {label: i for i, label in enumerate(self.args.labels_list)}\n",
    "\n",
    "        example_row = (\n",
    "            example,\n",
    "            label_map,\n",
    "            self.args.max_seq_length,\n",
    "            self.tokenizer,\n",
    "            bool(self.args.model_type in [\"xlnet\"]),\n",
    "            self.tokenizer.cls_token,\n",
    "            2 if self.args.model_type in [\"xlnet\"] else 0,\n",
    "            self.tokenizer.sep_token,\n",
    "            bool(self.args.model_type in [\"roberta\"]),\n",
    "            bool(self.args.model_type in [\"xlnet\"]),\n",
    "            self.tokenizer.convert_tokens_to_ids([self.tokenizer.pad_token])[0],\n",
    "            4 if self.args.model_type in [\"xlnet\"] else 0,\n",
    "            self.pad_token_label_id,\n",
    "            0,\n",
    "            True,\n",
    "        )\n",
    "\n",
    "        features = convert_example_to_feature(example_row)\n",
    "        all_input_ids = torch.tensor(features.input_ids, dtype=torch.long)\n",
    "        all_input_mask = torch.tensor(features.input_mask, dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor(features.segment_ids, dtype=torch.long)\n",
    "        all_label_ids = torch.tensor(features.label_ids, dtype=torch.long)\n",
    "        return (all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "global_args = {\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"best_model_dir\": \"outputs/best_model\",\n",
    "    \"cache_dir\": \"cache_dir/\",\n",
    "    \"config\": {},\n",
    "    \"do_lower_case\": False,\n",
    "    \"early_stopping_consider_epochs\": False,\n",
    "    \"early_stopping_delta\": 0,\n",
    "    \"early_stopping_metric\": \"eval_loss\",\n",
    "    \"early_stopping_metric_minimize\": True,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"encoding\": None,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"evaluate_during_training\": False,\n",
    "    \"evaluate_during_training_silent\": True,\n",
    "    \"evaluate_during_training_steps\": 2000,\n",
    "    \"evaluate_during_training_verbose\": False,\n",
    "    \"fp16\": True,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"learning_rate\": 4e-5,\n",
    "    \"local_rank\": -1,\n",
    "    \"logging_steps\": 50,\n",
    "    \"manual_seed\": None,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"multiprocessing_chunksize\": 500,\n",
    "    \"n_gpu\": 1,\n",
    "    \"no_cache\": False,\n",
    "    \"no_save\": False,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"output_dir\": \"outputs/\",\n",
    "    \"overwrite_output_dir\": False,\n",
    "    \"process_count\": cpu_count() - 2 if cpu_count() > 2 else 1,\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"save_best_model\": True,\n",
    "    \"save_eval_checkpoints\": True,\n",
    "    \"save_model_every_epoch\": True,\n",
    "    \"save_steps\": 2000,\n",
    "    \"save_optimizer_and_scheduler\": True,\n",
    "    \"silent\": False,\n",
    "    \"tensorboard_dir\": None,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"use_cached_eval_features\": False,\n",
    "    \"use_early_stopping\": False,\n",
    "    \"use_multiprocessing\": True,\n",
    "    \"wandb_kwargs\": {},\n",
    "    \"wandb_project\": None,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"weight_decay\": 0,\n",
    "}\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    global_args[\"process_count\"] = min(global_args[\"process_count\"], 61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import asdict, dataclass, field, fields\n",
    "from multiprocessing import cpu_count\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def get_default_process_count():\n",
    "    process_count = cpu_count() - 2 if cpu_count() > 2 else 1\n",
    "    if sys.platform == \"win32\":\n",
    "        process_count = min(process_count, 61)\n",
    "\n",
    "    return process_count\n",
    "\n",
    "\n",
    "def get_special_tokens():\n",
    "    return [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    adam_epsilon: float = 1e-8\n",
    "    best_model_dir: str = \"outputs/best_model\"\n",
    "    cache_dir: str = \"cache_dir/\"\n",
    "    config: dict = field(default_factory=dict)\n",
    "    cosine_schedule_num_cycles: float = 0.5\n",
    "    custom_layer_parameters: list = field(default_factory=list)\n",
    "    custom_parameter_groups: list = field(default_factory=list)\n",
    "    dataloader_num_workers: int = 0\n",
    "    do_lower_case: bool = False\n",
    "    dynamic_quantize: bool = False\n",
    "    early_stopping_consider_epochs: bool = False\n",
    "    early_stopping_delta: float = 0\n",
    "    early_stopping_metric: str = \"eval_loss\"\n",
    "    early_stopping_metric_minimize: bool = True\n",
    "    early_stopping_patience: int = 3\n",
    "    encoding: str = None\n",
    "    adafactor_eps: tuple = field(default_factory=lambda: (1e-30, 1e-3))\n",
    "    adafactor_clip_threshold: float = 1.0\n",
    "    adafactor_decay_rate: float = -0.8\n",
    "    adafactor_beta1: float = None\n",
    "    adafactor_scale_parameter: bool = True\n",
    "    adafactor_relative_step: bool = True\n",
    "    adafactor_warmup_init: bool = True\n",
    "    eval_batch_size: int = 8\n",
    "    evaluate_during_training: bool = False\n",
    "    evaluate_during_training_silent: bool = True\n",
    "    evaluate_during_training_steps: int = 2000\n",
    "    evaluate_during_training_verbose: bool = False\n",
    "    evaluate_each_epoch: bool = True\n",
    "    fp16: bool = True\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    learning_rate: float = 4e-5\n",
    "    local_rank: int = -1\n",
    "    logging_steps: int = 50\n",
    "    manual_seed: int = None\n",
    "    max_grad_norm: float = 1.0\n",
    "    max_seq_length: int = 128\n",
    "    model_name: str = None\n",
    "    model_type: str = None\n",
    "    multiprocessing_chunksize: int = -1\n",
    "    n_gpu: int = 1\n",
    "    no_cache: bool = False\n",
    "    no_save: bool = False\n",
    "    not_saved_args: list = field(default_factory=list)\n",
    "    num_train_epochs: int = 1\n",
    "    optimizer: str = \"AdamW\"\n",
    "    output_dir: str = \"outputs/\"\n",
    "    overwrite_output_dir: bool = False\n",
    "    process_count: int = field(default_factory=get_default_process_count)\n",
    "    polynomial_decay_schedule_lr_end: float = 1e-7\n",
    "    polynomial_decay_schedule_power: float = 1.0\n",
    "    quantized_model: bool = False\n",
    "    reprocess_input_data: bool = True\n",
    "    save_best_model: bool = True\n",
    "    save_eval_checkpoints: bool = True\n",
    "    save_model_every_epoch: bool = True\n",
    "    save_optimizer_and_scheduler: bool = True\n",
    "    save_steps: int = 2000\n",
    "    scheduler: str = \"linear_schedule_with_warmup\"\n",
    "    silent: bool = False\n",
    "    skip_special_tokens: bool = True\n",
    "    tensorboard_dir: str = None\n",
    "    thread_count: int = None\n",
    "    tokenizer_type: str = None\n",
    "    tokenizer_name: str = None\n",
    "    train_batch_size: int = 8\n",
    "    train_custom_parameters_only: bool = False\n",
    "    use_cached_eval_features: bool = False\n",
    "    use_early_stopping: bool = False\n",
    "    use_multiprocessing: bool = True\n",
    "    use_multiprocessing_for_evaluation: bool = False\n",
    "    wandb_kwargs: dict = field(default_factory=dict)\n",
    "    wandb_project: str = None\n",
    "    warmup_ratio: float = 0.06\n",
    "    warmup_steps: int = 0\n",
    "    weight_decay: float = 0.0\n",
    "\n",
    "    def update_from_dict(self, new_values):\n",
    "        if isinstance(new_values, dict):\n",
    "            for key, value in new_values.items():\n",
    "                setattr(self, key, value)\n",
    "        else:\n",
    "            raise (TypeError(f\"{new_values} is not a Python dict.\"))\n",
    "\n",
    "    def get_args_for_saving(self):\n",
    "        args_for_saving = {key: value for key, value in asdict(self).items() if key not in self.not_saved_args}\n",
    "        return args_for_saving\n",
    "\n",
    "    def save(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(os.path.join(output_dir, \"model_args.json\"), \"w\") as f:\n",
    "            args_dict = self.get_args_for_saving()\n",
    "            if args_dict[\"tokenizer_type\"] is not None and not isinstance(args_dict[\"tokenizer_type\"], str):\n",
    "                args_dict[\"tokenizer_type\"] = type(args_dict[\"tokenizer_type\"]).__name__\n",
    "            json.dump(args_dict, f)\n",
    "\n",
    "    def load(self, input_dir):\n",
    "        if input_dir:\n",
    "            model_args_file = os.path.join(input_dir, \"model_args.json\")\n",
    "            if os.path.isfile(model_args_file):\n",
    "                with open(model_args_file, \"r\") as f:\n",
    "                    model_args = json.load(f)\n",
    "\n",
    "                self.update_from_dict(model_args)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClassificationArgs(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a ClassificationModel\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"ClassificationModel\"\n",
    "    labels_list: list = field(default_factory=list)\n",
    "    labels_map: dict = field(default_factory=dict)\n",
    "    lazy_delimiter: str = \"\\t\"\n",
    "    lazy_labels_column: int = 1\n",
    "    lazy_loading: bool = False\n",
    "    lazy_loading_start_line: int = 1\n",
    "    lazy_text_a_column: bool = None\n",
    "    lazy_text_b_column: bool = None\n",
    "    lazy_text_column: int = 0\n",
    "    onnx: bool = False\n",
    "    regression: bool = False\n",
    "    sliding_window: bool = False\n",
    "    special_tokens_list: list = field(default_factory=list)\n",
    "    stride: float = 0.8\n",
    "    tie_value: int = 1\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultiLabelClassificationArgs(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a MultiLabelClassificationModel\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"MultiLabelClassificationModel\"\n",
    "    sliding_window: bool = False\n",
    "    stride: float = 0.8\n",
    "    threshold: float = 0.5\n",
    "    tie_value: int = 1\n",
    "    labels_list: list = field(default_factory=list)\n",
    "    labels_map: dict = field(default_factory=dict)\n",
    "    lazy_loading: bool = False\n",
    "    special_tokens_list: list = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NERArgs(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a NERModel\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"NERModel\"\n",
    "    classification_report: bool = False\n",
    "    labels_list: list = field(default_factory=list)\n",
    "    lazy_loading: bool = False\n",
    "    lazy_loading_start_line: int = 0\n",
    "    onnx: bool = False\n",
    "    special_tokens_list: list = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestionAnsweringArgs(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a QuestionAnsweringModel\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"QuestionAnsweringModel\"\n",
    "    doc_stride: int = 384\n",
    "    early_stopping_metric: str = \"correct\"\n",
    "    early_stopping_metric_minimize: bool = False\n",
    "    lazy_loading: bool = False\n",
    "    max_answer_length: int = 100\n",
    "    max_query_length: int = 64\n",
    "    n_best_size: int = 20\n",
    "    null_score_diff_threshold: float = 0.0\n",
    "    special_tokens_list: list = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class T5Args(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a T5Model\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"T5Model\"\n",
    "    dataset_class: Dataset = None\n",
    "    do_sample: bool = False\n",
    "    early_stopping: bool = True\n",
    "    evaluate_generated_text: bool = False\n",
    "    length_penalty: float = 2.0\n",
    "    max_length: int = 20\n",
    "    max_steps: int = -1\n",
    "    num_beams: int = 1\n",
    "    num_return_sequences: int = 1\n",
    "    preprocess_inputs: bool = True\n",
    "    repetition_penalty: float = 1.0\n",
    "    scheduler: str = \"constant_schedule_with_warmup\"\n",
    "    adafactor_relative_step: bool = False\n",
    "    adafactor_scale_parameter: bool = False\n",
    "    adafactor_warmup_init: bool = False\n",
    "    learning_rate: float = 1e-3\n",
    "    optimizer: str = \"Adafactor\"\n",
    "    special_tokens_list: list = field(default_factory=list)\n",
    "    top_k: float = None\n",
    "    top_p: float = None\n",
    "    use_multiprocessed_decoding: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LanguageModelingArgs(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a LanguageModelingModel\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"LanguageModelingModel\"\n",
    "    block_size: int = -1\n",
    "    config_name: str = None\n",
    "    dataset_class: Dataset = None\n",
    "    dataset_type: str = \"None\"\n",
    "    discriminator_config: dict = field(default_factory=dict)\n",
    "    discriminator_loss_weight: float = 50.0\n",
    "    generator_config: dict = field(default_factory=dict)\n",
    "    max_steps: int = -1\n",
    "    min_frequency: int = 2\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    sliding_window: bool = False\n",
    "    special_tokens: list = field(default_factory=get_special_tokens)\n",
    "    stride: float = 0.8\n",
    "    tie_generator_and_discriminator_embeddings: bool = True\n",
    "    tokenizer_name: str = None\n",
    "    vocab_size: int = None\n",
    "    clean_text: bool = True\n",
    "    handle_chinese_chars: bool = True\n",
    "    special_tokens_list: list = field(default_factory=list)\n",
    "    strip_accents: bool = True\n",
    "    local_rank: int = -1\n",
    "\n",
    "    def save(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(os.path.join(output_dir, \"model_args.json\"), \"w\") as f:\n",
    "            args_dict = self.get_args_for_saving()\n",
    "            if args_dict[\"dataset_class\"] is not None:\n",
    "                args_dict[\"dataset_class\"] = type(args_dict[\"dataset_class\"]).__name__\n",
    "            json.dump(self.get_args_for_saving(), f)\n",
    "\n",
    "    def load(self, input_dir):\n",
    "        if input_dir:\n",
    "            model_args_file = os.path.join(input_dir, \"model_args.json\")\n",
    "            if os.path.isfile(model_args_file):\n",
    "                with open(model_args_file, \"r\") as f:\n",
    "                    model_args = json.load(f)\n",
    "                if model_args[\"dataset_class\"]:\n",
    "                    warnings.warn(\n",
    "                        \"This model was trained using a custom dataset_class.\"\n",
    "                        \"This cannot be loaded automatically and must be specified in the model args\"\n",
    "                        \"when loading the model.\"\n",
    "                    )\n",
    "                self.update_from_dict(model_args)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqArgs(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a Seq2SeqModel\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"Seq2SeqModel\"\n",
    "    base_marian_model_name: str = None\n",
    "    dataset_class: Dataset = None\n",
    "    do_sample: bool = False\n",
    "    early_stopping: bool = True\n",
    "    evaluate_generated_text: bool = False\n",
    "    length_penalty: float = 2.0\n",
    "    max_length: int = 20\n",
    "    max_steps: int = -1\n",
    "    num_beams: int = 1\n",
    "    num_return_sequences: int = 1\n",
    "    repetition_penalty: float = 1.0\n",
    "    top_k: float = None\n",
    "    top_p: float = None\n",
    "    use_multiprocessed_decoding: bool = False\n",
    "    src_lang: str = \"en_XX\"\n",
    "    tgt_lang: str = \"ro_RO\"\n",
    "\n",
    "    def save(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(os.path.join(output_dir, \"model_args.json\"), \"w\") as f:\n",
    "            args_dict = self.get_args_for_saving()\n",
    "            if args_dict[\"dataset_class\"] is not None:\n",
    "                args_dict[\"dataset_class\"] = type(args_dict[\"dataset_class\"]).__name__\n",
    "            json.dump(self.get_args_for_saving(), f)\n",
    "\n",
    "    def load(self, input_dir):\n",
    "        if input_dir:\n",
    "            model_args_file = os.path.join(input_dir, \"model_args.json\")\n",
    "            if os.path.isfile(model_args_file):\n",
    "                with open(model_args_file, \"r\") as f:\n",
    "                    model_args = json.load(f)\n",
    "                if model_args[\"dataset_class\"]:\n",
    "                    warnings.warn(\n",
    "                        \"This model was trained using a custom dataset_class.\"\n",
    "                        \"This cannot be loaded automatically and must be specified in the model args\"\n",
    "                        \"when loading the model.\"\n",
    "                    )\n",
    "                self.update_from_dict(model_args)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LanguageGenerationArgs(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a LanguageGenerationModel\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"LanguageGenerationModel\"\n",
    "    do_sample: bool = True\n",
    "    early_stopping: bool = True\n",
    "    evaluate_generated_text: bool = False\n",
    "    length_penalty: float = 2.0\n",
    "    max_length: int = 20\n",
    "    max_steps: int = -1\n",
    "    num_beams: int = 1\n",
    "    num_return_sequences: int = 1\n",
    "    repetition_penalty: float = 1.0\n",
    "    top_k: float = 50\n",
    "    top_p: float = 0.95\n",
    "    prompt: str = \"\"\n",
    "    stop_token: str = None\n",
    "    temperature: float = 1.0\n",
    "    padding_text: str = \"\"\n",
    "    xlm_language: str = \"\"\n",
    "    config_name: str = None\n",
    "    tokenizer_name: str = None\n",
    "    special_tokens_list: list = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ConvAIArgs(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a ConvAIModel\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"ConvAIModel\"\n",
    "    do_sample: bool = True\n",
    "    lm_coef: float = 2.0\n",
    "    max_history: int = 2\n",
    "    max_length: int = 20\n",
    "    mc_coef: float = 1.0\n",
    "    min_length: int = 1\n",
    "    num_candidates: int = 2\n",
    "    personality_permutations: int = 1\n",
    "    temperature: float = 0.7\n",
    "    top_k: float = 0\n",
    "    top_p: float = 0.9\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultiModalClassificationArgs(ModelArgs):\n",
    "    \"\"\"\n",
    "    Model args for a MultiModalClassificationModel\n",
    "    \"\"\"\n",
    "\n",
    "    model_class: str = \"MultiModalClassificationModel\"\n",
    "    regression: bool = False\n",
    "    num_image_embeds: int = 1\n",
    "    text_label: str = \"text\"\n",
    "    labels_label: str = \"labels\"\n",
    "    images_label: str = \"images\"\n",
    "    image_type_extension: str = \"\"\n",
    "    data_type_extension: str = \"\"\n",
    "    special_tokens_list: list = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_config_to_sweep_values(sweep_config):\n",
    "    \"\"\"\n",
    "    Converts an instance of wandb.Config to plain values map.\n",
    "    wandb.Config varies across versions quite significantly,\n",
    "    so we use the `keys` method that works consistently.\n",
    "    \"\"\"\n",
    "\n",
    "    return {key: sweep_config[key] for key in sweep_config.keys()}\n"
   ]
  },
  {
   "source": [
    "# Build NER model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from dataclasses import asdict\n",
    "from multiprocessing import cpu_count\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.stats import pearsonr\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers.optimization import (\n",
    "    get_constant_schedule,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    ")\n",
    "from transformers.optimization import AdamW, Adafactor\n",
    "from transformers import (\n",
    "    AlbertConfig,\n",
    "    AlbertForTokenClassification,\n",
    "    AlbertTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    BertConfig,\n",
    "    BertForTokenClassification,\n",
    "    BertTokenizer,\n",
    "    BertweetTokenizer,\n",
    "    CamembertConfig,\n",
    "    CamembertForTokenClassification,\n",
    "    CamembertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForTokenClassification,\n",
    "    DistilBertTokenizer,\n",
    "    ElectraConfig,\n",
    "    ElectraForTokenClassification,\n",
    "    ElectraTokenizer,\n",
    "    LayoutLMConfig,\n",
    "    LayoutLMForTokenClassification,\n",
    "    LayoutLMTokenizer,\n",
    "    LongformerConfig,\n",
    "    LongformerForTokenClassification,\n",
    "    LongformerTokenizer,\n",
    "    MPNetConfig,\n",
    "    MPNetForTokenClassification,\n",
    "    MPNetTokenizer,\n",
    "    MobileBertConfig,\n",
    "    MobileBertForTokenClassification,\n",
    "    MobileBertTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForTokenClassification,\n",
    "    RobertaTokenizer,\n",
    "    SqueezeBertConfig,\n",
    "    SqueezeBertForTokenClassification,\n",
    "    SqueezeBertTokenizer,\n",
    "    WEIGHTS_NAME,\n",
    "    XLMRobertaConfig,\n",
    "    XLMRobertaForTokenClassification,\n",
    "    XLMRobertaTokenizer,\n",
    "    XLNetConfig,\n",
    "    XLNetForTokenClassification,\n",
    "    XLNetTokenizerFast,\n",
    ")\n",
    "from wandb import config\n",
    "from transformers.convert_graph_to_onnx import convert, quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import wandb\n",
    "\n",
    "    wandb_available = True\n",
    "except ImportError:\n",
    "    wandb_available = False\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "MODELS_WITH_EXTRA_SEP_TOKEN = [\"roberta\", \"camembert\", \"xlmroberta\", \"longformer\", \"mpnet\"]\n",
    "\n",
    "\n",
    "class NERModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type,\n",
    "        model_name,\n",
    "        labels=None,\n",
    "        args=None,\n",
    "        use_cuda=True,\n",
    "        cuda_device=-1,\n",
    "        onnx_execution_provider=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a NERModel\n",
    "        Args:\n",
    "            model_type: The type of model (bert, roberta)\n",
    "            model_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_model.bin).\n",
    "            labels (optional): A list of all Named Entity labels.  If not given, [\"O\", \"B-MISC\", \"I-MISC\",  \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"] will be used.\n",
    "            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n",
    "            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n",
    "            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n",
    "            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        MODEL_CLASSES = {\n",
    "            \"albert\": (AlbertConfig, AlbertForTokenClassification, AlbertTokenizer),\n",
    "            \"auto\": (AutoConfig, AutoModelForTokenClassification, AutoTokenizer),\n",
    "            \"bert\": (BertConfig, BertForTokenClassification, BertTokenizer),\n",
    "            \"bertweet\": (RobertaConfig, RobertaForTokenClassification, BertweetTokenizer),\n",
    "            \"camembert\": (CamembertConfig, CamembertForTokenClassification, CamembertTokenizer),\n",
    "            \"distilbert\": (DistilBertConfig, DistilBertForTokenClassification, DistilBertTokenizer),\n",
    "            \"electra\": (ElectraConfig, ElectraForTokenClassification, ElectraTokenizer),\n",
    "            \"layoutlm\": (LayoutLMConfig, LayoutLMForTokenClassification, LayoutLMTokenizer),\n",
    "            \"longformer\": (LongformerConfig, LongformerForTokenClassification, LongformerTokenizer),\n",
    "            \"mobilebert\": (MobileBertConfig, MobileBertForTokenClassification, MobileBertTokenizer),\n",
    "            \"mpnet\": (MPNetConfig, MPNetForTokenClassification, MPNetTokenizer),\n",
    "            \"roberta\": (RobertaConfig, RobertaForTokenClassification, RobertaTokenizer),\n",
    "            \"squeezebert\": (SqueezeBertConfig, SqueezeBertForTokenClassification, SqueezeBertTokenizer),\n",
    "            \"xlmroberta\": (XLMRobertaConfig, XLMRobertaForTokenClassification, XLMRobertaTokenizer),\n",
    "            \"xlnet\": (XLNetConfig, XLNetForTokenClassification, XLNetTokenizerFast),\n",
    "        }\n",
    "\n",
    "        self.args = self._load_model_args(model_name)\n",
    "\n",
    "        if isinstance(args, dict):\n",
    "            self.args.update_from_dict(args)\n",
    "        elif isinstance(args, NERArgs):\n",
    "            self.args = args\n",
    "\n",
    "        if \"sweep_config\" in kwargs:\n",
    "            self.is_sweeping = True\n",
    "            sweep_config = kwargs.pop(\"sweep_config\")\n",
    "            sweep_values = sweep_config_to_sweep_values(sweep_config)\n",
    "            self.args.update_from_dict(sweep_values)\n",
    "        else:\n",
    "            self.is_sweeping = False\n",
    "\n",
    "        if self.args.manual_seed:\n",
    "            random.seed(self.args.manual_seed)\n",
    "            np.random.seed(self.args.manual_seed)\n",
    "            torch.manual_seed(self.args.manual_seed)\n",
    "            if self.args.n_gpu > 0:\n",
    "                torch.cuda.manual_seed_all(self.args.manual_seed)\n",
    "\n",
    "        if not use_cuda:\n",
    "            self.args.fp16 = False\n",
    "\n",
    "        if labels and self.args.labels_list:\n",
    "            assert labels == self.args.labels_list\n",
    "            self.args.labels_list = labels\n",
    "        elif labels:\n",
    "            self.args.labels_list = labels\n",
    "        elif self.args.labels_list:\n",
    "            pass\n",
    "        else:\n",
    "            self.args.labels_list = [\n",
    "                \"O\",\n",
    "                \"B-MISC\",\n",
    "                \"I-MISC\",\n",
    "                \"B-PER\",\n",
    "                \"I-PER\",\n",
    "                \"B-ORG\",\n",
    "                \"I-ORG\",\n",
    "                \"B-LOC\",\n",
    "                \"I-LOC\",\n",
    "            ]\n",
    "        self.num_labels = len(self.args.labels_list)\n",
    "\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "        if self.num_labels:\n",
    "            self.config = config_class.from_pretrained(model_name, num_labels=self.num_labels, **self.args.config)\n",
    "            self.num_labels = self.num_labels\n",
    "        else:\n",
    "            self.config = config_class.from_pretrained(model_name, **self.args.config)\n",
    "            self.num_labels = self.config.num_labels\n",
    "\n",
    "        if use_cuda:\n",
    "            if torch.cuda.is_available():\n",
    "                if cuda_device == -1:\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                else:\n",
    "                    self.device = torch.device(f\"cuda:{cuda_device}\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"'use_cuda' set to True when cuda is unavailable.\"\n",
    "                    \"Make sure CUDA is available or set use_cuda=False.\"\n",
    "                )\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        if self.args.onnx:\n",
    "            from onnxruntime import InferenceSession, SessionOptions\n",
    "\n",
    "            if not onnx_execution_provider:\n",
    "                onnx_execution_provider = \"CUDAExecutionProvider\" if use_cuda else \"CPUExecutionProvider\"\n",
    "\n",
    "            options = SessionOptions()\n",
    "            options.intra_op_num_threads = 1\n",
    "\n",
    "            if self.args.dynamic_quantize:\n",
    "                model_path = quantize(Path(os.path.join(model_name, \"onnx_model.onnx\")))\n",
    "                self.model = InferenceSession(model_path.as_posix(), options, providers=[onnx_execution_provider])\n",
    "            else:\n",
    "                model_path = os.path.join(model_name, \"onnx_model.onnx\")\n",
    "                self.model = InferenceSession(model_path, options, providers=[onnx_execution_provider])\n",
    "        else:\n",
    "            if not self.args.quantized_model:\n",
    "                self.model = model_class.from_pretrained(model_name, config=self.config, **kwargs)\n",
    "            else:\n",
    "                quantized_weights = torch.load(os.path.join(model_name, \"pytorch_model.bin\"))\n",
    "                self.model = model_class.from_pretrained(None, config=self.config, state_dict=quantized_weights)\n",
    "\n",
    "            if self.args.dynamic_quantize:\n",
    "                self.model = torch.quantization.quantize_dynamic(self.model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "            if self.args.quantized_model:\n",
    "                self.model.load_state_dict(quantized_weights)\n",
    "            if self.args.dynamic_quantize:\n",
    "                self.args.quantized_model = True\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "        if self.args.fp16:\n",
    "            try:\n",
    "                from torch.cuda import amp\n",
    "            except AttributeError:\n",
    "                raise AttributeError(\"fp16 requires Pytorch >= 1.6. Please update Pytorch or turn off fp16.\")\n",
    "\n",
    "        if model_name in [\n",
    "            \"vinai/bertweet-base\",\n",
    "            \"vinai/bertweet-covid19-base-cased\",\n",
    "            \"vinai/bertweet-covid19-base-uncased\",\n",
    "        ]:\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(\n",
    "                model_name, do_lower_case=self.args.do_lower_case, normalization=True, **kwargs\n",
    "            )\n",
    "        else:\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(\n",
    "                model_name, do_lower_case=self.args.do_lower_case, **kwargs\n",
    "            )\n",
    "\n",
    "        if self.args.special_tokens_list:\n",
    "            self.tokenizer.add_tokens(self.args.special_tokens_list, special_tokens=True)\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.args.model_name = model_name\n",
    "        self.args.model_type = model_type\n",
    "\n",
    "        self.pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "        if model_type == \"camembert\":\n",
    "            warnings.warn(\n",
    "                \"use_multiprocessing automatically disabled as CamemBERT\"\n",
    "                \" fails when using multiprocessing for feature conversion.\"\n",
    "            )\n",
    "            self.args.use_multiprocessing = False\n",
    "\n",
    "        if self.args.wandb_project and not wandb_available:\n",
    "            warnings.warn(\"wandb_project specified but wandb is not available. Wandb disabled.\")\n",
    "            self.args.wandb_project = None\n",
    "\n",
    "    def train_model(\n",
    "        self, train_data, output_dir=None, show_running_loss=True, args=None, eval_data=None, verbose=True, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the model using 'train_data'\n",
    "        Args:\n",
    "            train_data: train_data should be the path to a .txt file containing the training data OR a pandas DataFrame with 3 columns.\n",
    "                        If a text file is given the data should be in the CoNLL format. i.e. One word per line, with sentences seperated by an empty line.\n",
    "                        The first word of the line should be a word, and the last should be a Name Entity Tag.\n",
    "                        If a DataFrame is given, each sentence should be split into words, with each word assigned a tag, and with all words from the same sentence given the same sentence_id.\n",
    "            eval_data: Evaluation data (same format as train_data) against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.\n",
    "            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n",
    "            show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.\n",
    "            args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n",
    "            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\n",
    "                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n",
    "        Returns:\n",
    "            global_step: Number of global steps trained\n",
    "            training_details: Average training loss if evaluate_during_training is False or full training progress scores if evaluate_during_training is True\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        if args:\n",
    "            self.args.update_from_dict(args)\n",
    "\n",
    "        if self.args.silent:\n",
    "            show_running_loss = False\n",
    "\n",
    "        if self.args.evaluate_during_training and eval_data is None:\n",
    "            if \"eval_df\" in kwargs:\n",
    "                warnings.warn(\n",
    "                    \"The eval_df parameter has been renamed to eval_data.\"\n",
    "                    \" Using eval_df will raise an error in a future version.\"\n",
    "                )\n",
    "                eval_data = kwargs.pop(\"eval_df\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"evaluate_during_training is enabled but eval_data is not specified.\"\n",
    "                    \" Pass eval_data to model.train_model() if using evaluate_during_training.\"\n",
    "                )\n",
    "\n",
    "        if not output_dir:\n",
    "            output_dir = self.args.output_dir\n",
    "\n",
    "        if os.path.exists(output_dir) and os.listdir(output_dir) and not self.args.overwrite_output_dir:\n",
    "            raise ValueError(\n",
    "                \"Output directory ({}) already exists and is not empty.\"\n",
    "                \" Use --overwrite_output_dir to overcome.\".format(output_dir)\n",
    "            )\n",
    "\n",
    "        self._move_model_to_device()\n",
    "\n",
    "        train_dataset = self.load_and_cache_examples(train_data)\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        global_step, training_details = self.train(\n",
    "            train_dataset, output_dir, show_running_loss=show_running_loss, eval_data=eval_data, **kwargs\n",
    "        )\n",
    "\n",
    "        self.save_model(model=self.model)\n",
    "\n",
    "        logger.info(\" Training of {} model complete. Saved to {}.\".format(self.args.model_type, output_dir))\n",
    "\n",
    "        return global_step, training_details\n",
    "\n",
    "    def train(self, train_dataset, output_dir, show_running_loss=True, eval_data=None, verbose=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Trains the model on train_dataset.\n",
    "        Utility function to be used by the train_model() method. Not intended to be used directly.\n",
    "        \"\"\"\n",
    "\n",
    "        device = self.device\n",
    "        model = self.model\n",
    "        args = self.args\n",
    "\n",
    "        tb_writer = SummaryWriter(logdir=args.tensorboard_dir)\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=args.train_batch_size,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "        )\n",
    "\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "        optimizer_grouped_parameters = []\n",
    "        custom_parameter_names = set()\n",
    "        for group in self.args.custom_parameter_groups:\n",
    "            params = group.pop(\"params\")\n",
    "            custom_parameter_names.update(params)\n",
    "            param_group = {**group}\n",
    "            param_group[\"params\"] = [p for n, p in model.named_parameters() if n in params]\n",
    "            optimizer_grouped_parameters.append(param_group)\n",
    "\n",
    "        for group in self.args.custom_layer_parameters:\n",
    "            layer_number = group.pop(\"layer\")\n",
    "            layer = f\"layer.{layer_number}.\"\n",
    "            group_d = {**group}\n",
    "            group_nd = {**group}\n",
    "            group_nd[\"weight_decay\"] = 0.0\n",
    "            params_d = []\n",
    "            params_nd = []\n",
    "            for n, p in model.named_parameters():\n",
    "                if n not in custom_parameter_names and layer in n:\n",
    "                    if any(nd in n for nd in no_decay):\n",
    "                        params_nd.append(p)\n",
    "                    else:\n",
    "                        params_d.append(p)\n",
    "                    custom_parameter_names.add(n)\n",
    "            group_d[\"params\"] = params_d\n",
    "            group_nd[\"params\"] = params_nd\n",
    "\n",
    "            optimizer_grouped_parameters.append(group_d)\n",
    "            optimizer_grouped_parameters.append(group_nd)\n",
    "\n",
    "        if not self.args.train_custom_parameters_only:\n",
    "            optimizer_grouped_parameters.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"params\": [\n",
    "                            p\n",
    "                            for n, p in model.named_parameters()\n",
    "                            if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "                        ],\n",
    "                        \"weight_decay\": args.weight_decay,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [\n",
    "                            p\n",
    "                            for n, p in model.named_parameters()\n",
    "                            if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "                        ],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        warmup_steps = math.ceil(t_total * args.warmup_ratio)\n",
    "        args.warmup_steps = warmup_steps if args.warmup_steps == 0 else args.warmup_steps\n",
    "\n",
    "        if args.optimizer == \"AdamW\":\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "        elif args.optimizer == \"Adafactor\":\n",
    "            optimizer = Adafactor(\n",
    "                optimizer_grouped_parameters,\n",
    "                lr=args.learning_rate,\n",
    "                eps=args.adafactor_eps,\n",
    "                clip_threshold=args.adafactor_clip_threshold,\n",
    "                decay_rate=args.adafactor_decay_rate,\n",
    "                beta1=args.adafactor_beta1,\n",
    "                weight_decay=args.weight_decay,\n",
    "                scale_parameter=args.adafactor_scale_parameter,\n",
    "                relative_step=args.adafactor_relative_step,\n",
    "                warmup_init=args.adafactor_warmup_init,\n",
    "            )\n",
    "            print(\"Using Adafactor for T5\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"{} is not a valid optimizer class. Please use one of ('AdamW', 'Adafactor') instead.\".format(\n",
    "                    args.optimizer\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if args.scheduler == \"constant_schedule\":\n",
    "            scheduler = get_constant_schedule(optimizer)\n",
    "\n",
    "        elif args.scheduler == \"constant_schedule_with_warmup\":\n",
    "            scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps)\n",
    "\n",
    "        elif args.scheduler == \"linear_schedule_with_warmup\":\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "            )\n",
    "\n",
    "        elif args.scheduler == \"cosine_schedule_with_warmup\":\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=args.warmup_steps,\n",
    "                num_training_steps=t_total,\n",
    "                num_cycles=args.cosine_schedule_num_cycles,\n",
    "            )\n",
    "\n",
    "        elif args.scheduler == \"cosine_with_hard_restarts_schedule_with_warmup\":\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=args.warmup_steps,\n",
    "                num_training_steps=t_total,\n",
    "                num_cycles=args.cosine_schedule_num_cycles,\n",
    "            )\n",
    "\n",
    "        elif args.scheduler == \"polynomial_decay_schedule_with_warmup\":\n",
    "            scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=args.warmup_steps,\n",
    "                num_training_steps=t_total,\n",
    "                lr_end=args.polynomial_decay_schedule_lr_end,\n",
    "                power=args.polynomial_decay_schedule_power,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"{} is not a valid scheduler.\".format(args.scheduler))\n",
    "\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        global_step = 0\n",
    "        training_progress_scores = None\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "        model.zero_grad()\n",
    "        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.silent, mininterval=0)\n",
    "        epoch_number = 0\n",
    "        best_eval_metric = None\n",
    "        early_stopping_counter = 0\n",
    "        steps_trained_in_current_epoch = 0\n",
    "        epochs_trained = 0\n",
    "\n",
    "        if args.model_name and os.path.exists(args.model_name):\n",
    "            try:\n",
    "                # set global_step to gobal_step of last saved checkpoint from model path\n",
    "                checkpoint_suffix = args.model_name.split(\"/\")[-1].split(\"-\")\n",
    "                if len(checkpoint_suffix) > 2:\n",
    "                    checkpoint_suffix = checkpoint_suffix[1]\n",
    "                else:\n",
    "                    checkpoint_suffix = checkpoint_suffix[-1]\n",
    "                global_step = int(checkpoint_suffix)\n",
    "                epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "                steps_trained_in_current_epoch = global_step % (\n",
    "                    len(train_dataloader) // args.gradient_accumulation_steps\n",
    "                )\n",
    "\n",
    "                logger.info(\"   Continuing training from checkpoint, will skip to saved global_step\")\n",
    "                logger.info(\"   Continuing training from epoch %d\", epochs_trained)\n",
    "                logger.info(\"   Continuing training from global step %d\", global_step)\n",
    "                logger.info(\"   Will skip the first %d steps in the current epoch\", steps_trained_in_current_epoch)\n",
    "            except ValueError:\n",
    "                logger.info(\"   Starting fine-tuning.\")\n",
    "\n",
    "        if args.evaluate_during_training:\n",
    "            training_progress_scores = self._create_training_progress_scores(**kwargs)\n",
    "        if args.wandb_project:\n",
    "            wandb.init(project=args.wandb_project, config={**asdict(args)}, **args.wandb_kwargs)\n",
    "            wandb.watch(self.model)\n",
    "\n",
    "        if self.args.fp16:\n",
    "            from torch.cuda import amp\n",
    "\n",
    "            scaler = amp.GradScaler()\n",
    "\n",
    "        for _ in train_iterator:\n",
    "            model.train()\n",
    "            if epochs_trained > 0:\n",
    "                epochs_trained -= 1\n",
    "                continue\n",
    "            train_iterator.set_description(f\"Epoch {epoch_number + 1} of {args.num_train_epochs}\")\n",
    "            batch_iterator = tqdm(\n",
    "                train_dataloader,\n",
    "                desc=f\"Running Epoch {epoch_number} of {args.num_train_epochs}\",\n",
    "                disable=args.silent,\n",
    "                mininterval=0,\n",
    "            )\n",
    "            for step, batch in enumerate(batch_iterator):\n",
    "                if steps_trained_in_current_epoch > 0:\n",
    "                    steps_trained_in_current_epoch -= 1\n",
    "                    continue\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                inputs = self._get_inputs_dict(batch)\n",
    "\n",
    "                if self.args.fp16:\n",
    "                    with amp.autocast():\n",
    "                        outputs = model(**inputs)\n",
    "                        # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "                        loss = outputs[0]\n",
    "                else:\n",
    "                    outputs = model(**inputs)\n",
    "                    # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "                    loss = outputs[0]\n",
    "\n",
    "                if args.n_gpu > 1:\n",
    "                    loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "                current_loss = loss.item()\n",
    "\n",
    "                if show_running_loss:\n",
    "                    batch_iterator.set_description(\n",
    "                        f\"Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f}\"\n",
    "                    )\n",
    "\n",
    "                if args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "                if self.args.fp16:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    if self.args.fp16:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                    if args.optimizer == \"AdamW\":\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                    if self.args.fp16:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                        # Log metrics\n",
    "                        tb_writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], global_step)\n",
    "                        tb_writer.add_scalar(\n",
    "                            \"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step,\n",
    "                        )\n",
    "                        logging_loss = tr_loss\n",
    "                        if args.wandb_project or self.is_sweeping:\n",
    "                            wandb.log(\n",
    "                                {\n",
    "                                    \"Training loss\": current_loss,\n",
    "                                    \"lr\": scheduler.get_last_lr()[0],\n",
    "                                    \"global_step\": global_step,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                    if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                        # Save model checkpoint\n",
    "                        output_dir_current = os.path.join(output_dir, \"checkpoint-{}\".format(global_step))\n",
    "\n",
    "                        self.save_model(output_dir_current, optimizer, scheduler, model=model)\n",
    "\n",
    "                    if args.evaluate_during_training and (\n",
    "                        args.evaluate_during_training_steps > 0\n",
    "                        and global_step % args.evaluate_during_training_steps == 0\n",
    "                    ):\n",
    "\n",
    "                        output_dir_current = os.path.join(output_dir, \"checkpoint-{}\".format(global_step))\n",
    "\n",
    "                        os.makedirs(output_dir_current, exist_ok=True)\n",
    "\n",
    "                        # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results, _, _ = self.eval_model(\n",
    "                            eval_data,\n",
    "                            verbose=verbose and args.evaluate_during_training_verbose,\n",
    "                            wandb_log=False,\n",
    "                            output_dir=output_dir_current,\n",
    "                            **kwargs,\n",
    "                        )\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "\n",
    "                        if args.save_eval_checkpoints:\n",
    "                            self.save_model(output_dir_current, optimizer, scheduler, model=model, results=results)\n",
    "\n",
    "                        training_progress_scores[\"global_step\"].append(global_step)\n",
    "                        training_progress_scores[\"train_loss\"].append(current_loss)\n",
    "                        for key in results:\n",
    "                            training_progress_scores[key].append(results[key])\n",
    "                        report = pd.DataFrame(training_progress_scores)\n",
    "                        report.to_csv(\n",
    "                            os.path.join(args.output_dir, \"training_progress_scores.csv\"), index=False,\n",
    "                        )\n",
    "\n",
    "                        if args.wandb_project or self.is_sweeping:\n",
    "                            wandb.log(self._get_last_metrics(training_progress_scores))\n",
    "\n",
    "                        if not best_eval_metric:\n",
    "                            best_eval_metric = results[args.early_stopping_metric]\n",
    "                            self.save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n",
    "                        if best_eval_metric and args.early_stopping_metric_minimize:\n",
    "                            if results[args.early_stopping_metric] - best_eval_metric < args.early_stopping_delta:\n",
    "                                best_eval_metric = results[args.early_stopping_metric]\n",
    "                                self.save_model(\n",
    "                                    args.best_model_dir, optimizer, scheduler, model=model, results=results\n",
    "                                )\n",
    "                                early_stopping_counter = 0\n",
    "                            else:\n",
    "                                if args.use_early_stopping:\n",
    "                                    if early_stopping_counter < args.early_stopping_patience:\n",
    "                                        early_stopping_counter += 1\n",
    "                                        if verbose:\n",
    "                                            logger.info(f\" No improvement in {args.early_stopping_metric}\")\n",
    "                                            logger.info(f\" Current step: {early_stopping_counter}\")\n",
    "                                            logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n",
    "                                    else:\n",
    "                                        if verbose:\n",
    "                                            logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n",
    "                                            logger.info(\" Training terminated.\")\n",
    "                                            train_iterator.close()\n",
    "                                        return (\n",
    "                                            global_step,\n",
    "                                            tr_loss / global_step\n",
    "                                            if not self.args.evaluate_during_training\n",
    "                                            else training_progress_scores,\n",
    "                                        )\n",
    "                        else:\n",
    "                            if results[args.early_stopping_metric] - best_eval_metric > args.early_stopping_delta:\n",
    "                                best_eval_metric = results[args.early_stopping_metric]\n",
    "                                self.save_model(\n",
    "                                    args.best_model_dir, optimizer, scheduler, model=model, results=results\n",
    "                                )\n",
    "                                early_stopping_counter = 0\n",
    "                            else:\n",
    "                                if args.use_early_stopping:\n",
    "                                    if early_stopping_counter < args.early_stopping_patience:\n",
    "                                        early_stopping_counter += 1\n",
    "                                        if verbose:\n",
    "                                            logger.info(f\" No improvement in {args.early_stopping_metric}\")\n",
    "                                            logger.info(f\" Current step: {early_stopping_counter}\")\n",
    "                                            logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n",
    "                                    else:\n",
    "                                        if verbose:\n",
    "                                            logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n",
    "                                            logger.info(\" Training terminated.\")\n",
    "                                            train_iterator.close()\n",
    "                                        return (\n",
    "                                            global_step,\n",
    "                                            tr_loss / global_step\n",
    "                                            if not self.args.evaluate_during_training\n",
    "                                            else training_progress_scores,\n",
    "                                        )\n",
    "                        model.train()\n",
    "\n",
    "            epoch_number += 1\n",
    "            output_dir_current = os.path.join(output_dir, \"checkpoint-{}-epoch-{}\".format(global_step, epoch_number))\n",
    "\n",
    "            if args.save_model_every_epoch or args.evaluate_during_training:\n",
    "                os.makedirs(output_dir_current, exist_ok=True)\n",
    "\n",
    "            if args.save_model_every_epoch:\n",
    "                self.save_model(output_dir_current, optimizer, scheduler, model=model)\n",
    "\n",
    "            if args.evaluate_during_training and args.evaluate_each_epoch:\n",
    "                results, _, _ = self.eval_model(\n",
    "                    eval_data, verbose=verbose and args.evaluate_during_training_verbose, wandb_log=False, **kwargs\n",
    "                )\n",
    "\n",
    "                self.save_model(output_dir_current, optimizer, scheduler, results=results)\n",
    "\n",
    "                training_progress_scores[\"global_step\"].append(global_step)\n",
    "                training_progress_scores[\"train_loss\"].append(current_loss)\n",
    "                for key in results:\n",
    "                    training_progress_scores[key].append(results[key])\n",
    "                report = pd.DataFrame(training_progress_scores)\n",
    "                report.to_csv(os.path.join(args.output_dir, \"training_progress_scores.csv\"), index=False)\n",
    "\n",
    "                if args.wandb_project or self.is_sweeping:\n",
    "                    wandb.log(self._get_last_metrics(training_progress_scores))\n",
    "\n",
    "                if not best_eval_metric:\n",
    "                    best_eval_metric = results[args.early_stopping_metric]\n",
    "                    self.save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n",
    "                if best_eval_metric and args.early_stopping_metric_minimize:\n",
    "                    if results[args.early_stopping_metric] - best_eval_metric < args.early_stopping_delta:\n",
    "                        best_eval_metric = results[args.early_stopping_metric]\n",
    "                        self.save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        if args.use_early_stopping and args.early_stopping_consider_epochs:\n",
    "                            if early_stopping_counter < args.early_stopping_patience:\n",
    "                                early_stopping_counter += 1\n",
    "                                if verbose:\n",
    "                                    logger.info(f\" No improvement in {args.early_stopping_metric}\")\n",
    "                                    logger.info(f\" Current step: {early_stopping_counter}\")\n",
    "                                    logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n",
    "                            else:\n",
    "                                if verbose:\n",
    "                                    logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n",
    "                                    logger.info(\" Training terminated.\")\n",
    "                                    train_iterator.close()\n",
    "                                return (\n",
    "                                    global_step,\n",
    "                                    tr_loss / global_step\n",
    "                                    if not self.args.evaluate_during_training\n",
    "                                    else training_progress_scores,\n",
    "                                )\n",
    "                else:\n",
    "                    if results[args.early_stopping_metric] - best_eval_metric > args.early_stopping_delta:\n",
    "                        best_eval_metric = results[args.early_stopping_metric]\n",
    "                        self.save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n",
    "                        early_stopping_counter = 0\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        if args.use_early_stopping and args.early_stopping_consider_epochs:\n",
    "                            if early_stopping_counter < args.early_stopping_patience:\n",
    "                                early_stopping_counter += 1\n",
    "                                if verbose:\n",
    "                                    logger.info(f\" No improvement in {args.early_stopping_metric}\")\n",
    "                                    logger.info(f\" Current step: {early_stopping_counter}\")\n",
    "                                    logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n",
    "                            else:\n",
    "                                if verbose:\n",
    "                                    logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n",
    "                                    logger.info(\" Training terminated.\")\n",
    "                                    train_iterator.close()\n",
    "                                return (\n",
    "                                    global_step,\n",
    "                                    tr_loss / global_step\n",
    "                                    if not self.args.evaluate_during_training\n",
    "                                    else training_progress_scores,\n",
    "                                )\n",
    "\n",
    "        return (\n",
    "            global_step,\n",
    "            tr_loss / global_step if not self.args.evaluate_during_training else training_progress_scores,\n",
    "        )\n",
    "\n",
    "    def eval_model(self, eval_data, output_dir=None, verbose=True, silent=False, wandb_log=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Evaluates the model on eval_data. Saves results to output_dir.\n",
    "        Args:\n",
    "            eval_data: eval_data should be the path to a .txt file containing the evaluation data or a pandas DataFrame.\n",
    "                        If a text file is used the data should be in the CoNLL format. I.e. One word per line, with sentences seperated by an empty line.\n",
    "                        The first word of the line should be a word, and the last should be a Name Entity Tag.\n",
    "                        If a DataFrame is given, each sentence should be split into words, with each word assigned a tag, and with all words from the same sentence given the same sentence_id.\n",
    "            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n",
    "            verbose: If verbose, results will be printed to the console on completion of evaluation.\n",
    "            silent: If silent, tqdm progress bars will be hidden.\n",
    "            wandb_log: If True, evaluation results will be logged to wandb.\n",
    "            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\n",
    "                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n",
    "        Returns:\n",
    "            result: Dictionary containing evaluation results. (eval_loss, precision, recall, f1_score)\n",
    "            model_outputs: List of raw model outputs\n",
    "            preds_list: List of predicted tags\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "        if not output_dir:\n",
    "            output_dir = self.args.output_dir\n",
    "\n",
    "        self._move_model_to_device()\n",
    "\n",
    "        eval_dataset = self.load_and_cache_examples(eval_data, evaluate=True)\n",
    "\n",
    "        result, model_outputs, preds_list = self.evaluate(\n",
    "            eval_dataset, output_dir, verbose=verbose, silent=silent, wandb_log=wandb_log, **kwargs\n",
    "        )\n",
    "        self.results.update(result)\n",
    "\n",
    "        if verbose:\n",
    "            logger.info(self.results)\n",
    "\n",
    "        return result, model_outputs, preds_list\n",
    "\n",
    "    def evaluate(self, eval_dataset, output_dir, verbose=True, silent=False, wandb_log=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Evaluates the model on eval_dataset.\n",
    "        Utility function to be used by the eval_model() method. Not intended to be used directly.\n",
    "        \"\"\"\n",
    "\n",
    "        device = self.device\n",
    "        model = self.model\n",
    "        args = self.args\n",
    "        pad_token_label_id = self.pad_token_label_id\n",
    "        eval_output_dir = output_dir\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        eval_sampler = SequentialSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "        model.eval()\n",
    "\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        if self.args.fp16:\n",
    "            from torch.cuda import amp\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, disable=args.silent or silent, desc=\"Running Evaluation\"):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = self._get_inputs_dict(batch)\n",
    "\n",
    "                if self.args.fp16:\n",
    "                    with amp.autocast():\n",
    "                        outputs = model(**inputs)\n",
    "                        tmp_eval_loss, logits = outputs[:2]\n",
    "                else:\n",
    "                    outputs = model(**inputs)\n",
    "                    tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "                if self.args.n_gpu > 1:\n",
    "                    tmp_eval_loss = tmp_eval_loss.mean()\n",
    "                eval_loss += tmp_eval_loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "                out_input_ids = inputs[\"input_ids\"].detach().cpu().numpy()\n",
    "                out_attention_mask = inputs[\"attention_mask\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "                out_input_ids = np.append(out_input_ids, inputs[\"input_ids\"].detach().cpu().numpy(), axis=0)\n",
    "                out_attention_mask = np.append(\n",
    "                    out_attention_mask, inputs[\"attention_mask\"].detach().cpu().numpy(), axis=0,\n",
    "                )\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        token_logits = preds\n",
    "        preds = np.argmax(preds, axis=2)\n",
    "\n",
    "        label_map = {i: label for i, label in enumerate(self.args.labels_list)}\n",
    "\n",
    "        out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "        preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "        for i in range(out_label_ids.shape[0]):\n",
    "            for j in range(out_label_ids.shape[1]):\n",
    "                if out_label_ids[i, j] != pad_token_label_id:\n",
    "                    out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "                    preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "        word_tokens = []\n",
    "        for i in range(len(preds_list)):\n",
    "            w_log = self._convert_tokens_to_word_logits(\n",
    "                out_input_ids[i], out_label_ids[i], out_attention_mask[i], token_logits[i],\n",
    "            )\n",
    "            word_tokens.append(w_log)\n",
    "\n",
    "        model_outputs = [[word_tokens[i][j] for j in range(len(preds_list[i]))] for i in range(len(preds_list))]\n",
    "\n",
    "        extra_metrics = {}\n",
    "        for metric, func in kwargs.items():\n",
    "            extra_metrics[metric] = func(out_label_list, preds_list)\n",
    "\n",
    "        result = {\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"precision\": precision_score(out_label_list, preds_list),\n",
    "            \"recall\": recall_score(out_label_list, preds_list),\n",
    "            \"f1_score\": f1_score(out_label_list, preds_list),\n",
    "            **extra_metrics,\n",
    "        }\n",
    "\n",
    "        results.update(result)\n",
    "\n",
    "        os.makedirs(eval_output_dir, exist_ok=True)\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            if args.classification_report:\n",
    "                cls_report = classification_report(out_label_list, preds_list, digits=4)\n",
    "                writer.write(\"{}\\n\".format(cls_report))\n",
    "            for key in sorted(result.keys()):\n",
    "                writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n",
    "\n",
    "        if self.args.wandb_project and wandb_log:\n",
    "            wandb.init(project=args.wandb_project, config={**asdict(args)}, **args.wandb_kwargs)\n",
    "\n",
    "            labels_list = sorted(self.args.labels_list)\n",
    "\n",
    "            truth = [tag for out in out_label_list for tag in out]\n",
    "            preds = [tag for pred_out in preds_list for tag in pred_out]\n",
    "            outputs = [np.mean(logits, axis=0) for output in model_outputs for logits in output]\n",
    "\n",
    "            # ROC\n",
    "            wandb.log({\"roc\": wandb.plots.ROC(truth, outputs, labels_list)})\n",
    "\n",
    "            # Precision Recall\n",
    "            wandb.log({\"pr\": wandb.plots.precision_recall(truth, outputs, labels_list)})\n",
    "\n",
    "            # Confusion Matrix\n",
    "            wandb.sklearn.plot_confusion_matrix(\n",
    "                truth, preds, labels=labels_list,\n",
    "            )\n",
    "\n",
    "        return results, model_outputs, preds_list\n",
    "\n",
    "    def predict(self, to_predict, split_on_space=True):\n",
    "        \"\"\"\n",
    "        Performs predictions on a list of text.\n",
    "        Args:\n",
    "            to_predict: A python list of text (str) to be sent to the model for prediction.\n",
    "            split_on_space: If True, each sequence will be split by spaces for assigning labels.\n",
    "                            If False, to_predict must be a a list of lists, with the inner list being a\n",
    "                            list of strings consisting of the split sequences. The outer list is the list of sequences to\n",
    "                            predict on.\n",
    "        Returns:\n",
    "            preds: A Python list of lists with dicts containing each word mapped to its NER tag.\n",
    "            model_outputs: A Python list of lists with dicts containing each word mapped to its list with raw model output.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        device = self.device\n",
    "        model = self.model\n",
    "        args = self.args\n",
    "        pad_token_label_id = self.pad_token_label_id\n",
    "        preds = None\n",
    "\n",
    "        if split_on_space:\n",
    "            if self.args.model_type == \"layoutlm\":\n",
    "                predict_examples = [\n",
    "                    InputExample(\n",
    "                        i, sentence.split(), [self.args.labels_list[0] for word in sentence.split()], x0, y0, x1, y1\n",
    "                    )\n",
    "                    for i, (sentence, x0, y0, x1, y1) in enumerate(to_predict)\n",
    "                ]\n",
    "                to_predict = [sentence for sentence, *_ in to_predict]\n",
    "            else:\n",
    "                predict_examples = [\n",
    "                    InputExample(i, sentence.split(), [self.args.labels_list[0] for word in sentence.split()])\n",
    "                    for i, sentence in enumerate(to_predict)\n",
    "                ]\n",
    "        else:\n",
    "            if self.args.model_type == \"layoutlm\":\n",
    "                predict_examples = [\n",
    "                    InputExample(i, sentence, [self.args.labels_list[0] for word in sentence], x0, y0, x1, y1)\n",
    "                    for i, (sentence, x0, y0, x1, y1) in enumerate(to_predict)\n",
    "                ]\n",
    "                to_predict = [sentence for sentence, *_ in to_predict]\n",
    "            else:\n",
    "                predict_examples = [\n",
    "                    InputExample(i, sentence, [self.args.labels_list[0] for word in sentence])\n",
    "                    for i, sentence in enumerate(to_predict)\n",
    "                ]\n",
    "\n",
    "        eval_dataset = self.load_and_cache_examples(None, to_predict=predict_examples)\n",
    "\n",
    "        eval_sampler = SequentialSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        if self.args.onnx:\n",
    "            model_inputs = self.tokenizer.batch_encode_plus(\n",
    "                to_predict, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "\n",
    "            for input_ids, attention_mask in zip(model_inputs[\"input_ids\"], model_inputs[\"attention_mask\"]):\n",
    "                input_ids = input_ids.unsqueeze(0).detach().cpu().numpy()\n",
    "                attention_mask = attention_mask.unsqueeze(0).detach().cpu().numpy()\n",
    "                inputs_onnx = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "                # Run the model (None = get all the outputs)\n",
    "                output = self.model.run(None, inputs_onnx)\n",
    "\n",
    "                if preds is None:\n",
    "                    preds = output[0]\n",
    "                    out_input_ids = inputs_onnx[\"input_ids\"]\n",
    "                    out_attention_mask = inputs_onnx[\"attention_mask\"]\n",
    "                else:\n",
    "                    preds = np.append(preds, output[0], axis=0)\n",
    "                    out_input_ids = np.append(out_input_ids, inputs_onnx[\"input_ids\"], axis=0)\n",
    "                    out_attention_mask = np.append(out_attention_mask, inputs_onnx[\"attention_mask\"], axis=0,)\n",
    "            out_label_ids = np.zeros_like(out_input_ids)\n",
    "            for index in range(len(out_label_ids)):\n",
    "                out_label_ids[index][0] = -100\n",
    "                out_label_ids[index][-1] = -100\n",
    "        else:\n",
    "            self._move_model_to_device()\n",
    "\n",
    "            eval_loss = 0.0\n",
    "            nb_eval_steps = 0\n",
    "            preds = None\n",
    "            out_label_ids = None\n",
    "            model.eval()\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                model = torch.nn.DataParallel(model)\n",
    "\n",
    "            if self.args.fp16:\n",
    "                from torch.cuda import amp\n",
    "\n",
    "            for batch in tqdm(eval_dataloader, disable=args.silent, desc=\"Running Prediction\"):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inputs = self._get_inputs_dict(batch)\n",
    "\n",
    "                    if self.args.fp16:\n",
    "                        with amp.autocast():\n",
    "                            outputs = model(**inputs)\n",
    "                            tmp_eval_loss, logits = outputs[:2]\n",
    "                    else:\n",
    "                        outputs = model(**inputs)\n",
    "                        tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "                    if self.args.n_gpu > 1:\n",
    "                        tmp_eval_loss = tmp_eval_loss.mean()\n",
    "                    eval_loss += tmp_eval_loss.item()\n",
    "\n",
    "                nb_eval_steps += 1\n",
    "\n",
    "                if preds is None:\n",
    "                    preds = logits.detach().cpu().numpy()\n",
    "                    out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "                    out_input_ids = inputs[\"input_ids\"].detach().cpu().numpy()\n",
    "                    out_attention_mask = inputs[\"attention_mask\"].detach().cpu().numpy()\n",
    "                else:\n",
    "                    preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                    out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "                    out_input_ids = np.append(out_input_ids, inputs[\"input_ids\"].detach().cpu().numpy(), axis=0)\n",
    "                    out_attention_mask = np.append(\n",
    "                        out_attention_mask, inputs[\"attention_mask\"].detach().cpu().numpy(), axis=0,\n",
    "                    )\n",
    "\n",
    "            eval_loss = eval_loss / nb_eval_steps\n",
    "        token_logits = preds\n",
    "        preds = np.argmax(preds, axis=2)\n",
    "\n",
    "        label_map = {i: label for i, label in enumerate(self.args.labels_list)}\n",
    "\n",
    "        out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "        preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "        for i in range(out_label_ids.shape[0]):\n",
    "            for j in range(out_label_ids.shape[1]):\n",
    "                if out_label_ids[i, j] != pad_token_label_id:\n",
    "                    out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "                    preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "        if split_on_space:\n",
    "            preds = [\n",
    "                [{word: preds_list[i][j]} for j, word in enumerate(sentence.split()[: len(preds_list[i])])]\n",
    "                for i, sentence in enumerate(to_predict)\n",
    "            ]\n",
    "        else:\n",
    "            preds = [\n",
    "                [{word: preds_list[i][j]} for j, word in enumerate(sentence[: len(preds_list[i])])]\n",
    "                for i, sentence in enumerate(to_predict)\n",
    "            ]\n",
    "\n",
    "        word_tokens = []\n",
    "        for n, sentence in enumerate(to_predict):\n",
    "            w_log = self._convert_tokens_to_word_logits(\n",
    "                out_input_ids[n], out_label_ids[n], out_attention_mask[n], token_logits[n],\n",
    "            )\n",
    "            word_tokens.append(w_log)\n",
    "\n",
    "        if split_on_space:\n",
    "            model_outputs = [\n",
    "                [{word: word_tokens[i][j]} for j, word in enumerate(sentence.split()[: len(preds_list[i])])]\n",
    "                for i, sentence in enumerate(to_predict)\n",
    "            ]\n",
    "        else:\n",
    "            model_outputs = [\n",
    "                [{word: word_tokens[i][j]} for j, word in enumerate(sentence[: len(preds_list[i])])]\n",
    "                for i, sentence in enumerate(to_predict)\n",
    "            ]\n",
    "\n",
    "        return preds, model_outputs\n",
    "\n",
    "    def _convert_tokens_to_word_logits(self, input_ids, label_ids, attention_mask, logits):\n",
    "\n",
    "        ignore_ids = [\n",
    "            self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token),\n",
    "            self.tokenizer.convert_tokens_to_ids(self.tokenizer.sep_token),\n",
    "            self.tokenizer.convert_tokens_to_ids(self.tokenizer.cls_token),\n",
    "        ]\n",
    "\n",
    "        # Remove unuseful positions\n",
    "        masked_ids = input_ids[(1 == attention_mask)]\n",
    "        masked_labels = label_ids[(1 == attention_mask)]\n",
    "        masked_logits = logits[(1 == attention_mask)]\n",
    "        for id in ignore_ids:\n",
    "            masked_labels = masked_labels[(id != masked_ids)]\n",
    "            masked_logits = masked_logits[(id != masked_ids)]\n",
    "            masked_ids = masked_ids[(id != masked_ids)]\n",
    "\n",
    "        # Map to word logits\n",
    "        word_logits = []\n",
    "        tmp = []\n",
    "        for n, lab in enumerate(masked_labels):\n",
    "            if lab != self.pad_token_label_id:\n",
    "                if n != 0:\n",
    "                    word_logits.append(tmp)\n",
    "                tmp = [list(masked_logits[n])]\n",
    "            else:\n",
    "                tmp.append(list(masked_logits[n]))\n",
    "        word_logits.append(tmp)\n",
    "\n",
    "        return word_logits\n",
    "\n",
    "    def load_and_cache_examples(self, data, evaluate=False, no_cache=False, to_predict=None):\n",
    "        \"\"\"\n",
    "        Reads data_file and generates a TensorDataset containing InputFeatures. Caches the InputFeatures.\n",
    "        Utility function for train() and eval() methods. Not intended to be used directly.\n",
    "        Args:\n",
    "            data: Path to a .txt file containing training or evaluation data OR a pandas DataFrame containing 3 columns - sentence_id, words, labels.\n",
    "                    If a DataFrame is given, each sentence should be split into words, with each word assigned a tag, and with all words from the same sentence given the same sentence_id.\n",
    "            evaluate (optional): Indicates whether the examples are for evaluation or for training.\n",
    "            no_cache (optional): Force feature conversion and prevent caching. I.e. Ignore cached features even if present.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        process_count = self.args.process_count\n",
    "\n",
    "        tokenizer = self.tokenizer\n",
    "        args = self.args\n",
    "\n",
    "        if not no_cache:\n",
    "            no_cache = args.no_cache\n",
    "\n",
    "        mode = \"dev\" if evaluate else \"train\"\n",
    "        if not to_predict and isinstance(data, str) and self.args.lazy_loading:\n",
    "            dataset = LazyNERDataset(data, tokenizer, self.args)\n",
    "        else:\n",
    "            if to_predict:\n",
    "                examples = to_predict\n",
    "                no_cache = True\n",
    "            else:\n",
    "                if isinstance(data, str):\n",
    "                    examples = read_examples_from_file(\n",
    "                        data, mode, bbox=True if self.args.model_type == \"layoutlm\" else False\n",
    "                    )\n",
    "                else:\n",
    "                    if self.args.lazy_loading:\n",
    "                        raise ValueError(\"Input must be given as a path to a file when using lazy loading\")\n",
    "                    examples = get_examples_from_df(data, bbox=True if self.args.model_type == \"layoutlm\" else False)\n",
    "\n",
    "            cached_features_file = os.path.join(\n",
    "                args.cache_dir,\n",
    "                \"cached_{}_{}_{}_{}_{}\".format(\n",
    "                    mode, args.model_type, args.max_seq_length, self.num_labels, len(examples),\n",
    "                ),\n",
    "            )\n",
    "            if not no_cache:\n",
    "                os.makedirs(self.args.cache_dir, exist_ok=True)\n",
    "\n",
    "            if os.path.exists(cached_features_file) and (\n",
    "                (not args.reprocess_input_data and not no_cache)\n",
    "                or (mode == \"dev\" and args.use_cached_eval_features and not no_cache)\n",
    "            ):\n",
    "                features = torch.load(cached_features_file)\n",
    "                logger.info(f\" Features loaded from cache at {cached_features_file}\")\n",
    "            else:\n",
    "                logger.info(\" Converting to features started.\")\n",
    "                features = convert_examples_to_features(\n",
    "                    examples,\n",
    "                    self.args.labels_list,\n",
    "                    self.args.max_seq_length,\n",
    "                    self.tokenizer,\n",
    "                    # XLNet has a CLS token at the end\n",
    "                    cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n",
    "                    cls_token=tokenizer.cls_token,\n",
    "                    cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n",
    "                    sep_token=tokenizer.sep_token,\n",
    "                    # RoBERTa uses an extra separator b/w pairs of sentences,\n",
    "                    # cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "                    sep_token_extra=args.model_type in MODELS_WITH_EXTRA_SEP_TOKEN,\n",
    "                    # PAD on the left for XLNet\n",
    "                    pad_on_left=bool(args.model_type in [\"xlnet\"]),\n",
    "                    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                    pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
    "                    pad_token_label_id=self.pad_token_label_id,\n",
    "                    process_count=process_count,\n",
    "                    silent=args.silent,\n",
    "                    use_multiprocessing=args.use_multiprocessing,\n",
    "                    chunksize=args.multiprocessing_chunksize,\n",
    "                    mode=mode,\n",
    "                    use_multiprocessing_for_evaluation=args.use_multiprocessing_for_evaluation,\n",
    "                )\n",
    "\n",
    "                if not no_cache:\n",
    "                    torch.save(features, cached_features_file)\n",
    "\n",
    "            all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "            all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "            all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "            all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "            if self.args.model_type == \"layoutlm\":\n",
    "                all_bboxes = torch.tensor([f.bboxes for f in features], dtype=torch.long)\n",
    "\n",
    "            if self.args.onnx:\n",
    "                return all_label_ids\n",
    "\n",
    "            if self.args.model_type == \"layoutlm\":\n",
    "                dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_bboxes)\n",
    "            else:\n",
    "                dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def convert_to_onnx(self, output_dir=None, set_onnx_arg=True):\n",
    "        \"\"\"Convert the model to ONNX format and save to output_dir\n",
    "        Args:\n",
    "            output_dir (str, optional): If specified, ONNX model will be saved to output_dir (else args.output_dir will be used). Defaults to None.\n",
    "            set_onnx_arg (bool, optional): Updates the model args to set onnx=True. Defaults to True.\n",
    "        \"\"\"  # noqa\n",
    "        if not output_dir:\n",
    "            output_dir = os.path.join(self.args.output_dir, \"onnx\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        if os.listdir(output_dir):\n",
    "            raise ValueError(\n",
    "                \"Output directory ({}) already exists and is not empty.\"\n",
    "                \" Output directory for onnx conversion must be empty.\".format(output_dir)\n",
    "            )\n",
    "\n",
    "        onnx_model_name = os.path.join(output_dir, \"onnx_model.onnx\")\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            self.save_model(output_dir=temp_dir, model=self.model)\n",
    "\n",
    "            convert(\n",
    "                framework=\"pt\",\n",
    "                model=temp_dir,\n",
    "                tokenizer=self.tokenizer,\n",
    "                output=Path(onnx_model_name),\n",
    "                pipeline_name=\"ner\",\n",
    "                opset=11,\n",
    "            )\n",
    "\n",
    "        self.args.onnx = True\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        self.config.save_pretrained(output_dir)\n",
    "        self._save_model_args(output_dir)\n",
    "\n",
    "    def _move_model_to_device(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _get_last_metrics(self, metric_values):\n",
    "        return {metric: values[-1] for metric, values in metric_values.items()}\n",
    "\n",
    "    def _get_inputs_dict(self, batch):\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"labels\": batch[3],\n",
    "        }\n",
    "        # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
    "        if self.args.model_type in [\"bert\", \"xlnet\", \"albert\", \"layoutlm\"]:\n",
    "            inputs[\"token_type_ids\"] = batch[2]\n",
    "\n",
    "        if self.args.model_type == \"layoutlm\":\n",
    "            inputs[\"bbox\"] = batch[4]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _create_training_progress_scores(self, **kwargs):\n",
    "        extra_metrics = {key: [] for key in kwargs}\n",
    "        training_progress_scores = {\n",
    "            \"global_step\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1_score\": [],\n",
    "            \"train_loss\": [],\n",
    "            \"eval_loss\": [],\n",
    "            **extra_metrics,\n",
    "        }\n",
    "\n",
    "        return training_progress_scores\n",
    "\n",
    "    def save_model(self, output_dir=None, optimizer=None, scheduler=None, model=None, results=None):\n",
    "        if not output_dir:\n",
    "            output_dir = self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        if model and not self.args.no_save:\n",
    "            # Take care of distributed/parallel training\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "            torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "            if optimizer and scheduler and self.args.save_optimizer_and_scheduler:\n",
    "                torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            self._save_model_args(output_dir)\n",
    "\n",
    "        if results:\n",
    "            output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                for key in sorted(results.keys()):\n",
    "                    writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "\n",
    "    def _save_model_args(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.args.save(output_dir)\n",
    "\n",
    "    def _load_model_args(self, input_dir):\n",
    "        args = NERArgs()\n",
    "        args.load(input_dir)\n",
    "        return args\n",
    "\n",
    "    def get_named_parameters(self):\n",
    "        return [n for n, p in self.model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/1 [03:29<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7c22511194b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Predictions on arbitary text strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Some arbitary sentence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Simple Transformers sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-942e17dc42b1>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, to_predict, split_on_space)\u001b[0m\n\u001b[1;32m    919\u001b[0m                 ]\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_cache_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0meval_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-942e17dc42b1>\u001b[0m in \u001b[0;36mload_and_cache_examples\u001b[0;34m(self, data, evaluate, no_cache, to_predict)\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Converting to features started.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m                 features = convert_examples_to_features(\n\u001b[0m\u001b[1;32m   1130\u001b[0m                     \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-4355929e9865>\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, label_list, max_seq_length, tokenizer, cls_token_at_end, cls_token, cls_token_segment_id, sep_token, sep_token_extra, pad_on_left, pad_token, pad_token_segment_id, pad_token_label_id, sequence_a_segment_id, mask_padding_with_zero, process_count, chunksize, silent, use_multiprocessing, mode, use_multiprocessing_for_evaluation)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             features = list(\n\u001b[0m\u001b[1;32m    385\u001b[0m                 tqdm(\n\u001b[1;32m    386\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_examples_with_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 ))\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "# Creating train_df  and eval_df for demonstration\n",
    "train_data = [\n",
    "    [0, \"Simple\", \"B-MISC\"],\n",
    "    [0, \"Transformers\", \"I-MISC\"],\n",
    "    [0, \"started\", \"O\"],\n",
    "    [0, \"with\", \"O\"],\n",
    "    [0, \"text\", \"O\"],\n",
    "    [0, \"classification\", \"B-MISC\"],\n",
    "    [1, \"Simple\", \"B-MISC\"],\n",
    "    [1, \"Transformers\", \"I-MISC\"],\n",
    "    [1, \"can\", \"O\"],\n",
    "    [1, \"now\", \"O\"],\n",
    "    [1, \"perform\", \"O\"],\n",
    "    [1, \"NER\", \"B-MISC\"],\n",
    "]\n",
    "train_df = pd.DataFrame(train_data, columns=[\"sentence_id\", \"words\", \"labels\"])\n",
    "\n",
    "eval_data = [\n",
    "    [0, \"Simple\", \"B-MISC\"],\n",
    "    [0, \"Transformers\", \"I-MISC\"],\n",
    "    [0, \"was\", \"O\"],\n",
    "    [0, \"built\", \"O\"],\n",
    "    [0, \"for\", \"O\"],\n",
    "    [0, \"text\", \"O\"],\n",
    "    [0, \"classification\", \"B-MISC\"],\n",
    "    [1, \"Simple\", \"B-MISC\"],\n",
    "    [1, \"Transformers\", \"I-MISC\"],\n",
    "    [1, \"then\", \"O\"],\n",
    "    [1, \"expanded\", \"O\"],\n",
    "    [1, \"to\", \"O\"],\n",
    "    [1, \"perform\", \"O\"],\n",
    "    [1, \"NER\", \"B-MISC\"],\n",
    "]\n",
    "eval_df = pd.DataFrame(eval_data, columns=[\"sentence_id\", \"words\", \"labels\"])\n",
    "\n",
    "# Create a NERModel\n",
    "model = NERModel(\"bert\", \"bert-base-cased\", args={\"overwrite_output_dir\": True, \"reprocess_input_data\": True}, use_cuda=False)\n",
    "\n",
    "# # Train the model\n",
    "model.train_model(train_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "result, model_outputs, predictions = model.eval_model(eval_df)\n",
    "\n",
    "\n",
    "# Predictions on arbitary text strings\n",
    "sentences = [\"Some arbitary sentence\", \"Simple Transformers sentence\"]\n",
    "predictions, raw_outputs = model.predict(sentences)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# More detailed preditctions\n",
    "for n, (preds, outs) in enumerate(zip(predictions, raw_outputs)):\n",
    "    print(\"\\n___________________________\")\n",
    "    print(\"Sentence: \", sentences[n])\n",
    "    for pred, out in zip(preds, outs):\n",
    "        key = list(pred.keys())[0]\n",
    "        new_out = out[key]\n",
    "        preds = list(softmax(np.mean(new_out, axis=0)))\n",
    "        print(key, pred[key], preds[np.argmax(preds)], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}