{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/patsnap/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import keras\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim import logging\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as  pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "#convert trial data to spacy format\n",
    "df = pd.read_fwf('/Users/patsnap/Desktop/Neo4J_and_other_codes/SPacy_trial_data/8184428/train.txt',header =None)\n",
    "df.columns = ['Words']\n",
    "df['Words'] = df['Words'].str.split(\" \")\n",
    "df = pd.DataFrame(df.Words.values.tolist()).add_prefix('code_')\n",
    "df = df.drop(['code_2','code_3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>variable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>temperature</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>electron</td>\n",
       "      <td>B-CMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paramagnetic</td>\n",
       "      <td>I-CMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resonance</td>\n",
       "      <td>I-CMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words labels\n",
       "0      variable      O\n",
       "1   temperature      O\n",
       "2      electron  B-CMT\n",
       "3  paramagnetic  I-CMT\n",
       "4     resonance  I-CMT"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['words','labels']\n",
    "punctuations = [x for x in string.punctuation]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['words'] in punctuations:\n",
    "        row['labels'] == 'O'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>variable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>temperature</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words labels\n",
       "0     variable      O\n",
       "1  temperature      O"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check_O = df[df.labels == 'O']\n",
    "df_check_O.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-DSC',\n",
       " 'O',\n",
       " 'I-PRO',\n",
       " 'B-SMT',\n",
       " 'B-MAT',\n",
       " 'I-SPL',\n",
       " 'I-CMT',\n",
       " 'I-SMT',\n",
       " 'B-APL',\n",
       " 'B-DSC',\n",
       " 'B-SPL',\n",
       " 'B-CMT',\n",
       " 'B-PRO',\n",
       " 'I-APL',\n",
       " 'I-MAT']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = df.labels.tolist()\n",
    "label_list =list(set(label_list))\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.labels = df.labels.replace(\"B-MA\",\"B-MAT\")\n",
    "df.labels = df.labels.replace(\"B-M\",\"B-MAT\")\n",
    "df.labels = df.labels.replace(\"I-MA\",\"I-MAT\")\n",
    "df.labels = df.labels.replace(\"I-M\",\"I-MAT\")\n",
    "df.labels = df.labels.replace(\"I-DS\",\"I-DSC\")\n",
    "df.labels = df.labels.replace(\"B-DS\",\"B-DSC\")\n",
    "df.labels = df.labels.replace(\"I-D\",\"I-DSC\")\n",
    "df.labels = df.labels.replace(\"B-D\",\"B-DSC\")\n",
    "df.labels = df.labels.replace(\"I-PR\",\"I-PRO\")\n",
    "df.labels = df.labels.replace(\"B-PR\",\"B-PRO\")\n",
    "df.labels = df.labels.replace(\"I-P\",\"I-PRO\")\n",
    "df.labels = df.labels.replace(\"B-P\",\"B-PRO\")\n",
    "df.labels = df.labels.replace(\"I-AP\",\"I-APL\")\n",
    "df.labels = df.labels.replace(\"B-AP\",\"B-APL\")\n",
    "df.labels = df.labels.replace(\"I-A\",\"I-APL\")\n",
    "df.labels = df.labels.replace(\"B-A\",\"B-APL\")\n",
    "df.labels = df.labels.replace(\"I-SM\",\"I-SMT\")\n",
    "df.labels = df.labels.replace(\"B-SM\",\"B-SMT\")\n",
    "df.labels = df.labels.replace(\"I-S\",\"I-SMT\")\n",
    "df.labels = df.labels.replace(\"B-S\",\"B-SMT\")\n",
    "df.labels = df.labels.replace(\"I-CM\",\"I-CMT\")\n",
    "df.labels = df.labels.replace(\"B-CM\",\"B-CMT\")\n",
    "df.labels = df.labels.replace(\"I-C\",\"I-CMT\")\n",
    "df.labels = df.labels.replace(\"B-C\",\"B-CMT\")\n",
    "df.labels = df.labels.replace(\"B-C\",\"B-CMT\")\n",
    "df.labels = df.labels.replace(\"B-\",\"O\")\n",
    "df.labels = df.labels.replace(\"I-\",\"O\")\n",
    "df.labels = df.labels.replace(\"B\",\"O\")\n",
    "df.labels = df.labels.replace(\"I\",\"O\")\n",
    "df.labels = df.labels.replace(\"1/2\",\"O\")\n",
    "df.labels = df.labels.replace(\"\",\"O\")\n",
    "df.labels = df.labels.fillna(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "inorganic_material = df[(df.labels == 'B-MAT') | (df.labels == 'I-MAT')]\n",
    "inorganic_material = inorganic_material.words.tolist()\n",
    "inorganic_material = [x for x in inorganic_material if x not in punctuations]\n",
    "inorganic_material = [x for x in inorganic_material if x not in stop_words]\n",
    "sample_descripter = df[(df.labels == 'B-DSC') | (df.labels == 'I-DSC')]\n",
    "sample_descripter = sample_descripter.words.tolist()\n",
    "sample_descripter = [x for x in sample_descripter if x not in punctuations]\n",
    "sample_descripter = [x for x in sample_descripter if x not in stop_words]\n",
    "material_property = df[(df.labels == 'B-PRO') | (df.labels == 'I-PRO')]\n",
    "material_property = material_property.words.tolist()\n",
    "material_property = [x for x in material_property if x not in punctuations]\n",
    "material_property = [x for x in material_property if x not in stop_words]\n",
    "material_application = df[(df.labels == 'B-APL') | (df.labels == 'I-APL')]\n",
    "material_application = material_application.words.tolist()\n",
    "material_application = [x for x in material_application if x not in punctuations]\n",
    "material_application = [x for x in material_application if x not in stop_words]\n",
    "synthesis_method = df[(df.labels == 'B-SMT') | (df.labels == 'I-SMT')]\n",
    "synthesis_method = synthesis_method.words.tolist()\n",
    "synthesis_method = [x for x in synthesis_method if x not in punctuations]\n",
    "synthesis_method = [x for x in synthesis_method if x not in stop_words]\n",
    "characterization_method = df[(df.labels == 'B-CMT') | (df.labels == 'I-CMT')]\n",
    "characterization_method = characterization_method.words.tolist()\n",
    "characterization_method = [x for x in characterization_method if x not in punctuations]\n",
    "characterization_method = [x for x in characterization_method if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "def ordered_set(in_list):\n",
    "    out_list = []\n",
    "    added = set()\n",
    "    for val in in_list:\n",
    "        if not val in added:\n",
    "            out_list.append(val)\n",
    "            added.add(val)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "inorganic_material = ordered_set(inorganic_material)\n",
    "sample_descripter = ordered_set(sample_descripter)\n",
    "material_property = ordered_set(material_property)\n",
    "material_application = ordered_set(material_application)\n",
    "synthesis_method = ordered_set(synthesis_method)\n",
    "characterization_method = ordered_set(characterization_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/patsnap/Desktop/Neo4J_and_other_codes/SPacy_trial_data/8184428/train_bert.tsv', sep = '\\t',header=False, index =False)\n",
    "unknown_label_list = [\"None\"]\n",
    "def tsv_to_json_format(input_path,output_path,unknown_label_list):\n",
    "    try:\n",
    "        f=open(input_path,'r') # input file\n",
    "        fp=open(output_path, 'w') # output file\n",
    "        data_dict={}\n",
    "        annotations =[]\n",
    "        label_dict={}\n",
    "        s=''\n",
    "        start=0\n",
    "        for line in f:\n",
    "            if line[0:len(line)-1]!='.\\tO':\n",
    "                word,entity=line.split('\\t')\n",
    "                s+=word+\" \"\n",
    "                entity=entity[:len(entity)-1]\n",
    "                if entity not in unknown_label_list:\n",
    "                    if len(entity) != 1:\n",
    "                        d={}\n",
    "                        d['text']=word\n",
    "                        d['start']=start\n",
    "                        d['end']=start+len(word)-1  \n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity]=[]\n",
    "                            label_dict[entity].append(d) \n",
    "                start+=len(word)+1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                label_list=[]\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text']!=''):\n",
    "                            l=[ents,label_dict[ents][i]]\n",
    "                            for j in range(i+1,len(label_dict[ents])): \n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
    "                                    di={}\n",
    "                                    di['start']=label_dict[ents][j]['start']\n",
    "                                    di['end']=label_dict[ents][j]['end']\n",
    "                                    di['text']=label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text']=''\n",
    "                            label_list.append(l)                          \n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label={}\n",
    "                    label['label']=[entities[0]]\n",
    "                    label['points']=entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation']=annotations\n",
    "                annotations=[]\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict={}\n",
    "                start=0\n",
    "                label_dict={}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "tsv_to_json_format(\"/Users/patsnap/Desktop/Neo4J_and_other_codes/SPacy_trial_data/8184428/train_bert.tsv\",'/Users/patsnap/Desktop/Neo4J_and_other_codes/SPacy_trial_data/8184428/train_bert.json',unknown_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "lines=[]\n",
    "with open(\"/Users/patsnap/Desktop/Neo4J_and_other_codes/SPacy_trial_data/8184428/train_bert.json\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    text = data['content']\n",
    "    training_data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>variable temperature electron paramagnetic res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the peak - to - peak linewidth ( DHPP ) , g fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>furthermore , these parameters ( DHPP , g fact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the DHPP , g factor and NS decreased with incr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>characterization of CuInSe2 thin films produce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  variable temperature electron paramagnetic res...\n",
       "1  the peak - to - peak linewidth ( DHPP ) , g fa...\n",
       "2  furthermore , these parameters ( DHPP , g fact...\n",
       "3  the DHPP , g factor and NS decreased with incr...\n",
       "4  characterization of CuInSe2 thin films produce..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scientific_data = pd.DataFrame(training_data)\n",
    "scientific_data = scientific_data.dropna()\n",
    "scientific_data = scientific_data.reset_index(drop=True)\n",
    "scientific_data.columns = ['text']\n",
    "scientific_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4522\n"
     ]
    }
   ],
   "source": [
    "sd_list = []\n",
    "for x in inorganic_material:\n",
    "    sd_list.append(x)\n",
    "for x in sample_descripter:\n",
    "    sd_list.append(x)\n",
    "for x in material_property:\n",
    "    sd_list.append(x)\n",
    "for x in material_application:\n",
    "    sd_list.append(x)\n",
    "for x in synthesis_method:\n",
    "    sd_list.append(x)\n",
    "for x in characterization_method:\n",
    "    sd_list.append(x)\n",
    "\n",
    "sd_list = ordered_set(sd_list)\n",
    "\n",
    "my_idx = {}\n",
    "for i,w in enumerate(sd_list):\n",
    "    my_idx[\" \".join(w)] = i\n",
    "print(len(my_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe = MWETokenizer(sd_list,separator=' ')\n",
    "def spans(txt):\n",
    "    tokens=mwe.tokenize(word_tokenize(txt))\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a3b3b2535d40d597a030c356d381ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3755), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "all_item = []\n",
    "\n",
    "for i in tqdm(range(len(scientific_data))):\n",
    "    word_ls = []\n",
    "    \n",
    "    for token in spans(scientific_data.text[i]):\n",
    " \n",
    "        my_tuple = token[0]\n",
    "        #print(\"###\",token)\n",
    "       \n",
    "        #my_tuples = ' , '.join(map(str, my_tuple))\n",
    "        if token[0] in inorganic_material:\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]         \n",
    "            tag_list = ['I-MAT']*len(pos_list)\n",
    "            tag_list[0] = 'B-MAT' \n",
    "                       \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):           \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)                    \n",
    "             \n",
    "        elif token[0] in material_property:\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-PRO']*len(pos_list)\n",
    "            tag_list[0] = 'B-PRO' \n",
    "            \n",
    "             \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):   \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)\n",
    "        \n",
    "        elif token[0] in material_application:\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-APL']*len(pos_list)\n",
    "            tag_list[0] = 'B-APL' \n",
    "            \n",
    "             \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):   \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)\n",
    "                \n",
    "        elif token[0] in synthesis_method:\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-SMT']*len(pos_list)\n",
    "            tag_list[0] = 'B-SMT' \n",
    "            \n",
    "             \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):   \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)\n",
    "        \n",
    "        elif token[0] in characterization_method:\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-CMT']*len(pos_list)\n",
    "            tag_list[0] = 'B-CMT' \n",
    "            \n",
    "             \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):   \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)\n",
    "                \n",
    "        elif token[0] in sample_descripter:\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-DSC']*len(pos_list)\n",
    "            tag_list[0] = 'B-DSC' \n",
    "            \n",
    "             \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):   \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)               \n",
    "             \n",
    "        else:\n",
    "            #print(type(my_tuple))\n",
    "            my_pos = nltk.pos_tag([my_tuple])[0][1]\n",
    "            new_item = dict({'Sentence #': i+1, 'Tag' : 'O', 'Word': my_tuple.lower(),'POS': my_pos})\n",
    "            all_item.append(new_item)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B-PRO</td>\n",
       "      <td>variable</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B-PRO</td>\n",
       "      <td>temperature</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>B-PRO</td>\n",
       "      <td>electron</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>B-PRO</td>\n",
       "      <td>paramagnetic</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>B-PRO</td>\n",
       "      <td>resonance</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #    Tag          Word POS\n",
       "0           1  B-PRO      variable  JJ\n",
       "1           1  B-PRO   temperature  NN\n",
       "2           1  B-PRO      electron  NN\n",
       "3           1  B-PRO  paramagnetic  JJ\n",
       "4           1  B-PRO     resonance  NN"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(all_item)\n",
    "data.columns\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3755\n"
     ]
    }
   ],
   "source": [
    "#concat sentence\n",
    "getter = SentenceGetter(data)\n",
    "word_list = [ [s[0] for s in sent] for sent in getter.sentences]\n",
    "sentences = word_list\n",
    "sentences[0]\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "print(len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3755\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = list(set(data[\"Tag\"].values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i: t for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8226"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words); \n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-MAT'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 143\n",
    "bs = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['variable', 'temperature', 'electron', 'paramagnetic', 'resonance', 'studies', 'of', 'the', 'NiZn', 'ferrite', '/', 'O2Si', 'nanocomposite', 'effects', 'of', 'the', 'silica', 'content', 'and', 'temperature', 'on', 'the', 'magnetic', 'properties', 'of', 'Fe4NiO8Zn', '/', 'O2Si', 'nanocomposites', 'have', 'been', 'studied', 'by', 'electron', 'paramagnetic', 'resonance', '(', 'EPR', ')', 'technique']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = word_list\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n"
     ]
    }
   ],
   "source": [
    "tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n",
    "print(len(tokens_ids[0]),len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_ids[0]),len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "input_ids = pad_sequences(tokens_ids,\n",
    "                          maxlen=MAX_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")\n",
    "#input_ids = pad_sequences([convert_word2idx(txt) for txt in tokenized_texts],\n",
    "                          #maxlen=MAX_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "print(MAX_LEN)\n",
    "for i in tokens_ids:\n",
    "    if len(i) > MAX_LEN:\n",
    "        #print(tokens_ids)\n",
    "        print(\"need more\")\n",
    "        MAX_LEN = len(i)\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n",
    "len(t_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2106 6124  945 7578 7117 5695 1729 6058 7144 1316 4453 1294  231 7619\n",
      " 1729 6058 5007 3040 6040 6124 5507 6058 3468 3952 1729 7690 4453 1294\n",
      " 1241 4153 7791 4824 4909  945 7578 7117 2986 3825 2720 7975    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"int64\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train test\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to torhc tensor\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3379, 143])\n",
      "torch.Size([3379, 143])\n",
      "torch.Size([3379, 143])\n"
     ]
    }
   ],
   "source": [
    "print(tr_inputs.shape)\n",
    "print(tr_masks.shape)\n",
    "print(tr_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([376, 143])\n",
      "torch.Size([376, 143])\n",
      "torch.Size([376, 143])\n"
     ]
    }
   ],
   "source": [
    "print(val_inputs.shape)\n",
    "print(val_masks.shape)\n",
    "print(val_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune BERT\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.005642738886617392\n",
      "Validation loss: 0.19988851714879274\n",
      "Validation Accuracy: 0.3927192599067599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  10%|█         | 1/10 [1:11:05<10:39:45, 4265.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.2101881817092173\n",
      "Train loss: 0.004327756324511149\n",
      "Validation loss: 0.19262664268414179\n",
      "Validation Accuracy: 0.4443108974358975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  20%|██        | 2/10 [2:22:43<9:30:00, 4275.04s/it] \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.22487214412376966\n",
      "Train loss: 0.004180172605251669\n",
      "Validation loss: 0.18622403219342232\n",
      "Validation Accuracy: 0.5190122377622377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  30%|███       | 3/10 [3:36:15<8:23:32, 4316.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.2503657382174923\n",
      "Train loss: 0.004572242305996797\n",
      "Validation loss: 0.18492686500151953\n",
      "Validation Accuracy: 0.4609557109557109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  40%|████      | 4/10 [4:50:13<7:15:16, 4352.70s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.23026160115682923\n",
      "Train loss: 0.003611870582079853\n",
      "Validation loss: 0.18893760101248822\n",
      "Validation Accuracy: 0.5987762237762237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  50%|█████     | 5/10 [6:03:42<6:04:07, 4369.54s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.28535915216538005\n",
      "Train loss: 0.004188602416279708\n",
      "Validation loss: 0.18239737891902527\n",
      "Validation Accuracy: 0.4628132284382284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  60%|██████    | 6/10 [7:15:53<4:50:32, 4358.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.2304769945124525\n",
      "Train loss: 0.003957735422309785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  70%|███████   | 7/10 [8:15:44<3:26:24, 4128.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.18232093766952553\n",
      "Validation Accuracy: 0.4807510198135198\n",
      "F1-Score: 0.2364478638961883\n",
      "Train loss: 0.003206617165456267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  80%|████████  | 8/10 [9:11:58<2:10:03, 3901.63s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.20123906154185534\n",
      "Validation Accuracy: 0.4164481351981351\n",
      "F1-Score: 0.21673586397785685\n",
      "Train loss: 0.003637606643507134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  90%|█████████ | 9/10 [10:08:11<1:02:23, 3743.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.18374982243403792\n",
      "Validation Accuracy: 0.43817380536130535\n",
      "F1-Score: 0.22301038503738102\n",
      "Train loss: 0.0026998859462970736\n",
      "Validation loss: 0.2106392588466406\n",
      "Validation Accuracy: 0.5433238636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 100%|██████████| 10/10 [11:08:26<00:00, 4010.61s/it] \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.26049039811247493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels, true_inputs = [], [],[]\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        inputs = b_input_ids.to('cpu').numpy()\n",
    "         \n",
    "        true_inputs.append(inputs)\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n",
    "\n",
    "\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "all_data = 0\n",
    "for i,j,ll in zip(pred_tags,valid_tags,val_inputs):\n",
    "    for k,l,kk in zip(i,j,ll):\n",
    "        if k==l   : \n",
    "            count += 1\n",
    "            print(k,l,idx2word[kk.item()])\n",
    "        all_data += 1\n",
    "print(count)\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  4%|▍         | 1/24 [00:11<04:17, 11.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2/24 [00:24<04:20, 11.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 3/24 [00:35<04:05, 11.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 4/24 [00:46<03:48, 11.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 5/24 [00:57<03:33, 11.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 6/24 [01:08<03:22, 11.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 7/24 [01:20<03:15, 11.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 8/24 [01:32<03:04, 11.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 9/24 [01:44<02:54, 11.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 10/24 [01:55<02:40, 11.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 11/24 [02:06<02:29, 11.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 12/24 [02:18<02:17, 11.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 13/24 [02:29<02:04, 11.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 14/24 [02:40<01:52, 11.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 15/24 [02:51<01:40, 11.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 16/24 [03:02<01:29, 11.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 17/24 [03:13<01:17, 11.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 18/24 [03:25<01:08, 11.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 19/24 [03:38<00:59, 11.97s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 20/24 [03:52<00:49, 12.38s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 21/24 [04:05<00:38, 12.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 22/24 [04:18<00:25, 12.71s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 23/24 [04:31<00:12, 12.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 24/24 [04:37<00:00, 11.58s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2106392588466406\n",
      "Validation Accuracy: 0.5433238636363636\n",
      "Validation F1-Score: 0.26049039811247493\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "true_inputs = []\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "print(len(valid_dataloader))\n",
    "for batch in tqdm(valid_dataloader):\n",
    "    #print(len(batch))\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    inputs = b_input_ids.to('cpu').numpy()\n",
    "    true_inputs.append(inputs)\n",
    "    \n",
    "    \n",
    "    true_labels.append(label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
    "valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n",
    "\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "all_data = 0\n",
    "for i,j,ll in zip(pred_tags,valid_tags,val_inputs):\n",
    "    for k,l,kk in zip(i,j,ll):\n",
    "        if k==l   : \n",
    "            count += 1\n",
    "            #print(k,l,idx2word[kk.item()])  #get this and add this as list of tuples to later convert it to dataframe\n",
    "        all_data += 1\n",
    "#print(count)\n",
    "#print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
