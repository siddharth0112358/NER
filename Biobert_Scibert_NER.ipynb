{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fran-martinez/bio_ner_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MeRajat/SolvingAlmostAnythingWithBert/tree/master/biobert_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6a1b358de34e71b8039308d696bf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1558.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82910066da24737aca2672f17af1b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=258935.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d3b4a8f2ea4f45891ba6c82df29e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9e7f19e06a4509a14e0bd0f225ebc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a8527068e7491ba4b454e1cd563e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=439757565.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'glucocorticoid',\n",
       "  'score': 0.9894881248474121,\n",
       "  'entity': 'B-protein',\n",
       "  'index': 9},\n",
       " {'word': 'receptor',\n",
       "  'score': 0.989505410194397,\n",
       "  'entity': 'I-protein',\n",
       "  'index': 10},\n",
       " {'word': 'normal',\n",
       "  'score': 0.7680374383926392,\n",
       "  'entity': 'B-cell_type',\n",
       "  'index': 12},\n",
       " {'word': 'cs',\n",
       "  'score': 0.5176804065704346,\n",
       "  'entity': 'I-cell_type',\n",
       "  'index': 13},\n",
       " {'word': 'lymphocytes',\n",
       "  'score': 0.9898492097854614,\n",
       "  'entity': 'I-cell_type',\n",
       "  'index': 14}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = \"Mouse thymus was used as a source of glucocorticoid receptor from normal CS lymphocytes.\"\n",
    "\n",
    "nlp_ner = pipeline(\"ner\",\n",
    "                   model='fran-martinez/scibert_scivocab_cased_ner_jnlpba',\n",
    "                   tokenizer='fran-martinez/scibert_scivocab_cased_ner_jnlpba')\n",
    "\n",
    "nlp_ner(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse -> O\n",
      "thymus -> O\n",
      "was -> O\n",
      "used -> O\n",
      "as -> O\n",
      "a -> O\n",
      "source -> O\n",
      "of -> O\n",
      "glucocorticoid -> B-protein\n",
      "receptor -> I-protein\n",
      "from -> O\n",
      "normal -> B-cell_type\n",
      "cs -> I-cell_type\n",
      "lymphocytes -> I-cell_type\n",
      ". -> O\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Example\n",
    "text = \"Mouse thymus was used as a source of glucocorticoid receptor from normal CS lymphocytes.\"\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fran-martinez/scibert_scivocab_cased_ner_jnlpba\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"fran-martinez/scibert_scivocab_cased_ner_jnlpba\")\n",
    "\n",
    "# Get input for BERT\n",
    "input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# From the output let's take the first element of the tuple.\n",
    "# Then, let's get rid of [CLS] and [SEP] tokens (first and last)\n",
    "predictions = outputs[0].argmax(axis=-1)[0][1:-1]\n",
    "\n",
    "# Map label class indexes to string labels.\n",
    "for token, pred in zip(tokenizer.tokenize(text), predictions):\n",
    "    print(token, '->', model.config.id2label[pred.numpy().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9ece4611b84044ba344118cd902a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=454.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09dc47468b1473f97b344e96b88a9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f374178009604585836a333b68582b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2456785f2c6a4d8ead18f840a5f58d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=136.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44ec0e6b7124df4a103f604a0be51b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433288887.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at gsarti/biobert-nli and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': '[CLS]',\n",
       "  'score': 0.6273071765899658,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 0},\n",
       " {'word': 'Mouse',\n",
       "  'score': 0.5842256546020508,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 1},\n",
       " {'word': 'thy', 'score': 0.5531447529792786, 'entity': 'LABEL_1', 'index': 2},\n",
       " {'word': '##mus',\n",
       "  'score': 0.5514998435974121,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 3},\n",
       " {'word': 'was', 'score': 0.5502050518989563, 'entity': 'LABEL_0', 'index': 4},\n",
       " {'word': 'used',\n",
       "  'score': 0.5568333268165588,\n",
       "  'entity': 'LABEL_0',\n",
       "  'index': 5},\n",
       " {'word': 'as', 'score': 0.5459309220314026, 'entity': 'LABEL_0', 'index': 6},\n",
       " {'word': 'a', 'score': 0.5106245279312134, 'entity': 'LABEL_1', 'index': 7},\n",
       " {'word': 'source',\n",
       "  'score': 0.5343674421310425,\n",
       "  'entity': 'LABEL_0',\n",
       "  'index': 8},\n",
       " {'word': 'of', 'score': 0.5214571356773376, 'entity': 'LABEL_1', 'index': 9},\n",
       " {'word': 'g', 'score': 0.6315820813179016, 'entity': 'LABEL_1', 'index': 10},\n",
       " {'word': '##lu',\n",
       "  'score': 0.5991693139076233,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 11},\n",
       " {'word': '##co',\n",
       "  'score': 0.539835512638092,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 12},\n",
       " {'word': '##cor',\n",
       "  'score': 0.6031162738800049,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 13},\n",
       " {'word': '##tic',\n",
       "  'score': 0.5725679993629456,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 14},\n",
       " {'word': '##oid',\n",
       "  'score': 0.5044581890106201,\n",
       "  'entity': 'LABEL_0',\n",
       "  'index': 15},\n",
       " {'word': 'receptor',\n",
       "  'score': 0.5130326151847839,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 16},\n",
       " {'word': 'from',\n",
       "  'score': 0.5498432517051697,\n",
       "  'entity': 'LABEL_0',\n",
       "  'index': 17},\n",
       " {'word': 'normal',\n",
       "  'score': 0.5114613175392151,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 18},\n",
       " {'word': 'CS', 'score': 0.6222296953201294, 'entity': 'LABEL_1', 'index': 19},\n",
       " {'word': 'l', 'score': 0.504404604434967, 'entity': 'LABEL_1', 'index': 20},\n",
       " {'word': '##ymph',\n",
       "  'score': 0.5151495933532715,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 21},\n",
       " {'word': '##ocytes',\n",
       "  'score': 0.5946153998374939,\n",
       "  'entity': 'LABEL_0',\n",
       "  'index': 22},\n",
       " {'word': '.', 'score': 0.5368720889091492, 'entity': 'LABEL_1', 'index': 23},\n",
       " {'word': '[SEP]',\n",
       "  'score': 0.6273073554039001,\n",
       "  'entity': 'LABEL_1',\n",
       "  'index': 24}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = \"Mouse thymus was used as a source of glucocorticoid receptor from normal CS lymphocytes.\"\n",
    "\n",
    "nlp_ner = pipeline(\"ner\",\n",
    "                   model='gsarti/biobert-nli',\n",
    "                   tokenizer='gsarti/biobert-nli')\n",
    "\n",
    "nlp_ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# text = \"Mouse thymus was used as a source of glucocorticoid receptor from normal CS lymphocytes.\"\n",
    "\n",
    "# nlp_ner = pipeline(\"ner\",\n",
    "#                    model='dmis-lab/biobert-v1.1',\n",
    "#                    tokenizer='dmis-lab/biobert-v1.1')\n",
    "\n",
    "# nlp_ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# text = \"Mouse thymus was used as a source of glucocorticoid receptor from normal CS lymphocytes.\"\n",
    "\n",
    "# nlp_ner = pipeline(\"ner\",\n",
    "#                    model='ktrapeznikov/biobert_v1.1_pubmed_squad_v2',\n",
    "#                    tokenizer='ktrapeznikov/biobert_v1.1_pubmed_squad_v2')\n",
    "\n",
    "# nlp_ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (get_linear_schedule_with_warmup,\n",
    "                          BertForTokenClassification,\n",
    "                          AutoTokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataSample:\n",
    "    \"\"\"\n",
    "    A single training/test example (sentence) for token classification.\n",
    "    \"\"\"\n",
    "    words: List[str]\n",
    "    labels: List[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputBert:\n",
    "    \"\"\"\n",
    "    A single set of features of data.\n",
    "    Property names are the same names as the corresponding inputs to a BERT model.\n",
    "    \"\"\"\n",
    "    input_ids: torch.tensor\n",
    "    attention_mask: torch.tensor\n",
    "    token_type_ids: torch.tensor\n",
    "    labels: Optional[torch.tensor] = None\n",
    "\n",
    "\n",
    "class NerDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset: List[DataSample],\n",
    "                 tokenizer: PreTrainedTokenizer,\n",
    "                 labels2ind: Dict[str, int],\n",
    "                 max_len_seq: int = 512,\n",
    "                 bert_hugging: bool = True):\n",
    "        \"\"\"\n",
    "        Class that builds a torch Dataset specially designed for NER data.\n",
    "        Args:\n",
    "            dataset (list of `DataSample` instances): Each data sample is a dataclass\n",
    "                that contains two fields: `words` and `labels`. Both are lists of `str`.\n",
    "            tokenizer (`PreTrainedTokenizer`): Pre-trained tokenizer from transformers\n",
    "                library. Usually loaded as `AutoTokenizer.from_pretrained(...)`.\n",
    "            labels2ind (`dict`): maps `str` class labels into `int` indexes.\n",
    "            max_len_seq (`int`): Max length sequence for each example (sentence).\n",
    "            bert_hugging (`bool`):\n",
    "        \"\"\"\n",
    "        super(NerDataset).__init__()\n",
    "        self.bert_hugging = bert_hugging\n",
    "        self.max_len_seq = max_len_seq\n",
    "        self.label2ind = labels2ind\n",
    "        self.features = data2tensors(data=dataset,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     label2idx=self.label2ind,\n",
    "                                     max_seq_len=max_len_seq,\n",
    "                                     pad_token_label_id=nn.CrossEntropyLoss().ignore_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> Union[Dict[str, torch.tensor],\n",
    "                                      Tuple[List[torch.tensor], torch.tensor]]:\n",
    "        if self.bert_hugging:\n",
    "            return asdict(self.features[i])\n",
    "        else:\n",
    "            inputs = asdict(self.features[i])\n",
    "            labels = inputs.pop('labels')\n",
    "            return list(inputs.values()), labels\n",
    "\n",
    "\n",
    "def get_labels(data: List[DataSample]) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Automatically extract labels types from the data and its count.\n",
    "    Args:\n",
    "        data (list of `DataSample`): Each data sample is a dataclass that contains\n",
    "            two fields: `words` and `labels`. Both are lists of `str`.\n",
    "    Returns:\n",
    "        labels2idx (`dict`): maps `str` class labels into `int` indexes.\n",
    "        labels_count(`dict`): The number of words for each class label that appears in\n",
    "            the dataset. Usufull information if you want to apply class weights on\n",
    "            imbalanced data.\n",
    "    \"\"\"\n",
    "    labels = set()\n",
    "    labels_counts = defaultdict(int)\n",
    "    for sent in data:\n",
    "        labels.update(sent.labels)\n",
    "\n",
    "        for label_ in sent.labels:\n",
    "            labels_counts[label_] += 1\n",
    "\n",
    "    if \"O\" not in labels:\n",
    "        labels.add('O')\n",
    "        labels_counts['0'] = 0\n",
    "\n",
    "    # Convert list of labels ind a mapping labels -> index\n",
    "    labels2idx = {label_: i for i, label_ in enumerate(labels)}\n",
    "    return labels2idx, dict(labels_counts)\n",
    "\n",
    "\n",
    "def get_class_weight_tensor(labels2ind: Dict[str, int],\n",
    "                            labels_count: Dict[str, int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the class weights based on the class labels frequency within the dataset.\n",
    "    Args:\n",
    "        labels2ind (`dict`): maps `str` class labels into `int` indexes.\n",
    "        labels_count (`dict`): The number of words for each class label that appears in\n",
    "            the dataset.\n",
    "    Returns:\n",
    "        torch.Tensor with the class weights. Size (num_classes).\n",
    "    \"\"\"\n",
    "    label2ind_list = [(k, v) for k, v in labels2ind.items()]\n",
    "    label2ind_list.sort(key=lambda x: x[1])\n",
    "    total_labels = sum([count for label, count in labels_count.items()])\n",
    "    class_weights = [total_labels/labels_count[label] for label, _ in label2ind_list]\n",
    "    return torch.tensor(np.array(class_weights)/max(class_weights), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def read_data_from_file(file_path: str, sep: str = '\\t') -> List[DataSample]:\n",
    "    \"\"\"\n",
    "    Load data from a txt file (BIO tagging format) and transform it into the\n",
    "    required format (list of `DataSample` instances).\n",
    "    Args:\n",
    "        file_path (`str`): complete path where the data is located (path + filename).\n",
    "        sep (`str`): Symbol used to separete word from label at each line. Default `\\t`.\n",
    "    Returns:\n",
    "        List of `DataSample` instances containing words and labels.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    words = []\n",
    "    labels = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            splits = line.split(sep)\n",
    "            if len(splits) > 1:\n",
    "                words.append(splits[0])\n",
    "                labels.append(splits[-1].replace('\\n', ''))\n",
    "            else:\n",
    "                examples.append(DataSample(words=words, labels=labels))\n",
    "                words = []\n",
    "                labels = []\n",
    "    return examples\n",
    "\n",
    "\n",
    "def data2tensors(data: List[DataSample],\n",
    "                 tokenizer: PreTrainedTokenizer,\n",
    "                 label2idx: Dict[str, int],\n",
    "                 pad_token_label_id: int = -100,\n",
    "                 max_seq_len: int = 512) -> List[InputBert]:\n",
    "    \"\"\"\n",
    "    Takes data and converts it into tensors to feed the neural network.\n",
    "    Args:\n",
    "        data (`list`): List of `DataSample` instances containing words and labels.\n",
    "        tokenizer (`PreTrainedTokenizer`): Pre-trained tokenizer from transformers\n",
    "            library. Usually loaded as `AutoTokenizer.from_pretrained(...)`.\n",
    "        label2idx (`dict`): maps `str` class labels into `int` indexes.\n",
    "        pad_token_label_id (`int`): index to define the special token [PAD]\n",
    "        max_seq_len (`int`): Max sequence length.\n",
    "    Returns:\n",
    "        List of `InputBert` instances. `InputBert` is a dataclass that contains\n",
    "        `input_ids`, `attention_mask`, `token_type_ids` and `labels` (Optional).\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "    for sentence in data:\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(sentence.words, sentence.labels):\n",
    "            subword_tokens = tokenizer.tokenize(text=word)\n",
    "\n",
    "            # BERT could return an empty list of subtokens\n",
    "            if len(subword_tokens) > 0:\n",
    "                tokens.extend(subword_tokens)\n",
    "\n",
    "                # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "                label_ids.extend([label2idx[label]] + [pad_token_label_id] * (len(subword_tokens) - 1))\n",
    "                # if label.startswith('B'):\n",
    "                #     label_ids.extend([label2idx[label]] + [label2idx[f\"I{label[1:]}\"]] * (len(subword_tokens) - 1))\n",
    "                # else:\n",
    "                #     label_ids.extend([label2idx[label]] + [label2idx[label]] * (len(subword_tokens) - 1))\n",
    "\n",
    "        # Drop part of the sequence longer than max_seq_len (account also for [CLS] and [SEP])\n",
    "        if len(tokens) > max_seq_len - 2:\n",
    "            tokens = tokens[:max_seq_len - 2]\n",
    "            label_ids = label_ids[: max_seq_len - 2]\n",
    "\n",
    "        # Add special tokens  for the list of tokens and its corresponding labels.\n",
    "        # For BERT: cls_token = '[CLS]' and sep_token = '[SEP]'\n",
    "        # For RoBERTa: cls_token = '<s>' and sep_token = '</s>'\n",
    "        tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "        label_ids = [pad_token_label_id] + label_ids + [pad_token_label_id]\n",
    "\n",
    "        # Create an attention mask (used to locate the padding)\n",
    "        padding_len = (max_seq_len - len(tokens))\n",
    "        attention_mask = [1] * len(tokens) + [0] * padding_len\n",
    "\n",
    "        # Add padding\n",
    "        tokens += [tokenizer.pad_token] * padding_len\n",
    "        label_ids += [pad_token_label_id] * padding_len\n",
    "\n",
    "        # Convert tokens to ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Create segment_id. All zeros since we only have one sentence\n",
    "        segment_ids = [0] * max_seq_len\n",
    "\n",
    "        # Assert all the input has the expected length\n",
    "        assert len(input_ids) == max_seq_len\n",
    "        assert len(label_ids) == max_seq_len\n",
    "        assert len(attention_mask) == max_seq_len\n",
    "        assert len(segment_ids) == max_seq_len\n",
    "\n",
    "        # Append input features for each sequence/sentence\n",
    "        features.append((InputBert(input_ids=torch.tensor(input_ids),\n",
    "                                   attention_mask=torch.tensor(attention_mask),\n",
    "                                   token_type_ids=torch.tensor(segment_ids),\n",
    "                                   labels=torch.tensor(label_ids))))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "import torch\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def get_optimizer_with_weight_decay(model: PreTrainedModel,\n",
    "                                    optimizer: torch.optim.Optimizer,\n",
    "                                    learning_rate: Union[float, int],\n",
    "                                    weight_decay: Union[float, int]) -> torch.optim.Optimizer:\n",
    "    \"\"\"\n",
    "    Apply weight decay to all the network parameters but those called `bias` or  `LayerNorm.weight`.\n",
    "    Args:\n",
    "        model (`PreTrainedModel`): model to apply weight decay.\n",
    "        optimizer (`torch.optim.Optimizer`): The optimizer to use during training.\n",
    "        learning_rate (`float` or `int`): value of the learning rate to use during training.\n",
    "        weight_decay (`float` or `int`): value of the weight decay to apply.\n",
    "    Returns:\n",
    "        optimizer (`torch.optim.Optimizer`): the optimizer instantiated with the selected\n",
    "        learning rate and the parameters with and without weight decay.\n",
    "    \"\"\"\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    params = [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)]\n",
    "    params_nd = [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)]\n",
    "    optimizer_grouped_parameters = [{\"params\": params, \"weight_decay\": weight_decay},\n",
    "                                    {\"params\": params_nd, \"weight_decay\": 0.0}]\n",
    "\n",
    "    return optimizer(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-0.0.12.tar.gz (21 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from seqeval) (1.18.5)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from seqeval) (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from Keras>=2.2.4->seqeval) (1.5.2)\n",
      "Requirement already satisfied: h5py in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from Keras>=2.2.4->seqeval) (5.3.1)\n",
      "Requirement already satisfied: six in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from h5py->Keras>=2.2.4->seqeval) (1.12.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7423 sha256=fbc0723be9f04fb8dd00c6094dab0b3aedcf39ecbe9724fb38eeac54f31cc915\n",
      "  Stored in directory: /Users/sdeshpande/Library/Caches/pip/wheels/04/bf/20/90daf50b5a8173fc6ee715e6ebb0a16202cc43df471e323e88\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-0.0.12\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from typing import List, Tuple, Union, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from seqeval.metrics import f1_score\n",
    "from sklearn.metrics import classification_report as sklearn_report\n",
    "from torch import nn\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class BertTrainer:\n",
    "    def __init__(self,\n",
    "                 model: PreTrainedModel,\n",
    "                 tokenizer: PreTrainedTokenizer,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 n_epochs: int,\n",
    "                 labels2ind: Dict[str, int],\n",
    "                 scheduler: Optional[torch.optim.lr_scheduler.LambdaLR] = None,\n",
    "                 device: str = 'cpu',\n",
    "                 clipping: Optional[Union[int, float]] = None,\n",
    "                 accumulate_grad_every: int = 1,\n",
    "                 print_every: int = 10,\n",
    "                 print_val_mistakes: bool = False,\n",
    "                 output_dir: str = './'):\n",
    "\n",
    "        \"\"\"\n",
    "        Complete training and evaluation loop in Pytorch specially designed for\n",
    "        BERT-based models from transformers library. It allows to save the model\n",
    "        from the epoch with the best F1-score and the tokenizer. The class\n",
    "        optionally generates reports and figures with the obtained results that\n",
    "        are automatically stored in disk.\n",
    "        Args:\n",
    "            model (`PreTrainedModel`): Pre-trained model from transformers library.\n",
    "                For NER, usually loaded as `BertForTokenClassification.from_pretrained(...)`\n",
    "            tokenizer (`PreTrainedTokenizer`): Pre-trained tokenizer from transformers library.\n",
    "                Usually loaded as `AutoTokenizer.from_pretrained(...)`\n",
    "            optimizer (`torch.optim.Optimizer`): Pytorch Optimizer\n",
    "            n_epochs (`int`): Number of epochs to train.\n",
    "            labels2ind (`dict`): maps `str` class labels into `int` indexes.\n",
    "            scheduler (`torch.optim.lr_scheduler.LambdaLR`, `Optional`): Pytorch scheduler. It sets a\n",
    "                different learning rate for each training step to update the network weights.\n",
    "            device (`str`): Type of device where to train the network. It must be `cpu` or `cuda`.\n",
    "            clipping (`int` or `float`, `Optional`): max norm to apply to the gradients. If None,\n",
    "                no graddient clipping is applied.\n",
    "            accumulate_grad_every (`int`): How often you want to accumulate the gradient. This is useful\n",
    "                when there are limitations in the batch size due to memory issues. Let's say that in your\n",
    "                GPU only fits a model with batch size of 8 and you want to try a batch size of 32. Then,\n",
    "                you should set this parameter to 4 (8*4=32). Internally, a loop will be ran 4 times\n",
    "                accumulating the gradient for each step. Later, the network parameters will be updated.\n",
    "                So at the end, this is equivalent to train your network with a batch size of 32. The batch\n",
    "                size is inferred from `dataloader_train` argument.\n",
    "            print_every (`int`): How often you want to print loss. Measured in batches where a batch is\n",
    "                considered batch_size * accumulate_grad_every.\n",
    "            print_val_mistakes (`bool`): whether to print validation examples (sentences) where the model\n",
    "                commits at least one mistake. It is printed after each epoch. The printed info is the word\n",
    "                within each sentence, its predicted label and the real label. This is very useful to\n",
    "                inspect the behaviour of your model.\n",
    "            output_dir (`str`): Directory where file reports and images are saved.\n",
    "        Methods:\n",
    "            train(dataloader_train: DataLoader, dataloader_val: Optional[DataLoader] = None)\n",
    "                Complete training and evaluation (optional) loop in Pytorch.\n",
    "            evaluate(dataloader_val: DataLoader, epoch: int = 0, verbose: bool = False)\n",
    "                Evaluation on test data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.n_epochs = n_epochs\n",
    "        self.labels2ind = labels2ind\n",
    "        self.inds2labels = {v: k for k, v in self.labels2ind.items()}\n",
    "        self.device = device\n",
    "        self.clipping = clipping\n",
    "        self.accumulate_grad_every = accumulate_grad_every\n",
    "        self.print_every = print_every\n",
    "        self.print_val_mistakes = print_val_mistakes\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def _reformat_predictions(self,\n",
    "                              y_true: List[List[int]],\n",
    "                              y_pred: List[List[int]],\n",
    "                              input_ids: List[List[str]]\n",
    "                              ) -> Tuple[List[List[str]],\n",
    "                                         List[List[str]],\n",
    "                                         List[List[str]]]:\n",
    "        \"\"\"\n",
    "        Takes batch of tokens, labels (class indexes) and predictions (class indexes)\n",
    "        and get rid of unwanted tokens, that is, those that have as label the index\n",
    "        to ignore (i.e. padding tokens).  It also converts the label and prediction\n",
    "        indexes into their corresponding class name.\n",
    "        Args:\n",
    "            y_true (list of lists `int`): indexes of the real labels\n",
    "            y_pred (list of lists `int`): indexes of the predicted classes\n",
    "            input_ids (list of lists `str`) : tokens\n",
    "        Returns:\n",
    "            Tuple that contains the transformed input arguments\n",
    "        \"\"\"\n",
    "        # Map indexes to labels and remove ignored indexes\n",
    "        true_list = [[] for _ in range(len(y_true))]\n",
    "        pred_list = [[] for _ in range(len(y_pred))]\n",
    "        input_list = [[] for _ in range(len(input_ids))]\n",
    "\n",
    "        for i in range(len(y_true)):\n",
    "            for j in range(len(y_true[0])):\n",
    "                if y_true[i][j] != CrossEntropyLoss().ignore_index:\n",
    "                    true_list[i].append(self.inds2labels[y_true[i][j]])\n",
    "                    pred_list[i].append(self.inds2labels[y_pred[i][j]])\n",
    "                    input_list[i].append(input_ids[i][j])\n",
    "\n",
    "        return true_list, pred_list, input_list\n",
    "\n",
    "    def _print_missclassified_val_examples(self,\n",
    "                                           y_true: List[List[str]],\n",
    "                                           y_pred: List[List[str]],\n",
    "                                           input_ids: List[List[str]]):\n",
    "        \"\"\"\n",
    "        print validation examples (sentences) where the model commits at least\n",
    "        one mistake. It is printed after each epoch. This is very useful to\n",
    "        inspect the behaviour of your model.\n",
    "        Args:\n",
    "            y_true (list of lists `str`): real labels\n",
    "            y_pred (list of lists `str`): predicted classes\n",
    "            input_ids (list of lists `str`) : tokens\n",
    "        Examples::\n",
    "                TOKEN          LABEL          PRED\n",
    "                immunostaining O              O\n",
    "                showed         O              O\n",
    "                the            O              O\n",
    "                estrogen       B-cell_type    B-cell_type\n",
    "                receptor       I-cell_type    I-cell_type\n",
    "                cells          I-cell_type    O\n",
    "                                  ·\n",
    "                                  ·\n",
    "                                  ·\n",
    "                synovial       O              O\n",
    "                tissues        O              O\n",
    "                .              O              O\n",
    "        \"\"\"\n",
    "        # Print some examples (where the model fails)\n",
    "        for i in range(len(input_ids)):\n",
    "            if y_true[i] != y_pred[i]:\n",
    "                tokens = self.tokenizer.convert_ids_to_tokens(input_ids[i])\n",
    "                max_len_token = max([len(t) for t in tokens] +\n",
    "                                    [len(la) for la in self.labels2ind.keys()])\n",
    "\n",
    "                print(f\"\\n{'TOKEN':<{max_len_token}}\",\n",
    "                      f\"{'LABEL':<{max_len_token}}\",\n",
    "                      f\"{'PRED':<{max_len_token}}\")\n",
    "\n",
    "                for token, label_true, label_pred in zip(tokens, y_true[i], y_pred[i]):\n",
    "                    print(f\"{token:<{max_len_token}}\",\n",
    "                          f\"{label_true:<{max_len_token}}\",\n",
    "                          f\"{label_pred:<{max_len_token}}\")\n",
    "\n",
    "    def _write_report_to_file(self,\n",
    "                              report_entities: str,\n",
    "                              report_tokens: str,\n",
    "                              epoch: int,\n",
    "                              tr_loss: float,\n",
    "                              val_loss: float):\n",
    "        \"\"\"\n",
    "        Writes and saves the following info into a file called `classification_report.txt`\n",
    "        within the directory `output_dir` for the model from the best epoch:\n",
    "            - Classification report at span/entity level (for validation dataset).\n",
    "            - Classification report at word level (for validation dataset).\n",
    "            - Epoch where the best model was found (best F1-score in validation dataset)\n",
    "            - Training loss from the best epoch.\n",
    "            - Validation loss from the best epoch.\n",
    "        Args:\n",
    "            report_entities (`str`): classification report at entity/span level.\n",
    "            report_tokens (`str`): classification report at word level.\n",
    "            epoch (`int`): epoch\n",
    "            tr_loss (`float`): training loss\n",
    "            val_loss (`float`): validation loss\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.output_dir, 'classification_report.txt'), 'w') as f:\n",
    "            f.write(report_entities)\n",
    "            f.write(f'\\n{report_tokens}')\n",
    "            f.write(f\"\\nEpoch: {epoch} \"\n",
    "                    f\"\\n- Training Loss: {tr_loss}\"\n",
    "                    f\"\\n- Validation Loss: {val_loss}\")\n",
    "\n",
    "    def _save_model(self):\n",
    "        if not isinstance(self.model, PreTrainedModel):\n",
    "            raise ValueError(\"Trainer.model appears to not be a PreTrainedModel\")\n",
    "        self.model.save_pretrained(self.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "\n",
    "    def _estimate_gradients(self, batch: Dict[str, torch.Tensor]) -> float:\n",
    "        # Send tensors to device\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "        # estimate loss and gradient\n",
    "        loss, _ = self.model(**batch)\n",
    "        loss.backward()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def _update_network_params(self):\n",
    "        # Graddient clipping\n",
    "        if self.clipping is not None:\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.clipping)\n",
    "\n",
    "        # Udate parameters (accumulated gradient based on accumulated grad)\n",
    "        self.optimizer.step()\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "    def _validation_step(self,\n",
    "                         batch: Dict[str, torch.Tensor]\n",
    "                         ) -> Tuple[float, np.ndarray]:\n",
    "        # Send tensors to device\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "        # Predict and estimate error\n",
    "        with torch.no_grad():\n",
    "            loss, pred = self.model(**batch)\n",
    "\n",
    "        return loss.item(), pred.detach().cpu().numpy()\n",
    "\n",
    "    def evaluate(self,\n",
    "                 dataloader_val: DataLoader,\n",
    "                 epoch: int = 0,\n",
    "                 verbose: bool = False) -> Tuple[float, float, str, str]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataloader_val:\n",
    "            epoch:\n",
    "            verbose:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        n_steps_val = len(dataloader_val)\n",
    "        self.model.eval()\n",
    "\n",
    "        val_loss_cum = .0\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        input_ids = []\n",
    "        for step, batch in enumerate(dataloader_val):\n",
    "            val_loss, pred = self._validation_step(batch)\n",
    "            val_loss_cum += val_loss\n",
    "            y_true.extend(batch['labels'].tolist())\n",
    "            y_pred.extend(pred.argmax(axis=-1).tolist())\n",
    "            input_ids.extend(batch['input_ids'].tolist())\n",
    "\n",
    "        y_true, y_pred, input_ids = self._reformat_predictions(y_true, y_pred, input_ids)\n",
    "\n",
    "        # Performance Reports and loss\n",
    "        report_entities = seqeval_report(y_true=y_true, y_pred=y_pred, digits=4)\n",
    "        report_tokens = sklearn_report(y_true=list(itertools.chain(*y_true)),\n",
    "                                       y_pred=list(itertools.chain(*y_pred)), digits=4)\n",
    "\n",
    "        loss_val_epoch = val_loss_cum / n_steps_val\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"- Epoch: {epoch}/{self.n_epochs - 1} - Validation Loss: {loss_val_epoch}\")\n",
    "            print(report_entities)\n",
    "            print(report_tokens)\n",
    "\n",
    "        # Print some examples (where the model fails)\n",
    "        if self.print_val_mistakes and verbose:\n",
    "            self._print_missclassified_val_examples(y_true, y_pred, input_ids)\n",
    "\n",
    "        # Save model and write report to txt file\n",
    "        f1 = f1_score(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "        return loss_val_epoch, f1, report_entities, report_tokens\n",
    "\n",
    "    def train(self,\n",
    "              dataloader_train: DataLoader,\n",
    "              dataloader_val: Optional[DataLoader] = None\n",
    "              ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Complete training and evaluation (optional) loop in Pytorch.\n",
    "        Args:\n",
    "            dataloader_train (`torch.utils.data.dataloader.DataLoader`): Pytorch dataloader.\n",
    "            dataloader_val (`torch.utils.data.dataloader.DataLoader`, `Optional`):\n",
    "                Pytorch dataloader. If `None` no validation will be performed.\n",
    "        Returns:\n",
    "            loss_tr_epochs (list of `float`): training loss for each epoch\n",
    "            loss_val_epochs (list of `float`): validation loss for each epoch\n",
    "        \"\"\"\n",
    "        loss_tr_epochs = []\n",
    "        loss_val_epochs = []\n",
    "        f1_best = .0\n",
    "        lrs = []\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            tr_loss_mean = .0\n",
    "            tr_loss_cum = .0\n",
    "            step = -1\n",
    "\n",
    "            # Training\n",
    "            # -----------------------------\n",
    "            self.model.train()\n",
    "            self.model.zero_grad()\n",
    "            for i, batch in enumerate(dataloader_train):\n",
    "                # Estimate gradients and accumulate them\n",
    "                tr_loss = self._estimate_gradients(batch)\n",
    "                tr_loss_cum += tr_loss\n",
    "\n",
    "                # Update params every acumulated steps\n",
    "                if (i + 1) % self.accumulate_grad_every == 0:\n",
    "                    self._update_network_params()\n",
    "                    if self.scheduler is not None:\n",
    "                        lrs.append(self.scheduler.get_last_lr()[0])\n",
    "                    step += 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if step % self.print_every == 0:\n",
    "                    tr_loss_mean = tr_loss_cum/(i+1)\n",
    "                    print(f\"- Epoch: {epoch}/{self.n_epochs - 1}\",\n",
    "                          f\"- Step: {step:3}/{(len(dataloader_train)// self.accumulate_grad_every) - 1}\",\n",
    "                          f\"- Training Loss: {tr_loss_mean:.6f}\")\n",
    "\n",
    "            loss_tr_epochs.append(tr_loss_mean)\n",
    "            print(f\"- Epoch: {epoch}/{self.n_epochs - 1} - Training Loss: {tr_loss_mean}\")\n",
    "\n",
    "            # Plot training curve\n",
    "            plt.plot(loss_tr_epochs)\n",
    "            plt.xlabel('#Epochs')\n",
    "            plt.ylabel('Error')\n",
    "            plt.legend(['training'])\n",
    "\n",
    "            # Validation\n",
    "            # -----------------------------\n",
    "            if dataloader_val is not None:\n",
    "                val_loss, f1, report_ent, report_toks = self.evaluate(dataloader_val,\n",
    "                                                                      epoch=epoch,\n",
    "                                                                      verbose=True)\n",
    "                loss_val_epochs.append(val_loss)\n",
    "\n",
    "                if f1 > f1_best:\n",
    "                    f1_best = f1\n",
    "                    self._save_model()\n",
    "                    self._write_report_to_file(report_ent, report_toks, epoch,\n",
    "                                               tr_loss_mean, val_loss)\n",
    "\n",
    "                # Plot val curve\n",
    "                plt.plot(loss_val_epochs)\n",
    "                plt.legend(['training', 'validation'])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, 'error_curves.jpg'))\n",
    "            plt.close()\n",
    "\n",
    "            # Plot learning rate curve\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('#Batches')\n",
    "            plt.ylabel('Learning rate')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, 'learning_rate.jpg'))\n",
    "            plt.close()\n",
    "        return loss_tr_epochs, loss_val_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data\n",
    "DATA_TR_PATH = '/Users/sdeshpande/Desktop/bioinformatices/BIO_NER_data/data/BC5CDR-chem-IOB/train.tsv'\n",
    "DATA_VAL_PATH = '/Users/sdeshpande/Desktop/bioinformatices/BIO_NER_data/data/BC5CDR-chem-IOB/devel.tsv'\n",
    "DATA_TEST_PATH = None\n",
    "SEED = 42\n",
    "\n",
    "# MODEL\n",
    "MODEL_NAME = 'allenai/scibert_scivocab_uncased'\n",
    "MAX_LEN_SEQ = 128\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# Optimization parameters\n",
    "N_EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "BATCH_SIZE_VAL = 28\n",
    "WEIGHT_DECAY = 0\n",
    "LEARNING_RATE = 1e-4  # 2e-4\n",
    "RATIO_WARMUP_STEPS = .1\n",
    "DROPOUT = .3\n",
    "ACUMULATE_GRAD_EVERY = 4\n",
    "OPTIMIZER = Adam\n",
    "\n",
    "# Seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bf0a0477a446c7a90c7385aa4b28e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2adfed3dc6b4a448b3fd5cd9e5c9cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=227845.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b291c67ff59042caa62098e5f61a0ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442221694.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Epoch: 0/1 - Step:   0/141 - Training Loss: 0.962044\n",
      "- Epoch: 0/1 - Step:  10/141 - Training Loss: 0.547755\n",
      "- Epoch: 0/1 - Step:  20/141 - Training Loss: 0.414147\n",
      "- Epoch: 0/1 - Step:  30/141 - Training Loss: 0.321616\n",
      "- Epoch: 0/1 - Step:  40/141 - Training Loss: 0.261613\n",
      "- Epoch: 0/1 - Step:  50/141 - Training Loss: 0.221518\n",
      "- Epoch: 0/1 - Step:  60/141 - Training Loss: 0.193794\n",
      "- Epoch: 0/1 - Step:  70/141 - Training Loss: 0.173550\n",
      "- Epoch: 0/1 - Step:  80/141 - Training Loss: 0.158017\n",
      "- Epoch: 0/1 - Step:  90/141 - Training Loss: 0.145708\n",
      "- Epoch: 0/1 - Step: 100/141 - Training Loss: 0.134616\n",
      "- Epoch: 0/1 - Step: 110/141 - Training Loss: 0.125471\n",
      "- Epoch: 0/1 - Step: 120/141 - Training Loss: 0.118035\n",
      "- Epoch: 0/1 - Step: 130/141 - Training Loss: 0.111314\n",
      "- Epoch: 0/1 - Step: 140/141 - Training Loss: 0.105312\n",
      "- Epoch: 0/1 - Training Loss: 0.10531191656395714\n",
      "- Epoch: 0/1 - Validation Loss: 0.03321884393801333\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      " Chemical     0.8944    0.9558    0.9241      5337\n",
      "\n",
      "micro avg     0.8944    0.9558    0.9241      5337\n",
      "macro avg     0.8944    0.9558    0.9241      5337\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  B-Chemical     0.9150    0.9657    0.9397      5337\n",
      "  I-Chemical     0.8193    0.9007    0.8581      1722\n",
      "           O     0.9973    0.9930    0.9952    110004\n",
      "\n",
      "    accuracy                         0.9904    117063\n",
      "   macro avg     0.9105    0.9532    0.9310    117063\n",
      "weighted avg     0.9909    0.9904    0.9906    117063\n",
      "\n",
      "- Epoch: 1/1 - Step:   0/141 - Training Loss: 0.017240\n",
      "- Epoch: 1/1 - Step:  10/141 - Training Loss: 0.026981\n",
      "- Epoch: 1/1 - Step:  20/141 - Training Loss: 0.022255\n",
      "- Epoch: 1/1 - Step:  30/141 - Training Loss: 0.020749\n",
      "- Epoch: 1/1 - Step:  40/141 - Training Loss: 0.021669\n",
      "- Epoch: 1/1 - Step:  50/141 - Training Loss: 0.022852\n",
      "- Epoch: 1/1 - Step:  60/141 - Training Loss: 0.021474\n",
      "- Epoch: 1/1 - Step:  70/141 - Training Loss: 0.022802\n",
      "- Epoch: 1/1 - Step:  80/141 - Training Loss: 0.022234\n",
      "- Epoch: 1/1 - Step:  90/141 - Training Loss: 0.021671\n",
      "- Epoch: 1/1 - Step: 100/141 - Training Loss: 0.021532\n",
      "- Epoch: 1/1 - Step: 110/141 - Training Loss: 0.021375\n",
      "- Epoch: 1/1 - Step: 120/141 - Training Loss: 0.021076\n",
      "- Epoch: 1/1 - Step: 130/141 - Training Loss: 0.021224\n",
      "- Epoch: 1/1 - Step: 140/141 - Training Loss: 0.021337\n",
      "- Epoch: 1/1 - Training Loss: 0.02133749050776811\n",
      "- Epoch: 1/1 - Validation Loss: 0.028872023003669908\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      " Chemical     0.9131    0.9509    0.9316      5337\n",
      "\n",
      "micro avg     0.9131    0.9509    0.9316      5337\n",
      "macro avg     0.9131    0.9509    0.9316      5337\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  B-Chemical     0.9319    0.9582    0.9448      5337\n",
      "  I-Chemical     0.8343    0.8833    0.8581      1722\n",
      "           O     0.9966    0.9943    0.9955    110004\n",
      "\n",
      "    accuracy                         0.9910    117063\n",
      "   macro avg     0.9209    0.9453    0.9328    117063\n",
      "weighted avg     0.9913    0.9910    0.9911    117063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "training_set = read_data_from_file(DATA_TR_PATH)\n",
    "val_set = read_data_from_file(DATA_VAL_PATH)\n",
    "\n",
    "# Automatically extract labels and their indexes from data.\n",
    "labels2ind, labels_count = get_labels(training_set + val_set)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create loaders for datasets\n",
    "training_set = NerDataset(dataset=training_set,\n",
    "                          tokenizer=tokenizer,\n",
    "                          labels2ind=labels2ind,\n",
    "                          max_len_seq=MAX_LEN_SEQ)\n",
    "\n",
    "val_set = NerDataset(dataset=val_set,\n",
    "                     tokenizer=tokenizer,\n",
    "                     labels2ind=labels2ind,\n",
    "                     max_len_seq=MAX_LEN_SEQ)\n",
    "\n",
    "dataloader_tr = DataLoader(dataset=training_set,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=True)\n",
    "\n",
    "dataloader_val = DataLoader(dataset=val_set,\n",
    "                            batch_size=BATCH_SIZE_VAL,\n",
    "                            shuffle=False)\n",
    "\n",
    "# Load model\n",
    "nerbert = BertForTokenClassification.from_pretrained(MODEL_NAME,\n",
    "                                                     hidden_dropout_prob=DROPOUT,\n",
    "                                                     attention_probs_dropout_prob=DROPOUT,\n",
    "                                                     num_labels=len(labels2ind),\n",
    "                                                     id2label={str(v): k for k, v in labels2ind.items()})\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "optimizer = get_optimizer_with_weight_decay(model=nerbert,\n",
    "                                            optimizer=OPTIMIZER,\n",
    "                                            learning_rate=LEARNING_RATE,\n",
    "                                            weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "training_steps = (len(dataloader_tr)//ACUMULATE_GRAD_EVERY) * N_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n",
    "                                            num_warmup_steps=training_steps * RATIO_WARMUP_STEPS,\n",
    "                                            num_training_steps=training_steps)\n",
    "\n",
    "# Trainer\n",
    "trainer = BertTrainer(model=nerbert,\n",
    "                      tokenizer=tokenizer,\n",
    "                      optimizer=optimizer,\n",
    "                      scheduler=scheduler,\n",
    "                      labels2ind=labels2ind,\n",
    "                      device=DEVICE,\n",
    "                      n_epochs=N_EPOCHS,\n",
    "                      accumulate_grad_every=ACUMULATE_GRAD_EVERY,\n",
    "                      output_dir='./trained_models')\n",
    "\n",
    "# Train and validate model\n",
    "trainer.train(dataloader_train=dataloader_tr,\n",
    "              dataloader_val=dataloader_val)\n",
    "\n",
    "# Test the model on test set if any\n",
    "if DATA_TEST_PATH is not None:\n",
    "    print(f\"{'*'*40}\\n\\t\\tEVALUATION ON TEST SET\\n{'*'*40}\")\n",
    "    test_set = read_data_from_file(DATA_TEST_PATH)\n",
    "\n",
    "    test_set = NerDataset(dataset=test_set,\n",
    "                          tokenizer=tokenizer,\n",
    "                          labels2ind=labels2ind,\n",
    "                          max_len_seq=MAX_LEN_SEQ)\n",
    "\n",
    "    dataloader_test = DataLoader(dataset=test_set,\n",
    "                                 batch_size=BATCH_SIZE_VAL)\n",
    "\n",
    "    trainer.evaluate(dataloader_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = pipeline(\"ner\", model='./trained_models', tokenizer='./trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Triheptanoin (Dojolvi™), a synthetic medium-chain triglyceride, is being developed by Ultragenyx Pharmaceutical as a pharmaceutical-grade anaplerotic compound for use in the treatment of inherited metabolic disorders.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'tri',\n",
       "  'score': 0.9985841512680054,\n",
       "  'entity': 'B-Chemical',\n",
       "  'index': 1},\n",
       " {'word': '##hep',\n",
       "  'score': 0.989841878414154,\n",
       "  'entity': 'I-Chemical',\n",
       "  'index': 2},\n",
       " {'word': '##tan',\n",
       "  'score': 0.9938075542449951,\n",
       "  'entity': 'I-Chemical',\n",
       "  'index': 3},\n",
       " {'word': '##oin',\n",
       "  'score': 0.9875107407569885,\n",
       "  'entity': 'I-Chemical',\n",
       "  'index': 4},\n",
       " {'word': '[UNK]',\n",
       "  'score': 0.9598410725593567,\n",
       "  'entity': 'B-Chemical',\n",
       "  'index': 6}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BC5CDR tags :-\n",
    "    'B-Chemical', \n",
    "    'O', \n",
    "    'B-Disease', \n",
    "    'I-Disease', \n",
    "    'I-Chemical'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BioNLP13CG tags :-\n",
    "'B-Anatomical_system',\n",
    "'B-Cancer',\n",
    "'B-Cell', \n",
    "'B-Cellular_component',\n",
    "'B-Developing_anatomical_structure',\n",
    "'B-Gene_or_gene_product', \n",
    "'B-Immaterial_anatomical_entity',\n",
    "'B-Multi-tissue_structure',\n",
    "'B-Organ',\n",
    "'B-Organism', \n",
    "'B-Organism_subdivision',\n",
    "'B-Organism_substance',\n",
    "'B-Pathological_formation', \n",
    "'B-Simple_chemical',\n",
    "'B-Tissue',\n",
    "'I-Amino_acid',\n",
    "'I-Anatomical_system',\n",
    "'I-Cancer', \n",
    "'I-Cell',\n",
    "'I-Cellular_component',\n",
    "'I-Developing_anatomical_structure',\n",
    "'I-Gene_or_gene_product', \n",
    "'I-Immaterial_anatomical_entity',\n",
    "'I-Multi-tissue_structure',\n",
    "'I-Organ',\n",
    "'I-Organism', \n",
    "'I-Organism_subdivision',\n",
    "'I-Organism_substance',\n",
    "'I-Pathological_formation',\n",
    "'I-Simple_chemical', \n",
    "'I-Tissue',\n",
    "'O'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
