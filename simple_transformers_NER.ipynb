{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tokenizers==0.9.4 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (0.9.4)\n"
     ]
    }
   ],
   "source": [
    "#!pip install tokenizers==0.9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --ignore-installed six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.ner import NERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train_df  and eval_df for demonstration\n",
    "train_data = [\n",
    "    [0, \"Simple\", \"B-MISC\"],\n",
    "    [0, \"Transformers\", \"I-MISC\"],\n",
    "    [0, \"started\", \"O\"],\n",
    "    [0, \"with\", \"O\"],\n",
    "    [0, \"text\", \"O\"],\n",
    "    [0, \"classification\", \"B-MISC\"],\n",
    "    [1, \"Simple\", \"B-MISC\"],\n",
    "    [1, \"Transformers\", \"I-MISC\"],\n",
    "    [1, \"can\", \"O\"],\n",
    "    [1, \"now\", \"O\"],\n",
    "    [1, \"perform\", \"O\"],\n",
    "    [1, \"NER\", \"B-MISC\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data, columns=[\"sentence_id\", \"words\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = [\n",
    "    [0, \"Simple\", \"B-MISC\"],\n",
    "    [0, \"Transformers\", \"I-MISC\"],\n",
    "    [0, \"was\", \"O\"],\n",
    "    [0, \"built\", \"O\"],\n",
    "    [0, \"for\", \"O\"],\n",
    "    [0, \"text\", \"O\"],\n",
    "    [0, \"classification\", \"B-MISC\"],\n",
    "    [1, \"Simple\", \"B-MISC\"],\n",
    "    [1, \"Transformers\", \"I-MISC\"],\n",
    "    [1, \"then\", \"O\"],\n",
    "    [1, \"expanded\", \"O\"],\n",
    "    [1, \"to\", \"O\"],\n",
    "    [1, \"perform\", \"O\"],\n",
    "    [1, \"NER\", \"B-MISC\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(eval_data, columns=[\"sentence_id\", \"words\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 436M/436M [00:32<00:00, 13.3MB/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 558kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a NERModel\n",
    "model = NERModel(\"bert\", \"bert-base-cased\", use_cuda=False, args={\"overwrite_output_dir\": True, \"reprocess_input_data\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.04s/it]\n",
      "Epoch 1 of 1:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Running Epoch 0 of 1:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 0/1. Running Loss:    2.2104:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 0/1. Running Loss:    2.2104: 100%|██████████| 1/1 [00:02<00:00,  2.34s/it]\n",
      "Epoch 1 of 1: 100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 2.210407018661499)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.03s/it]\n",
      "Running Evaluation: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.89s/it]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s][[{'Some': 'I-PER'}, {'arbitary': 'B-LOC'}, {'sentence': 'I-PER'}], [{'Simple': 'I-PER'}, {'Transformers': 'I-MISC'}, {'sentence': 'I-MISC'}]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on arbitary text strings\n",
    "sentences = [\"Some arbitary sentence\", \"Simple Transformers sentence\"]\n",
    "predictions, raw_outputs = model.predict(sentences)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n___________________________\nSentence:  Some arbitary sentence\nSome I-PER 0.16811945 [0.07897804, 0.13034531, 0.12625937, 0.11895697, 0.16811945, 0.08026024, 0.08786559, 0.09299413, 0.116220914]\narbitary B-LOC 0.12736197 [0.084613964, 0.12178145, 0.12736197, 0.113595344, 0.12363647, 0.1101747, 0.0992397, 0.11738086, 0.10221564]\nsentence I-PER 0.14749877 [0.0953362, 0.12476595, 0.14368318, 0.11944499, 0.14749877, 0.086047135, 0.089323476, 0.10089625, 0.09300407]\n\n___________________________\nSentence:  Simple Transformers sentence\nSimple I-PER 0.15926047 [0.07373084, 0.115635455, 0.15782577, 0.10060794, 0.15926047, 0.0822481, 0.084375665, 0.09930078, 0.12701479]\nTransformers I-MISC 0.14975621 [0.07707294, 0.119259, 0.14975621, 0.113746494, 0.1296764, 0.08921113, 0.09652433, 0.10395571, 0.12079785]\nsentence I-MISC 0.14957005 [0.08473355, 0.11872178, 0.14957005, 0.121434666, 0.13516234, 0.09107949, 0.102629684, 0.09492215, 0.1017462]\n"
     ]
    }
   ],
   "source": [
    "# More detailed preditctions\n",
    "for n, (preds, outs) in enumerate(zip(predictions, raw_outputs)):\n",
    "    print(\"\\n___________________________\")\n",
    "    print(\"Sentence: \", sentences[n])\n",
    "    for pred, out in zip(preds, outs):\n",
    "        key = list(pred.keys())[0]\n",
    "        new_out = out[key]\n",
    "        preds = list(softmax(np.mean(new_out, axis=0)))\n",
    "        print(key, pred[key], preds[np.argmax(preds)], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}