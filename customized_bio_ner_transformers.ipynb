{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validate model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data -- for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "\n",
    "class BertForTokenClassificationCustom(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 num_labels: int,\n",
    "                 hidden_dropout_prob: float,\n",
    "                 attention_probs_dropout_prob: float):\n",
    "        \"\"\"\n",
    "        This model is a replica of BertForTokenClassification class but instead of being\n",
    "        a subclass of `PreTrainedModel` (transformers library) it is a subclass of `nn.Module`\n",
    "        from Pytorch. In fact, `BertForTokenClassification` is instantiated through `from_pretrained`\n",
    "        within the `init` method.\n",
    "        The only difference in functionallity is within the `forward` method.\n",
    "        Here the output of BERT model is reshaped to be directly compatible with\n",
    "        `nn.module.CrossEntropyLoss` (batch_size, num_classes, sequence_len). This is done in\n",
    "        order to make compatible this BERT model with the `torch_lr_finder` library.\n",
    "        Args:\n",
    "            model_name (`str`): model name as expected in Transformers library\n",
    "            num_labels (`int`): number of labels\n",
    "            hidden_dropout_prob(`float`): droput\n",
    "            attention_probs_dropout_prob (`float`): dropout in attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained(model_name,\n",
    "                                                               num_labels=num_labels,\n",
    "                                                               hidden_dropout_prob=hidden_dropout_prob,\n",
    "                                                               attention_probs_dropout_prob=attention_probs_dropout_prob)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        pretrained_model_name_or_path: str,\n",
    "                        num_labels: int = 2,\n",
    "                        hidden_dropout_prob: float = 0.,\n",
    "                        attention_probs_dropout_prob: float = 0.):\n",
    "\n",
    "        return BertForTokenClassificationCustom(model_name=pretrained_model_name_or_path,\n",
    "                                                num_labels=num_labels,\n",
    "                                                hidden_dropout_prob=hidden_dropout_prob,\n",
    "                                                attention_probs_dropout_prob=attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, input_bert):\n",
    "        outputs = self.bert(*input_bert)\n",
    "\n",
    "        # Prepare output to be compatible with nn.module.CrossEntropyLoss()\n",
    "        bert_shape = outputs[0].shape\n",
    "        return outputs[0].view(bert_shape[0], bert_shape[-1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "import torch\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def get_optimizer_with_weight_decay(model: PreTrainedModel,\n",
    "                                    optimizer: torch.optim.Optimizer,\n",
    "                                    learning_rate: Union[float, int],\n",
    "                                    weight_decay: Union[float, int]) -> torch.optim.Optimizer:\n",
    "    \"\"\"\n",
    "    Apply weight decay to all the network parameters but those called `bias` or  `LayerNorm.weight`.\n",
    "    Args:\n",
    "        model (`PreTrainedModel`): model to apply weight decay.\n",
    "        optimizer (`torch.optim.Optimizer`): The optimizer to use during training.\n",
    "        learning_rate (`float` or `int`): value of the learning rate to use during training.\n",
    "        weight_decay (`float` or `int`): value of the weight decay to apply.\n",
    "    Returns:\n",
    "        optimizer (`torch.optim.Optimizer`): the optimizer instantiated with the selected\n",
    "        learning rate and the parameters with and without weight decay.\n",
    "    \"\"\"\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    params = [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)]\n",
    "    params_nd = [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)]\n",
    "    optimizer_grouped_parameters = [{\"params\": params, \"weight_decay\": weight_decay},\n",
    "                                    {\"params\": params_nd, \"weight_decay\": 0.0}]\n",
    "\n",
    "    return optimizer(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from typing import List, Tuple, Union, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from seqeval.metrics import f1_score\n",
    "from sklearn.metrics import classification_report as sklearn_report\n",
    "from torch import nn\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class BertTrainer:\n",
    "    def __init__(self,\n",
    "                 model: PreTrainedModel,\n",
    "                 tokenizer: PreTrainedTokenizer,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 n_epochs: int,\n",
    "                 labels2ind: Dict[str, int],\n",
    "                 scheduler: Optional[torch.optim.lr_scheduler.LambdaLR] = None,\n",
    "                 device: str = 'cpu',\n",
    "                 clipping: Optional[Union[int, float]] = None,\n",
    "                 accumulate_grad_every: int = 1,\n",
    "                 print_every: int = 10,\n",
    "                 print_val_mistakes: bool = False,\n",
    "                 output_dir: str = './'):\n",
    "\n",
    "        \"\"\"\n",
    "        Complete training and evaluation loop in Pytorch specially designed for\n",
    "        BERT-based models from transformers library. It allows to save the model\n",
    "        from the epoch with the best F1-score and the tokenizer. The class\n",
    "        optionally generates reports and figures with the obtained results that\n",
    "        are automatically stored in disk.\n",
    "        Args:\n",
    "            model (`PreTrainedModel`): Pre-trained model from transformers library.\n",
    "                For NER, usually loaded as `BertForTokenClassification.from_pretrained(...)`\n",
    "            tokenizer (`PreTrainedTokenizer`): Pre-trained tokenizer from transformers library.\n",
    "                Usually loaded as `AutoTokenizer.from_pretrained(...)`\n",
    "            optimizer (`torch.optim.Optimizer`): Pytorch Optimizer\n",
    "            n_epochs (`int`): Number of epochs to train.\n",
    "            labels2ind (`dict`): maps `str` class labels into `int` indexes.\n",
    "            scheduler (`torch.optim.lr_scheduler.LambdaLR`, `Optional`): Pytorch scheduler. It sets a\n",
    "                different learning rate for each training step to update the network weights.\n",
    "            device (`str`): Type of device where to train the network. It must be `cpu` or `cuda`.\n",
    "            clipping (`int` or `float`, `Optional`): max norm to apply to the gradients. If None,\n",
    "                no graddient clipping is applied.\n",
    "            accumulate_grad_every (`int`): How often you want to accumulate the gradient. This is useful\n",
    "                when there are limitations in the batch size due to memory issues. Let's say that in your\n",
    "                GPU only fits a model with batch size of 8 and you want to try a batch size of 32. Then,\n",
    "                you should set this parameter to 4 (8*4=32). Internally, a loop will be ran 4 times\n",
    "                accumulating the gradient for each step. Later, the network parameters will be updated.\n",
    "                So at the end, this is equivalent to train your network with a batch size of 32. The batch\n",
    "                size is inferred from `dataloader_train` argument.\n",
    "            print_every (`int`): How often you want to print loss. Measured in batches where a batch is\n",
    "                considered batch_size * accumulate_grad_every.\n",
    "            print_val_mistakes (`bool`): whether to print validation examples (sentences) where the model\n",
    "                commits at least one mistake. It is printed after each epoch. The printed info is the word\n",
    "                within each sentence, its predicted label and the real label. This is very useful to\n",
    "                inspect the behaviour of your model.\n",
    "            output_dir (`str`): Directory where file reports and images are saved.\n",
    "        Methods:\n",
    "            train(dataloader_train: DataLoader, dataloader_val: Optional[DataLoader] = None)\n",
    "                Complete training and evaluation (optional) loop in Pytorch.\n",
    "            evaluate(dataloader_val: DataLoader, epoch: int = 0, verbose: bool = False)\n",
    "                Evaluation on test data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.n_epochs = n_epochs\n",
    "        self.labels2ind = labels2ind\n",
    "        self.inds2labels = {v: k for k, v in self.labels2ind.items()}\n",
    "        self.device = device\n",
    "        self.clipping = clipping\n",
    "        self.accumulate_grad_every = accumulate_grad_every\n",
    "        self.print_every = print_every\n",
    "        self.print_val_mistakes = print_val_mistakes\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def _reformat_predictions(self,\n",
    "                              y_true: List[List[int]],\n",
    "                              y_pred: List[List[int]],\n",
    "                              input_ids: List[List[str]]\n",
    "                              ) -> Tuple[List[List[str]],\n",
    "                                         List[List[str]],\n",
    "                                         List[List[str]]]:\n",
    "        \"\"\"\n",
    "        Takes batch of tokens, labels (class indexes) and predictions (class indexes)\n",
    "        and get rid of unwanted tokens, that is, those that have as label the index\n",
    "        to ignore (i.e. padding tokens).  It also converts the label and prediction\n",
    "        indexes into their corresponding class name.\n",
    "        Args:\n",
    "            y_true (list of lists `int`): indexes of the real labels\n",
    "            y_pred (list of lists `int`): indexes of the predicted classes\n",
    "            input_ids (list of lists `str`) : tokens\n",
    "        Returns:\n",
    "            Tuple that contains the transformed input arguments\n",
    "        \"\"\"\n",
    "        # Map indexes to labels and remove ignored indexes\n",
    "        true_list = [[] for _ in range(len(y_true))]\n",
    "        pred_list = [[] for _ in range(len(y_pred))]\n",
    "        input_list = [[] for _ in range(len(input_ids))]\n",
    "\n",
    "        for i in range(len(y_true)):\n",
    "            for j in range(len(y_true[0])):\n",
    "                if y_true[i][j] != CrossEntropyLoss().ignore_index:\n",
    "                    true_list[i].append(self.inds2labels[y_true[i][j]])\n",
    "                    pred_list[i].append(self.inds2labels[y_pred[i][j]])\n",
    "                    input_list[i].append(input_ids[i][j])\n",
    "\n",
    "        return true_list, pred_list, input_list\n",
    "\n",
    "    def _print_missclassified_val_examples(self,\n",
    "                                           y_true: List[List[str]],\n",
    "                                           y_pred: List[List[str]],\n",
    "                                           input_ids: List[List[str]]):\n",
    "        \"\"\"\n",
    "        print validation examples (sentences) where the model commits at least\n",
    "        one mistake. It is printed after each epoch. This is very useful to\n",
    "        inspect the behaviour of your model.\n",
    "        Args:\n",
    "            y_true (list of lists `str`): real labels\n",
    "            y_pred (list of lists `str`): predicted classes\n",
    "            input_ids (list of lists `str`) : tokens\n",
    "        Examples::\n",
    "                TOKEN          LABEL          PRED\n",
    "                immunostaining O              O\n",
    "                showed         O              O\n",
    "                the            O              O\n",
    "                estrogen       B-cell_type    B-cell_type\n",
    "                receptor       I-cell_type    I-cell_type\n",
    "                cells          I-cell_type    O\n",
    "                                  ·\n",
    "                                  ·\n",
    "                                  ·\n",
    "                synovial       O              O\n",
    "                tissues        O              O\n",
    "                .              O              O\n",
    "        \"\"\"\n",
    "        # Print some examples (where the model fails)\n",
    "        for i in range(len(input_ids)):\n",
    "            if y_true[i] != y_pred[i]:\n",
    "                tokens = self.tokenizer.convert_ids_to_tokens(input_ids[i])\n",
    "                max_len_token = max([len(t) for t in tokens] +\n",
    "                                    [len(la) for la in self.labels2ind.keys()])\n",
    "\n",
    "                print(f\"\\n{'TOKEN':<{max_len_token}}\",\n",
    "                      f\"{'LABEL':<{max_len_token}}\",\n",
    "                      f\"{'PRED':<{max_len_token}}\")\n",
    "\n",
    "                for token, label_true, label_pred in zip(tokens, y_true[i], y_pred[i]):\n",
    "                    print(f\"{token:<{max_len_token}}\",\n",
    "                          f\"{label_true:<{max_len_token}}\",\n",
    "                          f\"{label_pred:<{max_len_token}}\")\n",
    "\n",
    "    def _write_report_to_file(self,\n",
    "                              report_entities: str,\n",
    "                              report_tokens: str,\n",
    "                              epoch: int,\n",
    "                              tr_loss: float,\n",
    "                              val_loss: float):\n",
    "        \"\"\"\n",
    "        Writes and saves the following info into a file called `classification_report.txt`\n",
    "        within the directory `output_dir` for the model from the best epoch:\n",
    "            - Classification report at span/entity level (for validation dataset).\n",
    "            - Classification report at word level (for validation dataset).\n",
    "            - Epoch where the best model was found (best F1-score in validation dataset)\n",
    "            - Training loss from the best epoch.\n",
    "            - Validation loss from the best epoch.\n",
    "        Args:\n",
    "            report_entities (`str`): classification report at entity/span level.\n",
    "            report_tokens (`str`): classification report at word level.\n",
    "            epoch (`int`): epoch\n",
    "            tr_loss (`float`): training loss\n",
    "            val_loss (`float`): validation loss\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.output_dir, 'classification_report.txt'), 'w') as f:\n",
    "            f.write(report_entities)\n",
    "            f.write(f'\\n{report_tokens}')\n",
    "            f.write(f\"\\nEpoch: {epoch} \"\n",
    "                    f\"\\n- Training Loss: {tr_loss}\"\n",
    "                    f\"\\n- Validation Loss: {val_loss}\")\n",
    "\n",
    "    def _save_model(self):\n",
    "        if not isinstance(self.model, PreTrainedModel):\n",
    "            raise ValueError(\"Trainer.model appears to not be a PreTrainedModel\")\n",
    "        self.model.save_pretrained(self.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "\n",
    "    def _estimate_gradients(self, batch: Dict[str, torch.Tensor]) -> float:\n",
    "        # Send tensors to device\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "        # estimate loss and gradient\n",
    "        loss, _ = self.model(**batch)\n",
    "        loss.backward()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def _update_network_params(self):\n",
    "        # Graddient clipping\n",
    "        if self.clipping is not None:\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.clipping)\n",
    "\n",
    "        # Udate parameters (accumulated gradient based on accumulated grad)\n",
    "        self.optimizer.step()\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "    def _validation_step(self,\n",
    "                         batch: Dict[str, torch.Tensor]\n",
    "                         ) -> Tuple[float, np.ndarray]:\n",
    "        # Send tensors to device\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "        # Predict and estimate error\n",
    "        with torch.no_grad():\n",
    "            loss, pred = self.model(**batch)\n",
    "\n",
    "        return loss.item(), pred.detach().cpu().numpy()\n",
    "\n",
    "    def evaluate(self,\n",
    "                 dataloader_val: DataLoader,\n",
    "                 epoch: int = 0,\n",
    "                 verbose: bool = False) -> Tuple[float, float, str, str]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataloader_val:\n",
    "            epoch:\n",
    "            verbose:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        n_steps_val = len(dataloader_val)\n",
    "        self.model.eval()\n",
    "\n",
    "        val_loss_cum = .0\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        input_ids = []\n",
    "        for step, batch in enumerate(dataloader_val):\n",
    "            val_loss, pred = self._validation_step(batch)\n",
    "            val_loss_cum += val_loss\n",
    "            y_true.extend(batch['labels'].tolist())\n",
    "            y_pred.extend(pred.argmax(axis=-1).tolist())\n",
    "            input_ids.extend(batch['input_ids'].tolist())\n",
    "\n",
    "        y_true, y_pred, input_ids = self._reformat_predictions(y_true, y_pred, input_ids)\n",
    "\n",
    "        # Performance Reports and loss\n",
    "        report_entities = seqeval_report(y_true=y_true, y_pred=y_pred, digits=4)\n",
    "        report_tokens = sklearn_report(y_true=list(itertools.chain(*y_true)),\n",
    "                                       y_pred=list(itertools.chain(*y_pred)), digits=4)\n",
    "\n",
    "        loss_val_epoch = val_loss_cum / n_steps_val\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"- Epoch: {epoch}/{self.n_epochs - 1} - Validation Loss: {loss_val_epoch}\")\n",
    "            print(report_entities)\n",
    "            print(report_tokens)\n",
    "\n",
    "        # Print some examples (where the model fails)\n",
    "        if self.print_val_mistakes and verbose:\n",
    "            self._print_missclassified_val_examples(y_true, y_pred, input_ids)\n",
    "\n",
    "        # Save model and write report to txt file\n",
    "        f1 = f1_score(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "        return loss_val_epoch, f1, report_entities, report_tokens\n",
    "\n",
    "    def train(self,\n",
    "              dataloader_train: DataLoader,\n",
    "              dataloader_val: Optional[DataLoader] = None\n",
    "              ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Complete training and evaluation (optional) loop in Pytorch.\n",
    "        Args:\n",
    "            dataloader_train (`torch.utils.data.dataloader.DataLoader`): Pytorch dataloader.\n",
    "            dataloader_val (`torch.utils.data.dataloader.DataLoader`, `Optional`):\n",
    "                Pytorch dataloader. If `None` no validation will be performed.\n",
    "        Returns:\n",
    "            loss_tr_epochs (list of `float`): training loss for each epoch\n",
    "            loss_val_epochs (list of `float`): validation loss for each epoch\n",
    "        \"\"\"\n",
    "        loss_tr_epochs = []\n",
    "        loss_val_epochs = []\n",
    "        f1_best = .0\n",
    "        lrs = []\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            tr_loss_mean = .0\n",
    "            tr_loss_cum = .0\n",
    "            step = -1\n",
    "\n",
    "            # Training\n",
    "            # -----------------------------\n",
    "            self.model.train()\n",
    "            self.model.zero_grad()\n",
    "            for i, batch in enumerate(dataloader_train):\n",
    "                # Estimate gradients and accumulate them\n",
    "                tr_loss = self._estimate_gradients(batch)\n",
    "                tr_loss_cum += tr_loss\n",
    "\n",
    "                # Update params every acumulated steps\n",
    "                if (i + 1) % self.accumulate_grad_every == 0:\n",
    "                    self._update_network_params()\n",
    "                    if self.scheduler is not None:\n",
    "                        lrs.append(self.scheduler.get_last_lr()[0])\n",
    "                    step += 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if step % self.print_every == 0:\n",
    "                    tr_loss_mean = tr_loss_cum/(i+1)\n",
    "                    print(f\"- Epoch: {epoch}/{self.n_epochs - 1}\",\n",
    "                          f\"- Step: {step:3}/{(len(dataloader_train)// self.accumulate_grad_every) - 1}\",\n",
    "                          f\"- Training Loss: {tr_loss_mean:.6f}\")\n",
    "\n",
    "            loss_tr_epochs.append(tr_loss_mean)\n",
    "            print(f\"- Epoch: {epoch}/{self.n_epochs - 1} - Training Loss: {tr_loss_mean}\")\n",
    "\n",
    "            # Plot training curve\n",
    "            plt.plot(loss_tr_epochs)\n",
    "            plt.xlabel('#Epochs')\n",
    "            plt.ylabel('Error')\n",
    "            plt.legend(['training'])\n",
    "\n",
    "            # Validation\n",
    "            # -----------------------------\n",
    "            if dataloader_val is not None:\n",
    "                val_loss, f1, report_ent, report_toks = self.evaluate(dataloader_val,\n",
    "                                                                      epoch=epoch,\n",
    "                                                                      verbose=True)\n",
    "                loss_val_epochs.append(val_loss)\n",
    "\n",
    "                if f1 > f1_best:\n",
    "                    f1_best = f1\n",
    "                    self._save_model()\n",
    "                    self._write_report_to_file(report_ent, report_toks, epoch,\n",
    "                                               tr_loss_mean, val_loss)\n",
    "\n",
    "                # Plot val curve\n",
    "                plt.plot(loss_val_epochs)\n",
    "                plt.legend(['training', 'validation'])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, 'error_curves.jpg'))\n",
    "            plt.close()\n",
    "\n",
    "            # Plot learning rate curve\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('#Batches')\n",
    "            plt.ylabel('Learning rate')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, 'learning_rate.jpg'))\n",
    "            plt.close()\n",
    "        return loss_tr_epochs, loss_val_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataSample:\n",
    "    \"\"\"\n",
    "    A single training/test example (sentence) for token classification.\n",
    "    \"\"\"\n",
    "    words: List[str]\n",
    "    labels: List[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputBert:\n",
    "    \"\"\"\n",
    "    A single set of features of data.\n",
    "    Property names are the same names as the corresponding inputs to a BERT model.\n",
    "    \"\"\"\n",
    "    input_ids: torch.tensor\n",
    "    attention_mask: torch.tensor\n",
    "    token_type_ids: torch.tensor\n",
    "    labels: Optional[torch.tensor] = None\n",
    "\n",
    "\n",
    "class NerDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset: List[DataSample],\n",
    "                 tokenizer: PreTrainedTokenizer,\n",
    "                 labels2ind: Dict[str, int],\n",
    "                 max_len_seq: int = 512,\n",
    "                 bert_hugging: bool = True):\n",
    "        \"\"\"\n",
    "        Class that builds a torch Dataset specially designed for NER data.\n",
    "        Args:\n",
    "            dataset (list of `DataSample` instances): Each data sample is a dataclass\n",
    "                that contains two fields: `words` and `labels`. Both are lists of `str`.\n",
    "            tokenizer (`PreTrainedTokenizer`): Pre-trained tokenizer from transformers\n",
    "                library. Usually loaded as `AutoTokenizer.from_pretrained(...)`.\n",
    "            labels2ind (`dict`): maps `str` class labels into `int` indexes.\n",
    "            max_len_seq (`int`): Max length sequence for each example (sentence).\n",
    "            bert_hugging (`bool`):\n",
    "        \"\"\"\n",
    "        super(NerDataset).__init__()\n",
    "        self.bert_hugging = bert_hugging\n",
    "        self.max_len_seq = max_len_seq\n",
    "        self.label2ind = labels2ind\n",
    "        self.features = data2tensors(data=dataset,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     label2idx=self.label2ind,\n",
    "                                     max_seq_len=max_len_seq,\n",
    "                                     pad_token_label_id=nn.CrossEntropyLoss().ignore_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> Union[Dict[str, torch.tensor],\n",
    "                                      Tuple[List[torch.tensor], torch.tensor]]:\n",
    "        if self.bert_hugging:\n",
    "            return asdict(self.features[i])\n",
    "        else:\n",
    "            inputs = asdict(self.features[i])\n",
    "            labels = inputs.pop('labels')\n",
    "            return list(inputs.values()), labels\n",
    "\n",
    "\n",
    "def get_labels(data: List[DataSample]) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Automatically extract labels types from the data and its count.\n",
    "    Args:\n",
    "        data (list of `DataSample`): Each data sample is a dataclass that contains\n",
    "            two fields: `words` and `labels`. Both are lists of `str`.\n",
    "    Returns:\n",
    "        labels2idx (`dict`): maps `str` class labels into `int` indexes.\n",
    "        labels_count(`dict`): The number of words for each class label that appears in\n",
    "            the dataset. Usufull information if you want to apply class weights on\n",
    "            imbalanced data.\n",
    "    \"\"\"\n",
    "    labels = set()\n",
    "    labels_counts = defaultdict(int)\n",
    "    for sent in data:\n",
    "        labels.update(sent.labels)\n",
    "\n",
    "        for label_ in sent.labels:\n",
    "            labels_counts[label_] += 1\n",
    "\n",
    "    if \"O\" not in labels:\n",
    "        labels.add('O')\n",
    "        labels_counts['0'] = 0\n",
    "\n",
    "    # Convert list of labels ind a mapping labels -> index\n",
    "    labels2idx = {label_: i for i, label_ in enumerate(labels)}\n",
    "    return labels2idx, dict(labels_counts)\n",
    "\n",
    "\n",
    "def get_class_weight_tensor(labels2ind: Dict[str, int],\n",
    "                            labels_count: Dict[str, int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the class weights based on the class labels frequency within the dataset.\n",
    "    Args:\n",
    "        labels2ind (`dict`): maps `str` class labels into `int` indexes.\n",
    "        labels_count (`dict`): The number of words for each class label that appears in\n",
    "            the dataset.\n",
    "    Returns:\n",
    "        torch.Tensor with the class weights. Size (num_classes).\n",
    "    \"\"\"\n",
    "    label2ind_list = [(k, v) for k, v in labels2ind.items()]\n",
    "    label2ind_list.sort(key=lambda x: x[1])\n",
    "    total_labels = sum([count for label, count in labels_count.items()])\n",
    "    class_weights = [total_labels/labels_count[label] for label, _ in label2ind_list]\n",
    "    return torch.tensor(np.array(class_weights)/max(class_weights), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def read_data_from_file(file_path: str, sep: str = '\\t') -> List[DataSample]:\n",
    "    \"\"\"\n",
    "    Load data from a txt file (BIO tagging format) and transform it into the\n",
    "    required format (list of `DataSample` instances).\n",
    "    Args:\n",
    "        file_path (`str`): complete path where the data is located (path + filename).\n",
    "        sep (`str`): Symbol used to separete word from label at each line. Default `\\t`.\n",
    "    Returns:\n",
    "        List of `DataSample` instances containing words and labels.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    words = []\n",
    "    labels = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            splits = line.split(sep)\n",
    "            if len(splits) > 1:\n",
    "                words.append(splits[0])\n",
    "                labels.append(splits[-1].replace('\\n', ''))\n",
    "            else:\n",
    "                examples.append(DataSample(words=words, labels=labels))\n",
    "                words = []\n",
    "                labels = []\n",
    "    return examples\n",
    "\n",
    "\n",
    "def data2tensors(data: List[DataSample],\n",
    "                 tokenizer: PreTrainedTokenizer,\n",
    "                 label2idx: Dict[str, int],\n",
    "                 pad_token_label_id: int = -100,\n",
    "                 max_seq_len: int = 512) -> List[InputBert]:\n",
    "    \"\"\"\n",
    "    Takes data and converts it into tensors to feed the neural network.\n",
    "    Args:\n",
    "        data (`list`): List of `DataSample` instances containing words and labels.\n",
    "        tokenizer (`PreTrainedTokenizer`): Pre-trained tokenizer from transformers\n",
    "            library. Usually loaded as `AutoTokenizer.from_pretrained(...)`.\n",
    "        label2idx (`dict`): maps `str` class labels into `int` indexes.\n",
    "        pad_token_label_id (`int`): index to define the special token [PAD]\n",
    "        max_seq_len (`int`): Max sequence length.\n",
    "    Returns:\n",
    "        List of `InputBert` instances. `InputBert` is a dataclass that contains\n",
    "        `input_ids`, `attention_mask`, `token_type_ids` and `labels` (Optional).\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "    for sentence in data:\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(sentence.words, sentence.labels):\n",
    "            subword_tokens = tokenizer.tokenize(text=word)\n",
    "\n",
    "            # BERT could return an empty list of subtokens\n",
    "            if len(subword_tokens) > 0:\n",
    "                tokens.extend(subword_tokens)\n",
    "\n",
    "                # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "                label_ids.extend([label2idx[label]] + [pad_token_label_id] * (len(subword_tokens) - 1))\n",
    "                # if label.startswith('B'):\n",
    "                #     label_ids.extend([label2idx[label]] + [label2idx[f\"I{label[1:]}\"]] * (len(subword_tokens) - 1))\n",
    "                # else:\n",
    "                #     label_ids.extend([label2idx[label]] + [label2idx[label]] * (len(subword_tokens) - 1))\n",
    "\n",
    "        # Drop part of the sequence longer than max_seq_len (account also for [CLS] and [SEP])\n",
    "        if len(tokens) > max_seq_len - 2:\n",
    "            tokens = tokens[:max_seq_len - 2]\n",
    "            label_ids = label_ids[: max_seq_len - 2]\n",
    "\n",
    "        # Add special tokens  for the list of tokens and its corresponding labels.\n",
    "        # For BERT: cls_token = '[CLS]' and sep_token = '[SEP]'\n",
    "        # For RoBERTa: cls_token = '<s>' and sep_token = '</s>'\n",
    "        tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "        label_ids = [pad_token_label_id] + label_ids + [pad_token_label_id]\n",
    "\n",
    "        # Create an attention mask (used to locate the padding)\n",
    "        padding_len = (max_seq_len - len(tokens))\n",
    "        attention_mask = [1] * len(tokens) + [0] * padding_len\n",
    "\n",
    "        # Add padding\n",
    "        tokens += [tokenizer.pad_token] * padding_len\n",
    "        label_ids += [pad_token_label_id] * padding_len\n",
    "\n",
    "        # Convert tokens to ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Create segment_id. All zeros since we only have one sentence\n",
    "        segment_ids = [0] * max_seq_len\n",
    "\n",
    "        # Assert all the input has the expected length\n",
    "        assert len(input_ids) == max_seq_len\n",
    "        assert len(label_ids) == max_seq_len\n",
    "        assert len(attention_mask) == max_seq_len\n",
    "        assert len(segment_ids) == max_seq_len\n",
    "\n",
    "        # Append input features for each sequence/sentence\n",
    "        features.append((InputBert(input_ids=torch.tensor(input_ids),\n",
    "                                   attention_mask=torch.tensor(attention_mask),\n",
    "                                   token_type_ids=torch.tensor(segment_ids),\n",
    "                                   labels=torch.tensor(label_ids))))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/fran-martinez/bio_ner_bert\n",
    "import random\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (get_linear_schedule_with_warmup,\n",
    "                          BertForTokenClassification,\n",
    "                          AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TR_PATH = '/Users/sdeshpande/Desktop/bioinformatices/MTL-Bioinformatics-2016/data/JNLPBA/original-data/train/Genia4ERtask1.iob2'\n",
    "DATA_VAL_PATH = '/Users/sdeshpande/Desktop/bioinformatices/MTL-Bioinformatics-2016/data/JNLPBA/original-data/test/Genia4EReval1.iob2'\n",
    "DATA_TEST_PATH = None\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "MODEL_NAME = 'allenai/scibert_scivocab_uncased'\n",
    "MAX_LEN_SEQ = 128\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters\n",
    "N_EPOCHS = 6\n",
    "BATCH_SIZE = 8\n",
    "BATCH_SIZE_VAL = 28\n",
    "WEIGHT_DECAY = 0\n",
    "LEARNING_RATE = 1e-4  # 2e-4\n",
    "RATIO_WARMUP_STEPS = .1\n",
    "DROPOUT = .3\n",
    "ACUMULATE_GRAD_EVERY = 4\n",
    "OPTIMIZER = Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# get data\n",
    "training_set = read_data_from_file(DATA_TR_PATH)\n",
    "val_set = read_data_from_file(DATA_VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically extract labels and their indexes from data.\n",
    "labels2ind, labels_count = get_labels(training_set + val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loaders for datasets\n",
    "training_set = NerDataset(dataset=training_set,\n",
    "                          tokenizer=tokenizer,\n",
    "                          labels2ind=labels2ind,\n",
    "                          max_len_seq=MAX_LEN_SEQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = NerDataset(dataset=val_set,\n",
    "                     tokenizer=tokenizer,\n",
    "                     labels2ind=labels2ind,\n",
    "                     max_len_seq=MAX_LEN_SEQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_tr = DataLoader(dataset=training_set,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=True)\n",
    "\n",
    "dataloader_val = DataLoader(dataset=val_set,\n",
    "                            batch_size=BATCH_SIZE_VAL,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "nerbert = BertForTokenClassification.from_pretrained(MODEL_NAME,\n",
    "                                                     hidden_dropout_prob=DROPOUT,\n",
    "                                                     attention_probs_dropout_prob=DROPOUT,\n",
    "                                                     num_labels=len(labels2ind),\n",
    "                                                     id2label={str(v): k for k, v in labels2ind.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "optimizer = get_optimizer_with_weight_decay(model=nerbert,\n",
    "                                            optimizer=OPTIMIZER,\n",
    "                                            learning_rate=LEARNING_RATE,\n",
    "                                            weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "training_steps = (len(dataloader_tr)//ACUMULATE_GRAD_EVERY) * N_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n",
    "                                            num_warmup_steps=training_steps * RATIO_WARMUP_STEPS,\n",
    "                                            num_training_steps=training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = BertTrainer(model=nerbert,\n",
    "                      tokenizer=tokenizer,\n",
    "                      optimizer=optimizer,\n",
    "                      scheduler=scheduler,\n",
    "                      labels2ind=labels2ind,\n",
    "                      device=DEVICE,\n",
    "                      n_epochs=N_EPOCHS,\n",
    "                      accumulate_grad_every=ACUMULATE_GRAD_EVERY,\n",
    "                      output_dir='/Users/sdeshpande/Desktop/bioinformatices/bio_ner_bert/trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Epoch: 0/5 - Step:   0/578 - Training Loss: 2.409743\n",
      "- Epoch: 0/5 - Step:  10/578 - Training Loss: 2.291913\n",
      "- Epoch: 0/5 - Step:  20/578 - Training Loss: 2.035993\n",
      "- Epoch: 0/5 - Step:  30/578 - Training Loss: 1.754588\n",
      "- Epoch: 0/5 - Step:  40/578 - Training Loss: 1.574826\n",
      "- Epoch: 0/5 - Step:  50/578 - Training Loss: 1.451670\n",
      "- Epoch: 0/5 - Step:  60/578 - Training Loss: 1.351268\n",
      "- Epoch: 0/5 - Step:  70/578 - Training Loss: 1.264252\n",
      "- Epoch: 0/5 - Step:  80/578 - Training Loss: 1.180885\n",
      "- Epoch: 0/5 - Step:  90/578 - Training Loss: 1.111903\n",
      "- Epoch: 0/5 - Step: 100/578 - Training Loss: 1.048262\n",
      "- Epoch: 0/5 - Step: 110/578 - Training Loss: 0.988361\n",
      "- Epoch: 0/5 - Step: 120/578 - Training Loss: 0.936783\n",
      "- Epoch: 0/5 - Step: 130/578 - Training Loss: 0.891917\n",
      "- Epoch: 0/5 - Step: 140/578 - Training Loss: 0.849834\n",
      "- Epoch: 0/5 - Step: 150/578 - Training Loss: 0.812592\n",
      "- Epoch: 0/5 - Step: 160/578 - Training Loss: 0.780114\n",
      "- Epoch: 0/5 - Step: 170/578 - Training Loss: 0.751390\n",
      "- Epoch: 0/5 - Step: 180/578 - Training Loss: 0.724837\n",
      "- Epoch: 0/5 - Step: 190/578 - Training Loss: 0.700387\n",
      "- Epoch: 0/5 - Step: 200/578 - Training Loss: 0.679391\n",
      "- Epoch: 0/5 - Step: 210/578 - Training Loss: 0.658435\n",
      "- Epoch: 0/5 - Step: 220/578 - Training Loss: 0.638909\n",
      "- Epoch: 0/5 - Step: 230/578 - Training Loss: 0.621348\n",
      "- Epoch: 0/5 - Step: 240/578 - Training Loss: 0.605507\n",
      "- Epoch: 0/5 - Step: 250/578 - Training Loss: 0.590834\n",
      "- Epoch: 0/5 - Step: 260/578 - Training Loss: 0.577172\n",
      "- Epoch: 0/5 - Step: 270/578 - Training Loss: 0.563840\n",
      "- Epoch: 0/5 - Step: 280/578 - Training Loss: 0.551382\n",
      "- Epoch: 0/5 - Step: 290/578 - Training Loss: 0.540667\n",
      "- Epoch: 0/5 - Step: 300/578 - Training Loss: 0.530075\n",
      "- Epoch: 0/5 - Step: 310/578 - Training Loss: 0.520096\n",
      "- Epoch: 0/5 - Step: 320/578 - Training Loss: 0.510230\n",
      "- Epoch: 0/5 - Step: 330/578 - Training Loss: 0.501210\n",
      "- Epoch: 0/5 - Step: 340/578 - Training Loss: 0.493038\n",
      "- Epoch: 0/5 - Step: 350/578 - Training Loss: 0.484876\n",
      "- Epoch: 0/5 - Step: 360/578 - Training Loss: 0.477433\n",
      "- Epoch: 0/5 - Step: 370/578 - Training Loss: 0.470516\n",
      "- Epoch: 0/5 - Step: 380/578 - Training Loss: 0.464037\n",
      "- Epoch: 0/5 - Step: 390/578 - Training Loss: 0.457732\n",
      "- Epoch: 0/5 - Step: 400/578 - Training Loss: 0.451407\n",
      "- Epoch: 0/5 - Step: 410/578 - Training Loss: 0.445348\n",
      "- Epoch: 0/5 - Step: 420/578 - Training Loss: 0.439532\n",
      "- Epoch: 0/5 - Step: 430/578 - Training Loss: 0.434054\n",
      "- Epoch: 0/5 - Step: 440/578 - Training Loss: 0.428812\n",
      "- Epoch: 0/5 - Step: 450/578 - Training Loss: 0.423766\n",
      "- Epoch: 0/5 - Step: 460/578 - Training Loss: 0.419182\n",
      "- Epoch: 0/5 - Step: 470/578 - Training Loss: 0.414544\n",
      "- Epoch: 0/5 - Step: 480/578 - Training Loss: 0.409892\n",
      "- Epoch: 0/5 - Step: 490/578 - Training Loss: 0.405222\n",
      "- Epoch: 0/5 - Step: 500/578 - Training Loss: 0.401311\n",
      "- Epoch: 0/5 - Step: 510/578 - Training Loss: 0.397478\n",
      "- Epoch: 0/5 - Step: 520/578 - Training Loss: 0.393716\n",
      "- Epoch: 0/5 - Step: 530/578 - Training Loss: 0.390011\n",
      "- Epoch: 0/5 - Step: 540/578 - Training Loss: 0.386392\n",
      "- Epoch: 0/5 - Step: 550/578 - Training Loss: 0.383125\n",
      "- Epoch: 0/5 - Step: 560/578 - Training Loss: 0.379506\n",
      "- Epoch: 0/5 - Step: 570/578 - Training Loss: 0.375906\n",
      "- Epoch: 0/5 - Training Loss: 0.37590606879906147\n",
      "- Epoch: 0/5 - Validation Loss: 0.21983185259328372\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  protein     0.6748    0.8274    0.7433      5063\n",
      "cell_type     0.7146    0.7562    0.7348      1920\n",
      "cell_line     0.5194    0.6420    0.5742       500\n",
      "      DNA     0.6571    0.7619    0.7056      1054\n",
      "      RNA     0.6074    0.8390    0.7046       118\n",
      "\n",
      "micro avg     0.6701    0.7931    0.7264      8655\n",
      "macro avg     0.6716    0.7931    0.7266      8655\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-DNA     0.7193    0.7827    0.7497      1054\n",
      "       B-RNA     0.6846    0.8644    0.7640       118\n",
      " B-cell_line     0.5952    0.6880    0.6382       500\n",
      " B-cell_type     0.7730    0.7661    0.7696      1920\n",
      "   B-protein     0.7432    0.8866    0.8086      5063\n",
      "       I-DNA     0.7554    0.9289    0.8332      1785\n",
      "       I-RNA     0.7148    0.9786    0.8262       187\n",
      " I-cell_line     0.6171    0.7806    0.6893       989\n",
      " I-cell_type     0.8082    0.8595    0.8331      2990\n",
      "   I-protein     0.7930    0.8321    0.8121      4770\n",
      "           O     0.9859    0.9577    0.9716     81520\n",
      "\n",
      "    accuracy                         0.9362    100896\n",
      "   macro avg     0.7445    0.8478    0.7905    100896\n",
      "weighted avg     0.9420    0.9362    0.9382    100896\n",
      "\n",
      "- Epoch: 1/5 - Step:   0/578 - Training Loss: 0.194631\n",
      "- Epoch: 1/5 - Step:  10/578 - Training Loss: 0.174847\n",
      "- Epoch: 1/5 - Step:  20/578 - Training Loss: 0.178185\n",
      "- Epoch: 1/5 - Step:  30/578 - Training Loss: 0.176801\n",
      "- Epoch: 1/5 - Step:  40/578 - Training Loss: 0.169648\n",
      "- Epoch: 1/5 - Step:  50/578 - Training Loss: 0.167953\n",
      "- Epoch: 1/5 - Step:  60/578 - Training Loss: 0.168343\n",
      "- Epoch: 1/5 - Step:  70/578 - Training Loss: 0.170109\n",
      "- Epoch: 1/5 - Step:  80/578 - Training Loss: 0.167456\n",
      "- Epoch: 1/5 - Step:  90/578 - Training Loss: 0.168269\n",
      "- Epoch: 1/5 - Step: 100/578 - Training Loss: 0.169785\n",
      "- Epoch: 1/5 - Step: 110/578 - Training Loss: 0.169157\n",
      "- Epoch: 1/5 - Step: 120/578 - Training Loss: 0.168992\n",
      "- Epoch: 1/5 - Step: 130/578 - Training Loss: 0.170593\n",
      "- Epoch: 1/5 - Step: 140/578 - Training Loss: 0.170998\n",
      "- Epoch: 1/5 - Step: 150/578 - Training Loss: 0.170703\n",
      "- Epoch: 1/5 - Step: 160/578 - Training Loss: 0.169594\n",
      "- Epoch: 1/5 - Step: 170/578 - Training Loss: 0.169422\n",
      "- Epoch: 1/5 - Step: 180/578 - Training Loss: 0.169801\n",
      "- Epoch: 1/5 - Step: 190/578 - Training Loss: 0.169455\n",
      "- Epoch: 1/5 - Step: 200/578 - Training Loss: 0.169566\n",
      "- Epoch: 1/5 - Step: 210/578 - Training Loss: 0.169670\n",
      "- Epoch: 1/5 - Step: 220/578 - Training Loss: 0.170162\n",
      "- Epoch: 1/5 - Step: 230/578 - Training Loss: 0.169552\n",
      "- Epoch: 1/5 - Step: 240/578 - Training Loss: 0.169281\n",
      "- Epoch: 1/5 - Step: 250/578 - Training Loss: 0.169377\n",
      "- Epoch: 1/5 - Step: 260/578 - Training Loss: 0.169369\n",
      "- Epoch: 1/5 - Step: 270/578 - Training Loss: 0.169910\n",
      "- Epoch: 1/5 - Step: 280/578 - Training Loss: 0.169712\n",
      "- Epoch: 1/5 - Step: 290/578 - Training Loss: 0.168908\n",
      "- Epoch: 1/5 - Step: 300/578 - Training Loss: 0.169336\n",
      "- Epoch: 1/5 - Step: 310/578 - Training Loss: 0.169043\n",
      "- Epoch: 1/5 - Step: 320/578 - Training Loss: 0.168454\n",
      "- Epoch: 1/5 - Step: 330/578 - Training Loss: 0.168214\n",
      "- Epoch: 1/5 - Step: 340/578 - Training Loss: 0.168754\n",
      "- Epoch: 1/5 - Step: 350/578 - Training Loss: 0.169476\n",
      "- Epoch: 1/5 - Step: 360/578 - Training Loss: 0.169347\n",
      "- Epoch: 1/5 - Step: 370/578 - Training Loss: 0.168734\n",
      "- Epoch: 1/5 - Step: 380/578 - Training Loss: 0.168242\n",
      "- Epoch: 1/5 - Step: 390/578 - Training Loss: 0.168317\n",
      "- Epoch: 1/5 - Step: 400/578 - Training Loss: 0.168012\n",
      "- Epoch: 1/5 - Step: 410/578 - Training Loss: 0.167816\n",
      "- Epoch: 1/5 - Step: 420/578 - Training Loss: 0.167761\n",
      "- Epoch: 1/5 - Step: 430/578 - Training Loss: 0.167558\n",
      "- Epoch: 1/5 - Step: 440/578 - Training Loss: 0.167199\n",
      "- Epoch: 1/5 - Step: 450/578 - Training Loss: 0.167198\n",
      "- Epoch: 1/5 - Step: 460/578 - Training Loss: 0.166987\n",
      "- Epoch: 1/5 - Step: 470/578 - Training Loss: 0.167405\n",
      "- Epoch: 1/5 - Step: 480/578 - Training Loss: 0.167157\n",
      "- Epoch: 1/5 - Step: 490/578 - Training Loss: 0.166912\n",
      "- Epoch: 1/5 - Step: 500/578 - Training Loss: 0.166598\n",
      "- Epoch: 1/5 - Step: 510/578 - Training Loss: 0.166324\n",
      "- Epoch: 1/5 - Step: 520/578 - Training Loss: 0.166282\n",
      "- Epoch: 1/5 - Step: 530/578 - Training Loss: 0.166510\n",
      "- Epoch: 1/5 - Step: 540/578 - Training Loss: 0.166419\n",
      "- Epoch: 1/5 - Step: 550/578 - Training Loss: 0.166110\n",
      "- Epoch: 1/5 - Step: 560/578 - Training Loss: 0.165950\n",
      "- Epoch: 1/5 - Step: 570/578 - Training Loss: 0.165707\n",
      "- Epoch: 1/5 - Training Loss: 0.165706807995401\n",
      "- Epoch: 1/5 - Validation Loss: 0.2029390742705352\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  protein     0.6580    0.8422    0.7388      5063\n",
      "cell_type     0.7401    0.7682    0.7539      1920\n",
      "cell_line     0.5588    0.6460    0.5993       500\n",
      "      DNA     0.6496    0.7334    0.6889      1054\n",
      "      RNA     0.7154    0.7881    0.7500       118\n",
      "\n",
      "micro avg     0.6680    0.8005    0.7283      8655\n",
      "macro avg     0.6703    0.8005    0.7282      8655\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-DNA     0.7362    0.7865    0.7606      1054\n",
      "       B-RNA     0.7886    0.8220    0.8050       118\n",
      " B-cell_line     0.6622    0.6980    0.6796       500\n",
      " B-cell_type     0.8011    0.7844    0.7926      1920\n",
      "   B-protein     0.7221    0.9076    0.8043      5063\n",
      "       I-DNA     0.8165    0.8527    0.8342      1785\n",
      "       I-RNA     0.8108    0.9626    0.8802       187\n",
      " I-cell_line     0.7118    0.7391    0.7252       989\n",
      " I-cell_type     0.8351    0.8756    0.8549      2990\n",
      "   I-protein     0.7996    0.8447    0.8215      4770\n",
      "           O     0.9844    0.9611    0.9727     81520\n",
      "\n",
      "    accuracy                         0.9397    100896\n",
      "   macro avg     0.7880    0.8395    0.8119    100896\n",
      "weighted avg     0.9442    0.9397    0.9412    100896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Epoch: 2/5 - Step:   0/578 - Training Loss: 0.121493\n",
      "- Epoch: 2/5 - Step:  10/578 - Training Loss: 0.138313\n",
      "- Epoch: 2/5 - Step:  20/578 - Training Loss: 0.136047\n",
      "- Epoch: 2/5 - Step:  30/578 - Training Loss: 0.140055\n",
      "- Epoch: 2/5 - Step:  40/578 - Training Loss: 0.140122\n",
      "- Epoch: 2/5 - Step:  50/578 - Training Loss: 0.138909\n",
      "- Epoch: 2/5 - Step:  60/578 - Training Loss: 0.145421\n",
      "- Epoch: 2/5 - Step:  70/578 - Training Loss: 0.146230\n",
      "- Epoch: 2/5 - Step:  80/578 - Training Loss: 0.144953\n",
      "- Epoch: 2/5 - Step:  90/578 - Training Loss: 0.144989\n",
      "- Epoch: 2/5 - Step: 100/578 - Training Loss: 0.146096\n",
      "- Epoch: 2/5 - Step: 110/578 - Training Loss: 0.144722\n",
      "- Epoch: 2/5 - Step: 120/578 - Training Loss: 0.144695\n",
      "- Epoch: 2/5 - Step: 130/578 - Training Loss: 0.145071\n",
      "- Epoch: 2/5 - Step: 140/578 - Training Loss: 0.145181\n",
      "- Epoch: 2/5 - Step: 150/578 - Training Loss: 0.144830\n",
      "- Epoch: 2/5 - Step: 160/578 - Training Loss: 0.145522\n",
      "- Epoch: 2/5 - Step: 170/578 - Training Loss: 0.146039\n",
      "- Epoch: 2/5 - Step: 180/578 - Training Loss: 0.145754\n",
      "- Epoch: 2/5 - Step: 190/578 - Training Loss: 0.145553\n",
      "- Epoch: 2/5 - Step: 200/578 - Training Loss: 0.145705\n",
      "- Epoch: 2/5 - Step: 210/578 - Training Loss: 0.145066\n",
      "- Epoch: 2/5 - Step: 220/578 - Training Loss: 0.144369\n",
      "- Epoch: 2/5 - Step: 230/578 - Training Loss: 0.144596\n",
      "- Epoch: 2/5 - Step: 240/578 - Training Loss: 0.145058\n",
      "- Epoch: 2/5 - Step: 250/578 - Training Loss: 0.144553\n",
      "- Epoch: 2/5 - Step: 260/578 - Training Loss: 0.144604\n",
      "- Epoch: 2/5 - Step: 270/578 - Training Loss: 0.144622\n",
      "- Epoch: 2/5 - Step: 280/578 - Training Loss: 0.144539\n",
      "- Epoch: 2/5 - Step: 290/578 - Training Loss: 0.144217\n",
      "- Epoch: 2/5 - Step: 300/578 - Training Loss: 0.143641\n",
      "- Epoch: 2/5 - Step: 310/578 - Training Loss: 0.143585\n",
      "- Epoch: 2/5 - Step: 320/578 - Training Loss: 0.143365\n",
      "- Epoch: 2/5 - Step: 330/578 - Training Loss: 0.143341\n",
      "- Epoch: 2/5 - Step: 340/578 - Training Loss: 0.142999\n",
      "- Epoch: 2/5 - Step: 350/578 - Training Loss: 0.142950\n",
      "- Epoch: 2/5 - Step: 360/578 - Training Loss: 0.143208\n",
      "- Epoch: 2/5 - Step: 370/578 - Training Loss: 0.143071\n",
      "- Epoch: 2/5 - Step: 380/578 - Training Loss: 0.142866\n",
      "- Epoch: 2/5 - Step: 390/578 - Training Loss: 0.142265\n",
      "- Epoch: 2/5 - Step: 400/578 - Training Loss: 0.142170\n",
      "- Epoch: 2/5 - Step: 410/578 - Training Loss: 0.141836\n",
      "- Epoch: 2/5 - Step: 420/578 - Training Loss: 0.141695\n",
      "- Epoch: 2/5 - Step: 430/578 - Training Loss: 0.141229\n",
      "- Epoch: 2/5 - Step: 440/578 - Training Loss: 0.141152\n",
      "- Epoch: 2/5 - Step: 450/578 - Training Loss: 0.140986\n",
      "- Epoch: 2/5 - Step: 460/578 - Training Loss: 0.140878\n",
      "- Epoch: 2/5 - Step: 470/578 - Training Loss: 0.140828\n",
      "- Epoch: 2/5 - Step: 480/578 - Training Loss: 0.140918\n",
      "- Epoch: 2/5 - Step: 490/578 - Training Loss: 0.141027\n",
      "- Epoch: 2/5 - Step: 500/578 - Training Loss: 0.141039\n",
      "- Epoch: 2/5 - Step: 510/578 - Training Loss: 0.140986\n",
      "- Epoch: 2/5 - Step: 520/578 - Training Loss: 0.141105\n",
      "- Epoch: 2/5 - Step: 530/578 - Training Loss: 0.140776\n",
      "- Epoch: 2/5 - Step: 540/578 - Training Loss: 0.141024\n",
      "- Epoch: 2/5 - Step: 550/578 - Training Loss: 0.140859\n",
      "- Epoch: 2/5 - Step: 560/578 - Training Loss: 0.140510\n",
      "- Epoch: 2/5 - Step: 570/578 - Training Loss: 0.140481\n",
      "- Epoch: 2/5 - Training Loss: 0.14048133191983697\n",
      "- Epoch: 2/5 - Validation Loss: 0.21392140464614268\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  protein     0.6746    0.8434    0.7496      5063\n",
      "cell_type     0.7702    0.7312    0.7502      1920\n",
      "cell_line     0.4918    0.7160    0.5831       500\n",
      "      DNA     0.6784    0.7884    0.7293      1054\n",
      "      RNA     0.6376    0.8051    0.7116       118\n",
      "\n",
      "micro avg     0.6785    0.8039    0.7359      8655\n",
      "macro avg     0.6852    0.8039    0.7371      8655\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-DNA     0.7188    0.8150    0.7639      1054\n",
      "       B-RNA     0.6781    0.8390    0.7500       118\n",
      " B-cell_line     0.5643    0.7640    0.6491       500\n",
      " B-cell_type     0.8283    0.7464    0.7852      1920\n",
      "   B-protein     0.7342    0.9060    0.8111      5063\n",
      "       I-DNA     0.7805    0.9305    0.8490      1785\n",
      "       I-RNA     0.7860    0.9626    0.8654       187\n",
      " I-cell_line     0.5597    0.8342    0.6699       989\n",
      " I-cell_type     0.8372    0.8104    0.8236      2990\n",
      "   I-protein     0.7943    0.8541    0.8231      4770\n",
      "           O     0.9867    0.9568    0.9715     81520\n",
      "\n",
      "    accuracy                         0.9368    100896\n",
      "   macro avg     0.7517    0.8563    0.7965    100896\n",
      "weighted avg     0.9440    0.9368    0.9392    100896\n",
      "\n",
      "- Epoch: 3/5 - Step:   0/578 - Training Loss: 0.135275\n",
      "- Epoch: 3/5 - Step:  10/578 - Training Loss: 0.115492\n",
      "- Epoch: 3/5 - Step:  20/578 - Training Loss: 0.123795\n",
      "- Epoch: 3/5 - Step:  30/578 - Training Loss: 0.114251\n",
      "- Epoch: 3/5 - Step:  40/578 - Training Loss: 0.114911\n",
      "- Epoch: 3/5 - Step:  50/578 - Training Loss: 0.115010\n",
      "- Epoch: 3/5 - Step:  60/578 - Training Loss: 0.114114\n",
      "- Epoch: 3/5 - Step:  70/578 - Training Loss: 0.114807\n",
      "- Epoch: 3/5 - Step:  80/578 - Training Loss: 0.117724\n",
      "- Epoch: 3/5 - Step:  90/578 - Training Loss: 0.118459\n",
      "- Epoch: 3/5 - Step: 100/578 - Training Loss: 0.117282\n",
      "- Epoch: 3/5 - Step: 110/578 - Training Loss: 0.117887\n",
      "- Epoch: 3/5 - Step: 120/578 - Training Loss: 0.119047\n",
      "- Epoch: 3/5 - Step: 130/578 - Training Loss: 0.120076\n",
      "- Epoch: 3/5 - Step: 140/578 - Training Loss: 0.120830\n",
      "- Epoch: 3/5 - Step: 150/578 - Training Loss: 0.121876\n",
      "- Epoch: 3/5 - Step: 160/578 - Training Loss: 0.122311\n",
      "- Epoch: 3/5 - Step: 170/578 - Training Loss: 0.122161\n",
      "- Epoch: 3/5 - Step: 180/578 - Training Loss: 0.122308\n",
      "- Epoch: 3/5 - Step: 190/578 - Training Loss: 0.122142\n",
      "- Epoch: 3/5 - Step: 200/578 - Training Loss: 0.123040\n",
      "- Epoch: 3/5 - Step: 210/578 - Training Loss: 0.123236\n",
      "- Epoch: 3/5 - Step: 220/578 - Training Loss: 0.123114\n",
      "- Epoch: 3/5 - Step: 230/578 - Training Loss: 0.123316\n",
      "- Epoch: 3/5 - Step: 240/578 - Training Loss: 0.122938\n",
      "- Epoch: 3/5 - Step: 250/578 - Training Loss: 0.122728\n",
      "- Epoch: 3/5 - Step: 260/578 - Training Loss: 0.122071\n",
      "- Epoch: 3/5 - Step: 270/578 - Training Loss: 0.121548\n",
      "- Epoch: 3/5 - Step: 280/578 - Training Loss: 0.121549\n",
      "- Epoch: 3/5 - Step: 290/578 - Training Loss: 0.121483\n",
      "- Epoch: 3/5 - Step: 300/578 - Training Loss: 0.121064\n",
      "- Epoch: 3/5 - Step: 310/578 - Training Loss: 0.121036\n",
      "- Epoch: 3/5 - Step: 320/578 - Training Loss: 0.121002\n",
      "- Epoch: 3/5 - Step: 330/578 - Training Loss: 0.120905\n",
      "- Epoch: 3/5 - Step: 340/578 - Training Loss: 0.120883\n",
      "- Epoch: 3/5 - Step: 350/578 - Training Loss: 0.121012\n",
      "- Epoch: 3/5 - Step: 360/578 - Training Loss: 0.121426\n",
      "- Epoch: 3/5 - Step: 370/578 - Training Loss: 0.121337\n",
      "- Epoch: 3/5 - Step: 380/578 - Training Loss: 0.121713\n",
      "- Epoch: 3/5 - Step: 390/578 - Training Loss: 0.121783\n",
      "- Epoch: 3/5 - Step: 400/578 - Training Loss: 0.121289\n",
      "- Epoch: 3/5 - Step: 410/578 - Training Loss: 0.121163\n",
      "- Epoch: 3/5 - Step: 420/578 - Training Loss: 0.121599\n",
      "- Epoch: 3/5 - Step: 430/578 - Training Loss: 0.121810\n",
      "- Epoch: 3/5 - Step: 440/578 - Training Loss: 0.121786\n",
      "- Epoch: 3/5 - Step: 450/578 - Training Loss: 0.121847\n",
      "- Epoch: 3/5 - Step: 460/578 - Training Loss: 0.122263\n",
      "- Epoch: 3/5 - Step: 470/578 - Training Loss: 0.122261\n",
      "- Epoch: 3/5 - Step: 480/578 - Training Loss: 0.122247\n",
      "- Epoch: 3/5 - Step: 490/578 - Training Loss: 0.122397\n",
      "- Epoch: 3/5 - Step: 500/578 - Training Loss: 0.122693\n",
      "- Epoch: 3/5 - Step: 510/578 - Training Loss: 0.122655\n",
      "- Epoch: 3/5 - Step: 520/578 - Training Loss: 0.122806\n",
      "- Epoch: 3/5 - Step: 530/578 - Training Loss: 0.123224\n",
      "- Epoch: 3/5 - Step: 540/578 - Training Loss: 0.123305\n",
      "- Epoch: 3/5 - Step: 550/578 - Training Loss: 0.123340\n",
      "- Epoch: 3/5 - Step: 560/578 - Training Loss: 0.123426\n",
      "- Epoch: 3/5 - Step: 570/578 - Training Loss: 0.123308\n",
      "- Epoch: 3/5 - Training Loss: 0.12330803851843297\n",
      "- Epoch: 3/5 - Validation Loss: 0.21154211180797522\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  protein     0.6858    0.8384    0.7545      5063\n",
      "cell_type     0.7526    0.7448    0.7487      1920\n",
      "cell_line     0.5282    0.6920    0.5991       500\n",
      "      DNA     0.6820    0.7856    0.7302      1054\n",
      "      RNA     0.6620    0.7966    0.7231       118\n",
      "\n",
      "micro avg     0.6874    0.8022    0.7403      8655\n",
      "macro avg     0.6907    0.8022    0.7408      8655\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-DNA     0.7274    0.8178    0.7700      1054\n",
      "       B-RNA     0.7174    0.8390    0.7734       118\n",
      " B-cell_line     0.6167    0.7240    0.6661       500\n",
      " B-cell_type     0.7998    0.7615    0.7801      1920\n",
      "   B-protein     0.7423    0.8937    0.8110      5063\n",
      "       I-DNA     0.7960    0.9182    0.8528      1785\n",
      "       I-RNA     0.8241    0.9519    0.8834       187\n",
      " I-cell_line     0.6019    0.8119    0.6913       989\n",
      " I-cell_type     0.8407    0.8157    0.8280      2990\n",
      "   I-protein     0.8041    0.8526    0.8276      4770\n",
      "           O     0.9854    0.9609    0.9730     81520\n",
      "\n",
      "    accuracy                         0.9392    100896\n",
      "   macro avg     0.7687    0.8497    0.8052    100896\n",
      "weighted avg     0.9445    0.9392    0.9411    100896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Epoch: 4/5 - Step:   0/578 - Training Loss: 0.099437\n",
      "- Epoch: 4/5 - Step:  10/578 - Training Loss: 0.109565\n",
      "- Epoch: 4/5 - Step:  20/578 - Training Loss: 0.110913\n",
      "- Epoch: 4/5 - Step:  30/578 - Training Loss: 0.110964\n",
      "- Epoch: 4/5 - Step:  40/578 - Training Loss: 0.105466\n",
      "- Epoch: 4/5 - Step:  50/578 - Training Loss: 0.105688\n",
      "- Epoch: 4/5 - Step:  60/578 - Training Loss: 0.105968\n",
      "- Epoch: 4/5 - Step:  70/578 - Training Loss: 0.108394\n",
      "- Epoch: 4/5 - Step:  80/578 - Training Loss: 0.109063\n",
      "- Epoch: 4/5 - Step:  90/578 - Training Loss: 0.110021\n",
      "- Epoch: 4/5 - Step: 100/578 - Training Loss: 0.110566\n",
      "- Epoch: 4/5 - Step: 110/578 - Training Loss: 0.109592\n",
      "- Epoch: 4/5 - Step: 120/578 - Training Loss: 0.108561\n",
      "- Epoch: 4/5 - Step: 130/578 - Training Loss: 0.108495\n",
      "- Epoch: 4/5 - Step: 140/578 - Training Loss: 0.108864\n",
      "- Epoch: 4/5 - Step: 150/578 - Training Loss: 0.108699\n",
      "- Epoch: 4/5 - Step: 160/578 - Training Loss: 0.108431\n",
      "- Epoch: 4/5 - Step: 170/578 - Training Loss: 0.109087\n",
      "- Epoch: 4/5 - Step: 180/578 - Training Loss: 0.109062\n",
      "- Epoch: 4/5 - Step: 190/578 - Training Loss: 0.109313\n",
      "- Epoch: 4/5 - Step: 200/578 - Training Loss: 0.108401\n",
      "- Epoch: 4/5 - Step: 210/578 - Training Loss: 0.108972\n",
      "- Epoch: 4/5 - Step: 220/578 - Training Loss: 0.109385\n",
      "- Epoch: 4/5 - Step: 230/578 - Training Loss: 0.109603\n",
      "- Epoch: 4/5 - Step: 240/578 - Training Loss: 0.109243\n",
      "- Epoch: 4/5 - Step: 250/578 - Training Loss: 0.108941\n",
      "- Epoch: 4/5 - Step: 260/578 - Training Loss: 0.108854\n",
      "- Epoch: 4/5 - Step: 270/578 - Training Loss: 0.108734\n",
      "- Epoch: 4/5 - Step: 280/578 - Training Loss: 0.109021\n",
      "- Epoch: 4/5 - Step: 290/578 - Training Loss: 0.109439\n",
      "- Epoch: 4/5 - Step: 300/578 - Training Loss: 0.109383\n",
      "- Epoch: 4/5 - Step: 310/578 - Training Loss: 0.109623\n",
      "- Epoch: 4/5 - Step: 320/578 - Training Loss: 0.109293\n",
      "- Epoch: 4/5 - Step: 330/578 - Training Loss: 0.109408\n",
      "- Epoch: 4/5 - Step: 340/578 - Training Loss: 0.109531\n",
      "- Epoch: 4/5 - Step: 350/578 - Training Loss: 0.109734\n",
      "- Epoch: 4/5 - Step: 360/578 - Training Loss: 0.109709\n",
      "- Epoch: 4/5 - Step: 370/578 - Training Loss: 0.109669\n",
      "- Epoch: 4/5 - Step: 380/578 - Training Loss: 0.109388\n",
      "- Epoch: 4/5 - Step: 390/578 - Training Loss: 0.108955\n",
      "- Epoch: 4/5 - Step: 400/578 - Training Loss: 0.108742\n",
      "- Epoch: 4/5 - Step: 410/578 - Training Loss: 0.109112\n",
      "- Epoch: 4/5 - Step: 420/578 - Training Loss: 0.108778\n",
      "- Epoch: 4/5 - Step: 430/578 - Training Loss: 0.108747\n",
      "- Epoch: 4/5 - Step: 440/578 - Training Loss: 0.108653\n",
      "- Epoch: 4/5 - Step: 450/578 - Training Loss: 0.108940\n",
      "- Epoch: 4/5 - Step: 460/578 - Training Loss: 0.108670\n",
      "- Epoch: 4/5 - Step: 470/578 - Training Loss: 0.108617\n",
      "- Epoch: 4/5 - Step: 480/578 - Training Loss: 0.108558\n",
      "- Epoch: 4/5 - Step: 490/578 - Training Loss: 0.108391\n",
      "- Epoch: 4/5 - Step: 500/578 - Training Loss: 0.108497\n",
      "- Epoch: 4/5 - Step: 510/578 - Training Loss: 0.108462\n",
      "- Epoch: 4/5 - Step: 520/578 - Training Loss: 0.108436\n",
      "- Epoch: 4/5 - Step: 530/578 - Training Loss: 0.108488\n",
      "- Epoch: 4/5 - Step: 540/578 - Training Loss: 0.108515\n",
      "- Epoch: 4/5 - Step: 550/578 - Training Loss: 0.108276\n",
      "- Epoch: 4/5 - Step: 560/578 - Training Loss: 0.108530\n",
      "- Epoch: 4/5 - Step: 570/578 - Training Loss: 0.108237\n",
      "- Epoch: 4/5 - Training Loss: 0.10823663306339348\n",
      "- Epoch: 4/5 - Validation Loss: 0.21867519077183545\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  protein     0.6814    0.8388    0.7519      5063\n",
      "cell_type     0.7363    0.7547    0.7454      1920\n",
      "cell_line     0.5519    0.7120    0.6218       500\n",
      "      DNA     0.6669    0.7751    0.7170      1054\n",
      "      RNA     0.6691    0.7712    0.7165       118\n",
      "\n",
      "micro avg     0.6819    0.8042    0.7380      8655\n",
      "macro avg     0.6842    0.8042    0.7382      8655\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-DNA     0.7233    0.8159    0.7668      1054\n",
      "       B-RNA     0.7252    0.8051    0.7631       118\n",
      " B-cell_line     0.6371    0.7480    0.6881       500\n",
      " B-cell_type     0.7898    0.7729    0.7813      1920\n",
      "   B-protein     0.7368    0.8969    0.8090      5063\n",
      "       I-DNA     0.7988    0.9008    0.8468      1785\n",
      "       I-RNA     0.8382    0.9144    0.8747       187\n",
      " I-cell_line     0.6463    0.8129    0.7201       989\n",
      " I-cell_type     0.8482    0.8221    0.8349      2990\n",
      "   I-protein     0.8092    0.8409    0.8247      4770\n",
      "           O     0.9842    0.9614    0.9727     81520\n",
      "\n",
      "    accuracy                         0.9394    100896\n",
      "   macro avg     0.7761    0.8447    0.8075    100896\n",
      "weighted avg     0.9442    0.9394    0.9411    100896\n",
      "\n",
      "- Epoch: 5/5 - Step:   0/578 - Training Loss: 0.080225\n",
      "- Epoch: 5/5 - Step:  10/578 - Training Loss: 0.076072\n",
      "- Epoch: 5/5 - Step:  20/578 - Training Loss: 0.081776\n",
      "- Epoch: 5/5 - Step:  30/578 - Training Loss: 0.086376\n",
      "- Epoch: 5/5 - Step:  40/578 - Training Loss: 0.091827\n",
      "- Epoch: 5/5 - Step:  50/578 - Training Loss: 0.091400\n",
      "- Epoch: 5/5 - Step:  60/578 - Training Loss: 0.094104\n",
      "- Epoch: 5/5 - Step:  70/578 - Training Loss: 0.094106\n",
      "- Epoch: 5/5 - Step:  80/578 - Training Loss: 0.092941\n",
      "- Epoch: 5/5 - Step:  90/578 - Training Loss: 0.092284\n",
      "- Epoch: 5/5 - Step: 100/578 - Training Loss: 0.093783\n",
      "- Epoch: 5/5 - Step: 110/578 - Training Loss: 0.095500\n",
      "- Epoch: 5/5 - Step: 120/578 - Training Loss: 0.096202\n",
      "- Epoch: 5/5 - Step: 130/578 - Training Loss: 0.096260\n",
      "- Epoch: 5/5 - Step: 140/578 - Training Loss: 0.095815\n",
      "- Epoch: 5/5 - Step: 150/578 - Training Loss: 0.095541\n",
      "- Epoch: 5/5 - Step: 160/578 - Training Loss: 0.095586\n",
      "- Epoch: 5/5 - Step: 170/578 - Training Loss: 0.095476\n",
      "- Epoch: 5/5 - Step: 180/578 - Training Loss: 0.095718\n",
      "- Epoch: 5/5 - Step: 190/578 - Training Loss: 0.095142\n",
      "- Epoch: 5/5 - Step: 200/578 - Training Loss: 0.095767\n",
      "- Epoch: 5/5 - Step: 210/578 - Training Loss: 0.095554\n",
      "- Epoch: 5/5 - Step: 220/578 - Training Loss: 0.095866\n",
      "- Epoch: 5/5 - Step: 230/578 - Training Loss: 0.096232\n",
      "- Epoch: 5/5 - Step: 240/578 - Training Loss: 0.096233\n",
      "- Epoch: 5/5 - Step: 250/578 - Training Loss: 0.095897\n",
      "- Epoch: 5/5 - Step: 260/578 - Training Loss: 0.095483\n",
      "- Epoch: 5/5 - Step: 270/578 - Training Loss: 0.095294\n",
      "- Epoch: 5/5 - Step: 280/578 - Training Loss: 0.095373\n",
      "- Epoch: 5/5 - Step: 290/578 - Training Loss: 0.095730\n",
      "- Epoch: 5/5 - Step: 300/578 - Training Loss: 0.095788\n",
      "- Epoch: 5/5 - Step: 310/578 - Training Loss: 0.095615\n",
      "- Epoch: 5/5 - Step: 320/578 - Training Loss: 0.095505\n",
      "- Epoch: 5/5 - Step: 330/578 - Training Loss: 0.095337\n",
      "- Epoch: 5/5 - Step: 340/578 - Training Loss: 0.095092\n",
      "- Epoch: 5/5 - Step: 350/578 - Training Loss: 0.094783\n",
      "- Epoch: 5/5 - Step: 360/578 - Training Loss: 0.094920\n",
      "- Epoch: 5/5 - Step: 370/578 - Training Loss: 0.094932\n",
      "- Epoch: 5/5 - Step: 380/578 - Training Loss: 0.094838\n",
      "- Epoch: 5/5 - Step: 390/578 - Training Loss: 0.094996\n",
      "- Epoch: 5/5 - Step: 400/578 - Training Loss: 0.095393\n",
      "- Epoch: 5/5 - Step: 410/578 - Training Loss: 0.095402\n",
      "- Epoch: 5/5 - Step: 420/578 - Training Loss: 0.095488\n",
      "- Epoch: 5/5 - Step: 430/578 - Training Loss: 0.095643\n",
      "- Epoch: 5/5 - Step: 440/578 - Training Loss: 0.095789\n",
      "- Epoch: 5/5 - Step: 450/578 - Training Loss: 0.095753\n",
      "- Epoch: 5/5 - Step: 460/578 - Training Loss: 0.095750\n",
      "- Epoch: 5/5 - Step: 470/578 - Training Loss: 0.096046\n",
      "- Epoch: 5/5 - Step: 480/578 - Training Loss: 0.095750\n",
      "- Epoch: 5/5 - Step: 490/578 - Training Loss: 0.095951\n",
      "- Epoch: 5/5 - Step: 500/578 - Training Loss: 0.096097\n",
      "- Epoch: 5/5 - Step: 510/578 - Training Loss: 0.096364\n",
      "- Epoch: 5/5 - Step: 520/578 - Training Loss: 0.096374\n",
      "- Epoch: 5/5 - Step: 530/578 - Training Loss: 0.096482\n",
      "- Epoch: 5/5 - Step: 540/578 - Training Loss: 0.096337\n",
      "- Epoch: 5/5 - Step: 550/578 - Training Loss: 0.096212\n",
      "- Epoch: 5/5 - Step: 560/578 - Training Loss: 0.096076\n",
      "- Epoch: 5/5 - Step: 570/578 - Training Loss: 0.096295\n",
      "- Epoch: 5/5 - Training Loss: 0.0962954140582241\n",
      "- Epoch: 5/5 - Validation Loss: 0.22014311236747797\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "  protein     0.6889    0.8392    0.7567      5063\n",
      "cell_type     0.7578    0.7432    0.7505      1920\n",
      "cell_line     0.5410    0.7260    0.6200       500\n",
      "      DNA     0.6781    0.7676    0.7201      1054\n",
      "      RNA     0.6259    0.7797    0.6943       118\n",
      "\n",
      "micro avg     0.6897    0.8018    0.7416      8655\n",
      "macro avg     0.6935    0.8018    0.7421      8655\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-DNA     0.7357    0.8027    0.7677      1054\n",
      "       B-RNA     0.6901    0.8305    0.7538       118\n",
      " B-cell_line     0.6028    0.7620    0.6731       500\n",
      " B-cell_type     0.8117    0.7568    0.7833      1920\n",
      "   B-protein     0.7440    0.8902    0.8105      5063\n",
      "       I-DNA     0.8062    0.9020    0.8514      1785\n",
      "       I-RNA     0.8208    0.9305    0.8722       187\n",
      " I-cell_line     0.6235    0.8140    0.7061       989\n",
      " I-cell_type     0.8574    0.8043    0.8300      2990\n",
      "   I-protein     0.8003    0.8562    0.8273      4770\n",
      "           O     0.9837    0.9619    0.9727     81520\n",
      "\n",
      "    accuracy                         0.9394    100896\n",
      "   macro avg     0.7706    0.8465    0.8044    100896\n",
      "weighted avg     0.9442    0.9394    0.9410    100896\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.37590606879906147,\n",
       "  0.165706807995401,\n",
       "  0.14048133191983697,\n",
       "  0.12330803851843297,\n",
       "  0.10823663306339348,\n",
       "  0.0962954140582241],\n",
       " [0.21983185259328372,\n",
       "  0.2029390742705352,\n",
       "  0.21392140464614268,\n",
       "  0.21154211180797522,\n",
       "  0.21867519077183545,\n",
       "  0.22014311236747797])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and validate model\n",
    "trainer.train(dataloader_train=dataloader_tr,\n",
    "              dataloader_val=dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on test set if any\n",
    "if DATA_TEST_PATH is not None:\n",
    "    print(f\"{'*'*40}\\n\\t\\tEVALUATION ON TEST SET\\n{'*'*40}\")\n",
    "    test_set = read_data_from_file(DATA_TEST_PATH)\n",
    "\n",
    "    test_set = NerDataset(dataset=test_set,\n",
    "                          tokenizer=tokenizer,\n",
    "                          labels2ind=labels2ind,\n",
    "                          max_len_seq=MAX_LEN_SEQ)\n",
    "\n",
    "    dataloader_test = DataLoader(dataset=test_set,\n",
    "                                 batch_size=BATCH_SIZE_VAL)\n",
    "\n",
    "    trainer.evaluate(dataloader_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Find learning rate ----- \n",
    "# import random\n",
    "\n",
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch_lr_finder import LRFinder\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# from data_utils.data_utils import *\n",
    "# from nn_utils.neural_architectures import *\n",
    "# from nn_utils.neural_architectures import BertForTokenClassificationCustom\n",
    "# from nn_utils.optimizers import get_optimizer_with_weight_decay\n",
    "\n",
    "# DATA_TR_PATH = '/Users/sdeshpande/Desktop/bioinformatices/MTL-Bioinformatics-2016/data/JNLPBA/original-data/train/Genia4ERtask1.iob2'\n",
    "# SEED = 42\n",
    "\n",
    "# # MODEL\n",
    "# MODEL_NAME = 'allenai/scibert_scivocab_cased'\n",
    "# MAX_LEN_SEQ = 128\n",
    "\n",
    "# # Optimization parameters\n",
    "# BATCH_SIZE_TR = 32\n",
    "# LEARNING_RATE = 1e-6\n",
    "# CLIPPING = None\n",
    "# OPTIMIZER = Adam\n",
    "\n",
    "# # get data\n",
    "# training_set = read_data_from_file(DATA_TR_PATH)\n",
    "\n",
    "# # Automatically extract labels and their indexes from data.\n",
    "# labels2ind, labels_count = get_labels(training_set)\n",
    "\n",
    "# # Load data\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# training_set = NerDataset(dataset=training_set,\n",
    "#                           tokenizer=tokenizer,\n",
    "#                           labels2ind=labels2ind,\n",
    "#                           max_len_seq=MAX_LEN_SEQ,\n",
    "#                           bert_hugging=False)\n",
    "\n",
    "\n",
    "# dataloader_tr = DataLoader(dataset=training_set,\n",
    "#                            batch_size=BATCH_SIZE_TR,\n",
    "#                            shuffle=True)\n",
    "\n",
    "# # Seeds\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# legend = []\n",
    "# fig = None\n",
    "\n",
    "# for wd in [0, .1, 1e-2, 1e-3, 1e-4]:\n",
    "#     for dp in [.1, 0.2, .3]:\n",
    "#         nerbert = BertForTokenClassificationCustom.from_pretrained(pretrained_model_name_or_path=MODEL_NAME,\n",
    "#                                                                    num_labels=len(labels2ind),\n",
    "#                                                                    hidden_dropout_prob=dp,\n",
    "#                                                                    attention_probs_dropout_prob=dp)\n",
    "\n",
    "#         # Prepare optimizer and schedule (linear warmup and decay)\n",
    "#         optimizer = get_optimizer_with_weight_decay(model=nerbert,\n",
    "#                                                     optimizer=OPTIMIZER,\n",
    "#                                                     learning_rate=LEARNING_RATE,\n",
    "#                                                     weight_decay=wd)\n",
    "\n",
    "#         lr_finder = LRFinder(nerbert, optimizer, nn.CrossEntropyLoss(), device='cuda')\n",
    "#         lr_finder.range_test(train_loader=dataloader_tr, end_lr=1, num_iter=100)\n",
    "#         fig = lr_finder.plot(ax=fig)\n",
    "#         legend.append(f\"wd: {wd}\")\n",
    "\n",
    "# fig.figure.legend(legend, loc='best')\n",
    "# fig.figure.tight_layout()\n",
    "# fig.figure.show()\n",
    "# fig.figure.savefig('lr_finder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse -> O\n",
      "thymus -> O\n",
      "was -> O\n",
      "used -> O\n",
      "as -> O\n",
      "a -> O\n",
      "source -> O\n",
      "of -> O\n",
      "glucocorticoid -> B-protein\n",
      "receptor -> I-protein\n",
      "from -> O\n",
      "normal -> B-cell_type\n",
      "cs -> B-cell_type\n",
      "lymphocytes -> I-cell_type\n",
      ". -> O\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Example\n",
    "text = \"Mouse thymus was used as a source of glucocorticoid receptor from normal CS lymphocytes.\"\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/sdeshpande/Desktop/bioinformatices/bio_ner_bert/trained_models/\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"/Users/sdeshpande/Desktop/bioinformatices/bio_ner_bert/trained_models/\")\n",
    "\n",
    "# Get input for BERT\n",
    "input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# From the output let's take the first element of the tuple.\n",
    "# Then, let's get rid of [CLS] and [SEP] tokens (first and last)\n",
    "predictions = outputs[0].argmax(axis=-1)[0][1:-1]\n",
    "\n",
    "# Map label class indexes to string labels.\n",
    "for token, pred in zip(tokenizer.tokenize(text), predictions):\n",
    "    print(token, '->', model.config.id2label[pred.numpy().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'glucocorticoid',\n",
       "  'score': 0.9929465651512146,\n",
       "  'entity': 'B-protein',\n",
       "  'index': 9},\n",
       " {'word': 'receptor',\n",
       "  'score': 0.9913542866706848,\n",
       "  'entity': 'I-protein',\n",
       "  'index': 10},\n",
       " {'word': 'normal',\n",
       "  'score': 0.6053049564361572,\n",
       "  'entity': 'B-cell_type',\n",
       "  'index': 12},\n",
       " {'word': 'cs',\n",
       "  'score': 0.5391853451728821,\n",
       "  'entity': 'B-cell_type',\n",
       "  'index': 13},\n",
       " {'word': 'lymphocytes',\n",
       "  'score': 0.9937543869018555,\n",
       "  'entity': 'I-cell_type',\n",
       "  'index': 14}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = \"Mouse thymus was used as a source of glucocorticoid receptor from normal CS lymphocytes.\"\n",
    "\n",
    "nlp_ner = pipeline(\"ner\",\n",
    "                   model='/Users/sdeshpande/Desktop/bioinformatices/bio_ner_bert/trained_models/',\n",
    "                   tokenizer='/Users/sdeshpande/Desktop/bioinformatices/bio_ner_bert/trained_models/')\n",
    "\n",
    "nlp_ner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use pretrained one from transformers directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'glucocorticoid',\n",
       "  'score': 0.9894881248474121,\n",
       "  'entity': 'B-protein',\n",
       "  'index': 9},\n",
       " {'word': 'receptor',\n",
       "  'score': 0.989505410194397,\n",
       "  'entity': 'I-protein',\n",
       "  'index': 10},\n",
       " {'word': 'normal',\n",
       "  'score': 0.7680374383926392,\n",
       "  'entity': 'B-cell_type',\n",
       "  'index': 12},\n",
       " {'word': 'cs',\n",
       "  'score': 0.5176804065704346,\n",
       "  'entity': 'I-cell_type',\n",
       "  'index': 13},\n",
       " {'word': 'lymphocytes',\n",
       "  'score': 0.9898492097854614,\n",
       "  'entity': 'I-cell_type',\n",
       "  'index': 14}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = \"Mouse thymus was used as a source of glucocorticoid receptor from normal CS lymphocytes.\"\n",
    "\n",
    "nlp_ner = pipeline(\"ner\",\n",
    "                   model='/Users/sdeshpande/Desktop/bioinformatices/bio_ner_bert/trained_models/pytorch',\n",
    "                   tokenizer='fran-martinez/scibert_scivocab_cased_ner_jnlpba')\n",
    "\n",
    "nlp_ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse -> O\n",
      "thymus -> O\n",
      "was -> O\n",
      "used -> O\n",
      "as -> O\n",
      "a -> O\n",
      "source -> O\n",
      "of -> O\n",
      "glucocorticoid -> B-protein\n",
      "receptor -> I-protein\n",
      "from -> O\n",
      "normal -> B-cell_type\n",
      "cs -> I-cell_type\n",
      "lymphocytes -> I-cell_type\n",
      ". -> O\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Example\n",
    "text = \"Mouse thymus was used as a source of glucocorticoid receptor from normal CS lymphocytes.\"\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fran-martinez/scibert_scivocab_cased_ner_jnlpba\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"fran-martinez/scibert_scivocab_cased_ner_jnlpba\")\n",
    "\n",
    "# Get input for BERT\n",
    "input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# From the output let's take the first element of the tuple.\n",
    "# Then, let's get rid of [CLS] and [SEP] tokens (first and last)\n",
    "predictions = outputs[0].argmax(axis=-1)[0][1:-1]\n",
    "\n",
    "# Map label class indexes to string labels.\n",
    "for token, pred in zip(tokenizer.tokenize(text), predictions):\n",
    "    print(token, '->', model.config.id2label[pred.numpy().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
